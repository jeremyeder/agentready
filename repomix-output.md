This file is a merged representation of the entire codebase, combined into a single document by Repomix.

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)

# Directory Structure
```
.claude/
  commands/
    speckit.analyze.md
    speckit.checklist.md
    speckit.clarify.md
    speckit.constitution.md
    speckit.implement.md
    speckit.plan.md
    speckit.specify.md
    speckit.tasks.md
    speckit.taskstoissues.md
  settings.local.json
.github/
  coldstart-prompts/
    01-create-automated-demo.md
    02-fix-critical-security-logic-bugs-from-code-review.md
    03-bootstrap-agentready-repository-on-github.md
    04-report-header-with-repository-metadata.md
    05-improve-html-report-design-font-size-color-scheme.md
    06-report-schema-versioning.md
    07-research-report-generatorupdater-utility.md
    08-repomix-integration.md
    09-agentready-repository-agent.md
    10-customizable-html-report-themes.md
    11-fix-code-quality-issues-from-code-review.md
    12-improve-test-coverage-and-edge-case-handling.md
    13-add-security-quality-improvements-from-code-review.md
    14-align-subcommand-automated-remediation.md
    15-interactive-dashboard-with-automated-remediation.md
    16-github-app-integration-badge-status-checks.md
    README.md
.specify/
  memory/
    constitution.md
  scripts/
    bash/
      check-prerequisites.sh
      common.sh
      create-new-feature.sh
      setup-plan.sh
      update-agent-context.sh
  templates/
    agent-file-template.md
    checklist-template.md
    plan-template.md
    spec-template.md
    tasks-template.md
docs/
  _config.yml
examples/
  self-assessment/
    assessment-20251121.json
    README.md
    report-20251121.html
    report-20251121.md
scripts/
  backlog_to_issues.py
specs/
  001-agentready-scorer/
    .remediation/
      A1-proportional-scoring.md
      A2-tier-weight-distribution.md
      README.md
      U1-research-update-mechanism.md
      U2-research-validation.md
      U3-weight-validation.md
    checklists/
      requirements.md
    contracts/
      assessment-schema.json
      report-html-schema.md
      report-markdown-schema.md
      research-report-schema.md
    data-model.md
    plan.md
    quickstart.md
    research.md
    spec.md
    tasks.md
src/
  agentready/
    assessors/
      __init__.py
      base.py
      code_quality.py
      documentation.py
      structure.py
      stub_assessors.py
      testing.py
    cli/
      __init__.py
      bootstrap.py
      main.py
    data/
      agent-ready-codebase-attributes.md
      default-weights.yaml
    models/
      __init__.py
      assessment.py
      attribute.py
      citation.py
      config.py
      finding.py
      repository.py
    reporters/
      __init__.py
      base.py
      html.py
      markdown.py
    services/
      __init__.py
      bootstrap.py
      language_detector.py
      research_loader.py
      scanner.py
      scorer.py
    templates/
      bootstrap/
        issue_templates/
          bug_report.md.j2
          feature_request.md.j2
        workflows/
          agentready-assessment.yml.j2
          security.yml.j2
          tests-python.yml.j2
        CODE_OF_CONDUCT.md.j2
        CODEOWNERS.j2
        CONTRIBUTING.md.j2
        dependabot.yml.j2
        precommit-python.yaml.j2
        PULL_REQUEST_TEMPLATE.md.j2
      report.html.j2
    __init__.py
tests/
  integration/
    __init__.py
    test_scan_workflow.py
  unit/
    __init__.py
    test_assessors_structure.py
    test_models.py
    test_security.py
  __init__.py
.agentready-config.example.yaml
.gitignore
.repomixignore
agent-ready-codebase-attributes.md
BACKLOG.md
CLAUDE.md
GITHUB_ISSUES.md
pyproject.toml
README.md
repomix.config.json
repos.txt
```

# Files

## File: .claude/settings.local.json
````json
{
  "permissions": {
    "allow": [
      "Bash(tree:*)",
      "Bash(agentready bootstrap:*)"
    ],
    "deny": [],
    "ask": []
  }
}
````

## File: .github/coldstart-prompts/01-create-automated-demo.md
````markdown
# Coldstart Implementation Prompt: Create Automated Demo

**Priority**: P0
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Create Automated Demo

**Priority**: P0 (Critical - Showcase Value)

**Description**: Create an automated, self-contained demonstration of AgentReady that shows the tool assessing a sample repository, generating reports, and providing remediation guidance. This should be runnable with a single command and suitable for demos, presentations, and onboarding.

**Requirements**:
- Single command to run: `agentready demo`
- Self-contained sample repository (embedded in package or generated on-the-fly)
- Demonstrates full workflow:
  1. Repository analysis
  2. Attribute assessment
  3. Score calculation
  4. HTML/Markdown report generation
  5. Remediation suggestions
- Interactive terminal output showing progress
- Opens generated HTML report automatically in browser
- Reusable for presentations and stakeholder demos

**Implementation**:

```bash
# Run automated demo
agentready demo

# Output:
# ðŸ¤– AgentReady Demo
# ===================
#
# Creating sample repository...
# Analyzing structure...
# Running 25 assessments...
#   âœ… claude_md_file (100/100)
#   âŒ precommit_hooks (0/100)
#   ... [progress indicators]
#
# Assessment complete!
# Score: 67.3/100 (Silver)
#
# Generating reports...
#   ðŸ“„ demo-report.html (generated)
#   ðŸ“„ demo-report.md (generated)
#   ðŸ“„ demo-assessment.json (generated)
#
# Opening HTML report in browser...
```

**Sample Repository Options**:

**Option 1: Embedded Examples** (Ship with package)
```
src/agentready/demo/
â”œâ”€â”€ sample-python-repo/      # Python project (minimal)
â”‚   â”œâ”€â”€ src/myapp/
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â””â”€â”€ .gitignore
â”œâ”€â”€ sample-js-repo/          # JavaScript project
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ README.md
â””â”€â”€ sample-go-repo/          # Go project
```

**Option 2: Generate On-the-Fly** (Dynamic)
```python
def create_demo_repo(tmp_path: Path, language: str = "python") -> Path:
    """Create sample repository for demo."""
    repo = tmp_path / "demo-repo"
    repo.mkdir()

    if language == "python":
        # Create minimal Python project
        create_file(repo / "README.md", "# Demo Project\n\nSample Python project.")
        create_file(repo / "src/main.py", "def main(): pass")
        create_file(repo / ".gitignore", "*.pyc\n__pycache__/")
        # Missing: CLAUDE.md, tests/, pre-commit hooks (intentional for demo)

    return repo
```

**Demo Script Features**:
- **Progress indicators**: Show assessment running in real-time
- **Color-coded output**: Green for pass, red for fail, yellow for warnings
- **Simulated delays**: Add realistic pauses for "dramatic effect" in demos
- **Narration mode**: Optional verbose output explaining each step
- **Screenshot mode**: Generate high-quality terminal screenshots for docs
- **Record mode**: Save terminal session as GIF/video for presentations

**Configuration**:

```yaml
# Demo config embedded in package
demo:
  sample_repo: python  # or javascript, go, minimal
  show_progress: true
  auto_open_browser: true
  output_dir: ./demo-output
  narration: true  # Explain each step
  delay_ms: 500    # Pause between steps for visibility
```

**Use Cases**:

**Use Case 1: Stakeholder Demo**
```bash
# Quick 2-minute demo for leadership
agentready demo --narration
# Shows: What AgentReady does, how it scores, what reports look like
```

**Use Case 2: Onboarding New Users**
```bash
# Help new users understand the tool
agentready demo --tutorial
# Interactive walkthrough with explanations
```

**Use Case 3: Generate Demo Content for Docs**
```bash
# Create screenshots and videos for documentation
agentready demo --screenshot --record demo.gif
```

**Acceptance Criteria**:
- [ ] `agentready demo` runs without any setup
- [ ] Creates sample repository automatically
- [ ] Runs full assessment workflow
- [ ] Generates all report formats (HTML, Markdown, JSON)
- [ ] Opens HTML report in browser
- [ ] Colorful, engaging terminal output
- [ ] Completes in < 5 seconds
- [ ] No external dependencies required
- [ ] Works offline
- [ ] Includes narration mode for presentations

**Priority Justification**:
- Critical for showcasing tool value to stakeholders
- Needed for Red Hat internal demos and presentations
- Helps with user onboarding and adoption
- Low effort, high impact for visibility
- Required before pitching to other Red Hat teams

**Related**: Onboarding, documentation, marketing

**Notes**:
- Keep demo simple and fast (< 5 seconds)
- Focus on visual impact (colors, progress bars)
- Make it suitable for screenshots/videos
- Consider adding "failure scenario" demo too
- Could expand to multiple language demos
- Add to bootstrap command as part of repo setup

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 001-create-automated-demo

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: .github/coldstart-prompts/02-fix-critical-security-logic-bugs-from-code-review.md
````markdown
# Coldstart Implementation Prompt: Fix Critical Security & Logic Bugs from Code Review

**Priority**: P0
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Fix Critical Security & Logic Bugs from Code Review

**Priority**: P0 (Critical - Security & Correctness)

**Description**: Address critical bugs discovered in code review that affect security and assessment accuracy.

**Issues to Fix**:

1. **XSS Vulnerability in HTML Reports** (CRITICAL - Security)
   - **Location**: `src/agentready/templates/report.html.j2:579`
   - **Problem**: `assessment_json|safe` disables autoescaping for JSON embedded in JavaScript
   - **Risk**: Repository names, commit messages, file paths from git could contain malicious content
   - **Fix**: Replace with `JSON.parse({{ assessment_json|tojson }})`
   - **Add**: Content Security Policy headers to HTML reports

2. **StandardLayoutAssessor Logic Bug** (CRITICAL - Incorrect Scoring)
   - **Location**: `src/agentready/assessors/structure.py:48`
   - **Problem**: `(repository.path / "tests") or (repository.path / "test")` always evaluates to first path
   - **Impact**: Projects with `test/` instead of `tests/` scored incorrectly
   - **Fix**: Check both paths properly:
     ```python
     tests_path = repository.path / "tests"
     if not tests_path.exists():
         tests_path = repository.path / "test"
     has_tests = tests_path.exists()
     ```

**Implementation**:

**File 1**: `src/agentready/templates/report.html.j2`
```jinja2
<!-- BEFORE (VULNERABLE): -->
const ASSESSMENT = {{ assessment_json|safe }};

<!-- AFTER (SECURE): -->
const ASSESSMENT = JSON.parse({{ assessment_json|tojson }});
```

**File 2**: `src/agentready/assessors/structure.py`
```python
# BEFORE (BUGGY):
standard_dirs = {
    "src": repository.path / "src",
    "tests": (repository.path / "tests") or (repository.path / "test"),  # BUG!
}

# AFTER (CORRECT):
standard_dirs = {
    "src": repository.path / "src",
}

# Check for tests directory (either tests/ or test/)
tests_path = repository.path / "tests"
if not tests_path.exists():
    tests_path = repository.path / "test"
standard_dirs["tests"] = tests_path
```

**Test Cases to Add**:
```python
def test_xss_in_repository_name():
    """Test that malicious repo names are escaped in HTML."""
    repo = Repository(
        name="<script>alert('xss')</script>",
        # ...
    )
    html = HTMLReporter().generate(assessment, output)
    assert "<script>" not in html  # Should be escaped

def test_standard_layout_with_test_dir():
    """Test that 'test/' directory is recognized (not just 'tests/')."""
    # Create repo with test/ directory only
    repo_path = tmp_path / "repo"
    (repo_path / "test").mkdir(parents=True)

    assessor = StandardLayoutAssessor()
    finding = assessor.assess(Repository(...))
    assert finding.status == "pass"  # Should recognize test/ dir
```

**Acceptance Criteria**:
- [ ] XSS vulnerability patched with `tojson` filter
- [ ] CSP headers added to HTML reports (future)
- [ ] StandardLayoutAssessor recognizes both `tests/` and `test/`
- [ ] Tests added for XSS prevention
- [ ] Tests added for both test directory naming patterns
- [ ] All existing tests still pass

**Priority Justification**:
- **Security**: XSS is a P0 vulnerability
- **Correctness**: Incorrect scoring undermines tool credibility
- **Quick fixes**: Both are 5-10 minute changes

**Related**: Issue #2 (Report improvements), Bootstrap (#1 - needs secure reports)

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 002-fix-critical-security-&-logic-bugs-from-code-revie

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: .github/coldstart-prompts/03-bootstrap-agentready-repository-on-github.md
````markdown
# Coldstart Implementation Prompt: Bootstrap AgentReady Repository on GitHub

**Priority**: P0
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Bootstrap AgentReady Repository on GitHub

**Priority**: P0 (Critical - Dogfooding)

**Description**: Implement `agentready bootstrap` command to set up the agentready repository itself on GitHub with all best practices. This is the FIRST feature to implement - we'll dogfood our own tool!

**Vision**: Use AgentReady to bootstrap the AgentReady repository - demonstrating the tool's value while setting up our own infrastructure.

**Why P0**:
- Demonstrates tool value immediately (dogfooding)
- Sets up critical GitHub infrastructure (Actions, badges, PR templates)
- Validates bootstrap command design before external users
- Creates the foundation for GitHub App integration

**Requirements**:

1. **GitHub Repository Setup**
   - Create/update repository on GitHub via `gh` CLI
   - Set repository description and topics
   - Configure repository settings (PR requirements, branch protection)

2. **GitHub Actions Workflows**
   - `.github/workflows/agentready-assessment.yml` - Run assessment on PR/push
   - `.github/workflows/tests.yml` - Run pytest, linters
   - `.github/workflows/release.yml` - Publish to PyPI (future)

3. **GitHub Templates**
   - `.github/ISSUE_TEMPLATE/bug_report.md`
   - `.github/ISSUE_TEMPLATE/feature_request.md`
   - `.github/PULL_REQUEST_TEMPLATE.md`
   - `.github/CODEOWNERS`

4. **Pre-commit Hooks**
   - `.pre-commit-config.yaml` with black, isort, ruff
   - Conventional commit linting (commitlint)
   - Auto-run tests before commit

5. **Dependency Management**
   - Dependabot configuration (`.github/dependabot.yml`)
   - Security scanning (`.github/workflows/security.yml`)

6. **Documentation Updates**
   - Update README.md with badges
   - Add CONTRIBUTING.md
   - Add CODE_OF_CONDUCT.md (Red Hat standard)
   - Add LICENSE (Apache 2.0 or MIT)

**Command Interface**:

```bash
# Bootstrap current repository on GitHub
agentready bootstrap .

# Bootstrap with specific language
agentready bootstrap . --language python

# Bootstrap and create GitHub repo
agentready bootstrap . --create-repo redhat/agentready

# Dry run (show what would be created)
agentready bootstrap . --dry-run

# Interactive mode (confirm each file)
agentready bootstrap . --interactive
```

**What Gets Created**:

```
.github/
â”œâ”€â”€ workflows/
â”‚   â”œâ”€â”€ agentready-assessment.yml  # Run assessment on PR
â”‚   â”œâ”€â”€ tests.yml                  # Run tests and linters
â”‚   â””â”€â”€ dependabot.yml            # Dependency updates
â”œâ”€â”€ ISSUE_TEMPLATE/
â”‚   â”œâ”€â”€ bug_report.md
â”‚   â””â”€â”€ feature_request.md
â”œâ”€â”€ PULL_REQUEST_TEMPLATE.md
â”œâ”€â”€ CODEOWNERS
â””â”€â”€ dependabot.yml

.pre-commit-config.yaml
CONTRIBUTING.md
CODE_OF_CONDUCT.md
LICENSE
```

**GitHub Actions Workflow Example**:

```yaml
# .github/workflows/agentready-assessment.yml
name: AgentReady Assessment

on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches: [main]

jobs:
  assess:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install AgentReady
        run: |
          pip install agentready

      - name: Run Assessment
        run: |
          agentready assess . --verbose

      - name: Upload Reports
        uses: actions/upload-artifact@v4
        with:
          name: agentready-reports
          path: .agentready/

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('.agentready/report-latest.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
```

**Implementation Phases**:

**Phase 1: Core Bootstrap** (This Sprint)
- Implement `agentready bootstrap` command
- Create template files for GitHub Actions, pre-commit, templates
- Test on agentready repository itself
- Commit all generated files

**Phase 2: GitHub Integration** (Next Sprint)
- Use `gh` CLI to create/update repository
- Set up branch protection rules
- Configure repository settings
- Add repository badges

**Phase 3: Language-Specific Templates** (Future)
- Python-specific templates (pytest, black, mypy)
- JavaScript-specific (eslint, prettier, jest)
- Go-specific (golangci-lint, gotestsum)

**Files to Create**:

```
src/agentready/
â”œâ”€â”€ cli/bootstrap.py           # Bootstrap CLI command
â”œâ”€â”€ bootstrap/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ generator.py          # File generator
â”‚   â”œâ”€â”€ github_setup.py       # GitHub integration
â”‚   â””â”€â”€ templates/            # Template files
â”‚       â”œâ”€â”€ github/
â”‚       â”‚   â”œâ”€â”€ workflows/
â”‚       â”‚   â”‚   â”œâ”€â”€ agentready.yml.j2
â”‚       â”‚   â”‚   â””â”€â”€ tests.yml.j2
â”‚       â”‚   â”œâ”€â”€ ISSUE_TEMPLATE/
â”‚       â”‚   â””â”€â”€ PULL_REQUEST_TEMPLATE.md.j2
â”‚       â”œâ”€â”€ precommit.yaml.j2
â”‚       â”œâ”€â”€ CONTRIBUTING.md.j2
â”‚       â””â”€â”€ CODE_OF_CONDUCT.md.j2
tests/unit/test_bootstrap.py
```

**Acceptance Criteria**:

- [ ] `agentready bootstrap .` creates all required files
- [ ] GitHub Actions workflows are valid and functional
- [ ] Pre-commit hooks install and run successfully
- [ ] Repository badges added to README.md
- [ ] Dry-run mode shows what would be created
- [ ] Interactive mode prompts for confirmation
- [ ] Successfully bootstrap agentready repository itself
- [ ] All generated files committed to agentready repo
- [ ] GitHub Actions runs assessment on every PR

**Success Metrics**:

- AgentReady repository on GitHub has:
  - âœ… GitHub Actions running assessments
  - âœ… PR template with checklist
  - âœ… Issue templates for bugs/features
  - âœ… Pre-commit hooks configured
  - âœ… Dependabot enabled
  - âœ… Repository badge showing score
  - âœ… All linters passing in CI

**Priority Justification**:

This is P0 because:
1. **Dogfooding** - We need this for our own repository first
2. **Demonstrates value** - Shows AgentReady in action on itself
3. **Foundation** - Required before GitHub App integration
4. **Credibility** - Can't tell others to use it if we don't use it ourselves

**Related**: GitHub App Integration (#5), Align Command (#3)

**Notes**:
- Start with Python-specific templates (our use case)
- Keep templates simple and focused
- Use Jinja2 for template rendering
- Integrate with `gh` CLI for GitHub operations
- All templates should pass AgentReady assessment!

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 003-bootstrap-agentready-repository-on-github

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: .github/coldstart-prompts/04-report-header-with-repository-metadata.md
````markdown
# Coldstart Implementation Prompt: Report Header with Repository Metadata

**Priority**: P0
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Report Header with Repository Metadata

**Priority**: P0 (Critical - Blocking Usability)

**Description**: Add prominent report header showing what repository/agent/code was scanned. Currently reports lack context about what was assessed.

**Problem**: Users cannot identify what the report is about without digging into the details. No repository name, path, timestamp, or assessment context visible at the top.

**Requirements**:
- **Prominent header section** at the top of all report formats (HTML, Markdown, JSON)
- Repository name (bold, large font)
- Repository path (absolute path on filesystem or GitHub URL)
- Assessment timestamp (human-readable: "November 21, 2025 at 2:11 AM")
- Branch name and commit hash
- AgentReady version used for assessment
- Who ran the assessment (username@hostname)
- Command used: `agentready assess /path/to/repo --verbose`

**HTML Report Header Design**:
```html
<header class="report-header">
  <div class="repo-info">
    <h1>AgentReady Assessment Report</h1>
    <div class="repo-name">Repository: agentready</div>
    <div class="repo-path">/Users/jeder/repos/sk/agentready</div>
    <div class="repo-git">Branch: 001-agentready-scorer | Commit: d49947c</div>
  </div>
  <div class="meta-info">
    <div>Assessed: November 21, 2025 at 2:11 AM</div>
    <div>AgentReady Version: 1.0.0</div>
    <div>Run by: jeder@macbook</div>
  </div>
</header>
```

**Markdown Report Header**:
```markdown
# ðŸ¤– AgentReady Assessment Report

**Repository**: agentready
**Path**: `/Users/jeder/repos/sk/agentready`
**Branch**: `001-agentready-scorer` | **Commit**: `d49947c`
**Assessed**: November 21, 2025 at 2:11 AM
**AgentReady Version**: 1.0.0
**Run by**: jeder@macbook

---
```

**JSON Report Metadata**:
```json
{
  "metadata": {
    "agentready_version": "1.0.0",
    "assessment_timestamp": "2025-11-21T02:11:05Z",
    "assessment_timestamp_human": "November 21, 2025 at 2:11 AM",
    "executed_by": "jeder@macbook",
    "command": "agentready assess . --verbose",
    "working_directory": "/Users/jeder/repos/sk/agentready"
  },
  "repository": { ... }
}
```

**Implementation**:
- Add metadata collection to Scanner
- Update all reporter templates (HTML, Markdown)
- Enhance Assessment model with metadata field
- Position header prominently (before score summary)

**Acceptance Criteria**:
- âœ… User can immediately identify what repository was assessed
- âœ… Timestamp shows when assessment was run
- âœ… Git context (branch, commit) visible
- âœ… AgentReady version tracked for reproducibility

**Related**: Report generation, usability, debugging

**Notes**:
- This is blocking adoption - users confused about report context
- Critical for multi-repository workflows
- Needed for CI/CD integration (track which build)

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 004-report-header-with-repository-metadata

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: .github/coldstart-prompts/05-improve-html-report-design-font-size-color-scheme.md
````markdown
# Coldstart Implementation Prompt: Improve HTML Report Design (Font Size & Color Scheme)

**Priority**: P0
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Improve HTML Report Design (Font Size & Color Scheme)

**Priority**: P0 (Critical - Poor User Experience)

**Description**: Completely redesign HTML report color scheme and increase all font sizes by at least 4 points for readability.

**Problems**:
1. **Color scheme is "hideous"** (user feedback) - current purple gradient doesn't work
2. **Font sizes too small** - hard to read on modern displays
3. **Poor contrast** - some text hard to distinguish

**New Color Scheme** (Dark/Professional):
```css
:root {
  /* Base colors - mostly black, dark blue, purple, white */
  --background: #0a0e27;           /* Almost black with blue tint */
  --surface: #1a1f3a;              /* Dark blue surface */
  --surface-elevated: #252b4a;     /* Slightly lighter surface */

  /* Primary colors */
  --primary: #8b5cf6;              /* Purple (accent) */
  --primary-light: #a78bfa;        /* Light purple */
  --primary-dark: #6d28d9;         /* Dark purple */

  /* Text colors */
  --text-primary: #f8fafc;         /* Almost white */
  --text-secondary: #cbd5e1;       /* Light gray */
  --text-muted: #94a3b8;           /* Muted gray */

  /* Status colors */
  --success: #10b981;              /* Green (pass) */
  --warning: #f59e0b;              /* Amber (warning) */
  --danger: #ef4444;               /* Red (fail) */
  --neutral: #64748b;              /* Gray (skipped) */

  /* UI elements */
  --border: #334155;               /* Dark border */
  --shadow: rgba(0, 0, 0, 0.5);   /* Deep shadows */
}
```

**Font Size Increases** (+4pt minimum):
```css
/* Current â†’ New */
body { font-size: 14px â†’ 18px; }
h1 { font-size: 28px â†’ 36px; }
h2 { font-size: 24px â†’ 30px; }
h3 { font-size: 20px â†’ 26px; }
.score { font-size: 48px â†’ 56px; }
.attribute-name { font-size: 16px â†’ 22px; }
.evidence { font-size: 13px â†’ 17px; }
code { font-size: 13px â†’ 16px; }
```

**Design Mockup**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Dark navy blue background #1a1f3a]                â”‚
â”‚                                                     â”‚
â”‚  ðŸ¤– AgentReady Assessment Report                   â”‚
â”‚  Repository: agentready                            â”‚
â”‚  /Users/jeder/repos/sk/agentready                  â”‚
â”‚  [White text #f8fafc, 18px base font]             â”‚
â”‚                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [Purple accent card #8b5cf6]                       â”‚
â”‚                                                     â”‚
â”‚           75.4 / 100                               â”‚
â”‚           [56px, bold, white]                      â”‚
â”‚           ðŸ¥‡ Gold                                   â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation Checklist**:
- [ ] Replace gradient backgrounds with dark blue/black
- [ ] Update all font sizes (+4pt minimum)
- [ ] Use purple (#8b5cf6) sparingly as accent color
- [ ] Ensure white text on dark backgrounds (WCAG AA)
- [ ] Update certification level colors
- [ ] Redesign score cards with new scheme
- [ ] Test with colorblind simulators
- [ ] Add light mode as alternative (with same professional palette)

**Before/After Color Comparison**:
```
Current (Problems):
- Purple gradient everywhere: #667eea â†’ #764ba2 âŒ
- Small text: 14px base âŒ
- Busy, overwhelming âŒ

New (Professional):
- Dark blue/black base: #0a0e27, #1a1f3a âœ…
- Purple accents only: #8b5cf6 âœ…
- Large text: 18px base âœ…
- Clean, readable âœ…
```

**Acceptance Criteria**:
- âœ… All text easily readable (18px minimum body text)
- âœ… Color scheme uses black, dark blue, purple, white palette
- âœ… High contrast (WCAG 2.1 AA compliant)
- âœ… Professional appearance suitable for Red Hat engineering
- âœ… Purple used as accent, not dominant color

**Related**: HTML report generation, UX, accessibility

**Notes**:
- Current design blocks user adoption (visual issues)
- This is the first thing users see - must be excellent
- Consider adding screenshot to docs after redesign
- Font size critical for presentations and stakeholder reviews

---

## Schema & Configuration


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 005-improve-html-report-design-(font-size-&-color-sche

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: .github/coldstart-prompts/06-report-schema-versioning.md
````markdown
# Coldstart Implementation Prompt: Report Schema Versioning

**Priority**: P3
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Report Schema Versioning

**Priority**: P3 (Important)

**Description**: Define and version the JSON/HTML/Markdown report schemas to ensure backwards compatibility and enable schema evolution.

**Requirements**:
- JSON schema for assessment reports (contracts/assessment-schema.json exists)
- HTML schema for interactive reports (contracts/report-html-schema.md exists)
- Markdown schema for version control reports (contracts/report-markdown-schema.md exists)
- Schema versioning strategy (semantic versioning)
- Backwards compatibility testing
- Schema migration tools for major version changes

**Use Case**:
```bash
# Validate report against schema
agentready validate-report assessment-2025-11-20.json

# Migrate old report to new schema version
agentready migrate-report --from 1.0.0 --to 2.0.0 old-report.json
```

**Related**: Report generation, data model evolution

**Notes**:
- Schemas exist in contracts/ directory but need formal versioning
- Consider using JSON Schema Draft 2020-12
- Tool should validate generated reports against bundled schema
- Breaking schema changes should trigger major version bump

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 006-report-schema-versioning

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: .github/coldstart-prompts/07-research-report-generatorupdater-utility.md
````markdown
# Coldstart Implementation Prompt: Research Report Generator/Updater Utility

**Priority**: P4
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Research Report Generator/Updater Utility

**Priority**: P4 (Enhancement)

**Description**: Create a utility tool to help maintain and update the research report (agent-ready-codebase-attributes.md) following the validation schema defined in contracts/research-report-schema.md.

**Requirements**:
- Generate new research reports from templates
- Validate existing reports against schema (contracts/research-report-schema.md)
- Update/add attributes while maintaining schema compliance
- Automatically format citations and references
- Extract tier assignments and metadata
- Verify 25 attributes, 4 tiers, 20+ references
- Check for required sections (Definition, Measurable Criteria, Impact on Agent Behavior)

**Use Case**:
```bash
# Validate existing research report
agentready research validate agent-ready-codebase-attributes.md

# Generate new research report from template
agentready research init --output new-research.md

# Add new attribute to research report
agentready research add-attribute \
  --id "attribute_26" \
  --name "New Attribute" \
  --tier 2 \
  --file research.md

# Update metadata (version, date)
agentready research bump-version --type minor

# Lint and format research report
agentready research format research.md
```

**Features**:
- Schema validation (errors vs warnings per research-report-schema.md)
- Automated metadata header generation (version, date in YAML frontmatter)
- Attribute numbering consistency checks (1.1, 1.2, ..., 15.1)
- Citation deduplication and formatting
- Tier distribution balance warnings
- Category coverage analysis
- Markdown formatting enforcement (consistent structure)
- Reference URL reachability checks

**Related**: Research report maintenance, schema compliance, documentation quality

**Notes**:
- Must follow contracts/research-report-schema.md validation rules
- Should prevent invalid reports from being committed
- Could integrate with pre-commit hooks for research report changes
- Consider CLI commands under `agentready research` subcommand
- Tool should be self-documenting (help users fix validation errors)
- Future: Could use LLMs to help generate attribute descriptions from academic papers

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 007-research-report-generator/updater-utility

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: .github/coldstart-prompts/08-repomix-integration.md
````markdown
# Coldstart Implementation Prompt: Repomix Integration

**Priority**: P4
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Repomix Integration

**Priority**: P4 (Enhancement)

**Description**: Integrate with Repomix (https://github.com/yamadashy/repomix) to generate AI-optimized repository context files for both new and existing repositories.

**Requirements**:
- Generate Repomix output for existing repositories
- Include Repomix configuration in bootstrapped new repositories
- Optional GitHub Actions integration for automatic regeneration
- Support Repomix configuration customization
- Integrate with agentready assessment workflow

**Use Case**:
```bash
# Generate Repomix output for current repository
agentready repomix-generate

# Bootstrap new repo with Repomix integration
agentready init --repo my-project --language python --repomix

# This would:
# 1. Set up Repomix configuration
# 2. Add GitHub Action for automatic regeneration
# 3. Generate initial repository context file
# 4. Include Repomix output in .gitignore appropriately
```

**Features**:
- Automatic Repomix configuration based on repository language
- GitHub Actions workflow for CI/CD integration
- Custom ignore patterns aligned with agentready assessment
- Support for both markdown and XML output formats
- Integration with agentready bootstrap command

**Related**: Repository initialization, AI-assisted development workflows

**Notes**:
- Repomix generates optimized repository context for LLMs
- Could enhance CLAUDE.md with reference to Repomix output
- Should align with existing .gitignore patterns
- Consider adding Repomix freshness check to assessment attributes
- May improve agentready's own repository understanding

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 008-repomix-integration

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: .github/coldstart-prompts/09-agentready-repository-agent.md
````markdown
# Coldstart Implementation Prompt: AgentReady Repository Agent

**Priority**: P3
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

AgentReady Repository Agent

**Priority**: P3 (Important)

**Description**: Create a specialized Claude Code agent for the AgentReady repository to assist with development, testing, and maintenance tasks.

**Requirements**:
- Agent with deep knowledge of AgentReady architecture
- Understands assessment workflow, scoring logic, and report generation
- Can help with:
  - Implementing new assessors
  - Enhancing existing assessors
  - Writing tests for new features
  - Debugging assessment issues
  - Improving report templates
  - Optimizing performance

**Use Case**:
```bash
# In Claude Code, use the agentready-dev agent
/agentready-dev implement new assessor for dependency security scanning
/agentready-dev debug why Python type annotation detection is failing
/agentready-dev optimize assessment performance for large repositories
```

**Features**:
- Pre-loaded context about AgentReady codebase structure
- Knowledge of assessment attributes and scoring algorithm
- Understanding of tier-based weighting system
- Familiar with reporter implementations (HTML, Markdown)
- Can generate new assessors following established patterns

**Implementation**:
- Create `.claude/agents/agentready-dev.md` with agent specification
- Include links to key design documents (data-model.md, plan.md, research.md)
- Provide common development patterns and examples
- Reference test structure and coverage requirements

**Related**: Development workflow, code generation, testing

**Notes**:
- Agent should follow constitution principles (library-first, TDD when requested)
- Should know about stub assessors and how to expand them
- Can help with performance benchmarking and optimization
- Should understand the research report structure and attribute definitions

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 009-agentready-repository-agent

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: .github/coldstart-prompts/10-customizable-html-report-themes.md
````markdown
# Coldstart Implementation Prompt: Customizable HTML Report Themes

**Priority**: P4
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Customizable HTML Report Themes

**Priority**: P4 (Enhancement)

**Description**: Allow users to customize the appearance of HTML reports with themes, color schemes, and layout options.

**Requirements**:
- Theme system for HTML reports
- Pre-built themes (default, dark mode, high contrast, colorblind-friendly)
- Custom theme support via configuration
- Maintain accessibility standards (WCAG 2.1 AA)
- Preview themes before applying

**Use Case**:
```yaml
# .agentready-config.yaml
report_theme: dark  # or 'light', 'high-contrast', 'custom'

custom_theme:
  primary_color: "#2563eb"
  success_color: "#10b981"
  warning_color: "#f59e0b"
  danger_color: "#ef4444"
  background: "#1e293b"
  text: "#e2e8f0"
  font_family: "Inter, sans-serif"
```

**Features**:
- **Theme dropdown in top-right corner of HTML report** (runtime switching)
- **Quick dark/light mode toggle button** (one-click switching between dark and light)
- Multiple built-in themes (light, dark, high-contrast, solarized, dracula)
- Dark mode support with proper color inversion
- Custom color palettes
- Font selection (system fonts + web-safe fonts)
- Layout density options (compact, comfortable, spacious)
- Logo/branding customization
- Export theme as reusable configuration
- Save theme preference to localStorage (persists across reports)

**Implementation**:
- CSS custom properties (variables) for theming
- JavaScript theme switcher in HTML report (no page reload)
- Theme loader in HTMLReporter
- Validate theme configurations
- Preserve accessibility in all themes (WCAG 2.1 AA)
- Add theme preview command: `agentready theme-preview dark`
- Embed all theme CSS in single HTML file (offline-capable)

**Related**: HTML report generation, user experience

**Notes**:
- All themes must maintain WCAG 2.1 AA contrast ratios
- Dark mode should invert appropriately, not just be dark
- Consider colorblind-friendly palettes (Viridis, ColorBrewer)
- Custom themes should be shareable (export/import)
- Could add theme gallery in documentation

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 010-customizable-html-report-themes

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: .github/coldstart-prompts/11-fix-code-quality-issues-from-code-review.md
````markdown
# Coldstart Implementation Prompt: Fix Code Quality Issues from Code Review

**Priority**: P1
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Fix Code Quality Issues from Code Review

**Priority**: P1 (High - Quality & Reliability)

**Description**: Address P1 issues discovered in code review that affect reliability, accuracy, and code quality.

**Issues to Fix**:

1. **TOCTOU (Time-of-Check-Time-of-Use) in File Operations**
   - **Location**: Multiple assessors (`documentation.py:46-50`, `documentation.py:174-191`)
   - **Problem**: Check if file exists, then read in separate operation - file could be deleted in between
   - **Impact**: Crashes instead of graceful degradation
   - **Fix**: Use try-except around file reads instead of existence checks
   ```python
   # BEFORE:
   if claude_md_path.exists():
       size = claude_md_path.stat().st_size

   # AFTER:
   try:
       with open(claude_md_path, "r") as f:
           size = len(f.read())
   except FileNotFoundError:
       return Finding(...status="fail"...)
   except OSError as e:
       return Finding.error(self.attribute, f"Could not read: {e}")
   ```

2. **Inaccurate Type Annotation Detection**
   - **Location**: `src/agentready/assessors/code_quality.py:98-102`
   - **Problem**: Regex-based detection has false positives (string literals, dict literals)
   - **Impact**: Inflated type annotation coverage scores
   - **Fix**: Use AST parsing instead of regex:
   ```python
   import ast
   tree = ast.parse(content)
   for node in ast.walk(tree):
       if isinstance(node, ast.FunctionDef):
           total_functions += 1
           has_annotations = (node.returns is not None or
                            any(arg.annotation for arg in node.args.args))
           if has_annotations:
               typed_functions += 1
   ```

3. **Assessment Validation Semantic Confusion**
   - **Location**: `src/agentready/models/assessment.py:54-59`
   - **Problem**: Field named `attributes_skipped` but includes `error` and `not_applicable` statuses
   - **Impact**: Confusing API, unclear semantics
   - **Fix**: Rename to `attributes_not_assessed` OR add separate counters

**Acceptance Criteria**:
- [ ] All file operations use try-except pattern
- [ ] Type annotation detection uses AST parsing
- [ ] Assessment model fields clearly named
- [ ] Tests added for TOCTOU edge cases
- [ ] Tests added for type annotation false positives
- [ ] Documentation updated

**Priority Justification**: These affect reliability and measurement accuracy - critical for a quality assessment tool.

**Related**: Testing improvements, code quality

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 011-fix-code-quality-issues-from-code-review

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: .github/coldstart-prompts/12-improve-test-coverage-and-edge-case-handling.md
````markdown
# Coldstart Implementation Prompt: Improve Test Coverage and Edge Case Handling

**Priority**: P1
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Improve Test Coverage and Edge Case Handling

**Priority**: P1 (High - Quality Assurance)

**Description**: Increase test coverage from 37% to >80% and add tests for critical edge cases discovered in code review.

**Critical Test Gaps**:

1. **Error Handling Paths** (Currently 0% coverage)
   - OSError, PermissionError in file operations
   - MissingToolError in assessors
   - Invalid repository paths
   - Malformed git repositories

2. **Edge Cases** (No tests)
   - Empty repositories
   - Binary files instead of text
   - Symlinks in repository
   - Very large repositories (>10k files)
   - Repositories with `test/` vs `tests/` directories

3. **Security Test Cases**
   - XSS in repository names, commit messages
   - Path traversal attempts
   - Malicious file names

4. **Scorer Edge Cases**
   - All attributes skipped (score should be 0.0)
   - Config weights don't sum to 1.0
   - Division by zero scenarios

**Implementation**:
```python
# tests/unit/test_edge_cases.py
def test_empty_repository(tmp_path):
    """Test assessment of completely empty repository."""
    # Create empty git repo
    repo = Repository(path=tmp_path, ...)
    scanner = Scanner(repo, config=None)
    assessment = scanner.scan(assessors)
    # Should not crash, should have valid score
    assert 0.0 <= assessment.overall_score <= 100.0

def test_permission_denied_file(tmp_path):
    """Test graceful handling of permission errors."""
    # Create unreadable file
    restricted = tmp_path / "CLAUDE.md"
    restricted.write_text("test")
    restricted.chmod(0o000)

    assessor = CLAUDEmdAssessor()
    finding = assessor.assess(Repository(...))
    assert finding.status == "error"
    assert "permission" in finding.error_message.lower()

def test_binary_file_as_readme(tmp_path):
    """Test handling of binary files."""
    readme = tmp_path / "README.md"
    readme.write_bytes(b"\x00\x01\x02\x03")

    assessor = READMEAssessor()
    finding = assessor.assess(Repository(...))
    # Should not crash
```

**Acceptance Criteria**:
- [ ] Test coverage increased to >80%
- [ ] All error handling paths tested
- [ ] Edge cases for empty/malformed repos covered
- [ ] Security test cases added
- [ ] Integration tests for complete workflows
- [ ] CI runs coverage report and fails <75%

**Priority Justification**: Quality assessment tool must be thoroughly tested. Current 37% coverage is unacceptable.

**Related**: CI/CD improvements, reliability

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 012-improve-test-coverage-and-edge-case-handling

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: .github/coldstart-prompts/13-add-security-quality-improvements-from-code-review.md
````markdown
# Coldstart Implementation Prompt: Add Security & Quality Improvements from Code Review

**Priority**: P2
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Add Security & Quality Improvements from Code Review

**Priority**: P2 (Medium - Polish)

**Description**: Address P2 improvements from code review for better UX and robustness.

**Improvements**:

1. **Input Validation Warnings**
   - Warn when scanning sensitive directories (`/etc`, `/.ssh`, `/var`)
   - Confirm before scanning large repositories (>10k files)

2. **Scorer Semantic Clarity**
   - Document behavior when all attributes skipped (returns 0.0)
   - Consider returning `None` or special value for "not assessable"
   - Add explicit documentation of edge cases

3. **Content Security Policy Headers**
   - Add CSP to HTML reports for defense-in-depth
   - Prevent inline script execution
   - Whitelist only necessary sources

**Implementation**:
```python
# In CLI
sensitive_dirs = ['/etc', '/sys', '/proc', '/.ssh', '/var']
if any(str(repo_path).startswith(p) for p in sensitive_dirs):
    click.confirm(
        f"Warning: Scanning sensitive directory {repo_path}. Continue?",
        abort=True
    )

# In HTMLReporter
csp_header = (
    "<meta http-equiv='Content-Security-Policy' "
    "content=\"default-src 'self'; script-src 'unsafe-inline'; "
    "style-src 'unsafe-inline'\">"
)
```

**Acceptance Criteria**:
- [ ] Warnings for sensitive directories
- [ ] CSP headers in HTML reports
- [ ] Scorer edge cases documented
- [ ] User guide updated with best practices

**Priority Justification**: UX polish and defense-in-depth, not critical bugs.

**Related**: User experience, security hardening

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 013-add-security-&-quality-improvements-from-code-revi

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: .github/coldstart-prompts/14-align-subcommand-automated-remediation.md
````markdown
# Coldstart Implementation Prompt: Align Subcommand (Automated Remediation)

**Priority**: P1
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Align Subcommand (Automated Remediation)

**Priority**: P1 (Critical)

**Description**: Implement `agentready align` subcommand that automatically fixes failing attributes by generating and applying changes to the repository.

**Vision**: One command to align your repository with best practices - automatically create missing files, configure tooling, and improve code quality.

**Core Command**:

```bash
# Align repository to best practices
agentready align .

# Preview changes without applying
agentready align . --dry-run

# Apply specific attributes only
agentready align . --attributes claude_md_file,precommit_hooks

# Create GitHub PR instead of direct changes
agentready align . --create-pr

# Interactive mode (confirm each change)
agentready align . --interactive
```

**Supported Fixes**:

1. **Template-Based Fixes** (Auto-applicable):
   - `claude_md_file`: Generate CLAUDE.md from repository analysis
   - `gitignore_completeness`: Add missing patterns to .gitignore
   - `precommit_hooks`: Create .pre-commit-config.yaml with language-specific hooks
   - `readme_structure`: Scaffold missing README sections
   - `lock_files`: Generate lock files (package-lock.json, requirements.txt, etc.)
   - `issue_pr_templates`: Create .github/ISSUE_TEMPLATE and PULL_REQUEST_TEMPLATE
   - `conventional_commits`: Add commitlint configuration

2. **Command-Based Fixes** (Execute commands):
   - `lock_files`: Run `npm install`, `poetry lock`, `go mod tidy`
   - `precommit_hooks`: Run `pre-commit install`
   - `dependency_freshness`: Run `npm update`, `pip install --upgrade`

3. **AI-Powered Fixes** (Require LLM, optional):
   - `type_annotations`: Add type hints to Python functions
   - `inline_documentation`: Generate docstrings from function signatures
   - `cyclomatic_complexity`: Refactor high-complexity functions
   - `file_size_limits`: Split large files into smaller modules

**Workflow**:

```bash
# User runs alignment
$ agentready align . --dry-run

AgentReady Alignment Preview
============================

Repository: /Users/jeder/my-project
Current Score: 62.4/100 (Silver)
Projected Score: 84.7/100 (Gold) ðŸŽ¯

Changes to be applied:

  âœ… claude_md_file (10 points)
     CREATE CLAUDE.md (1.2 KB)

  âœ… precommit_hooks (3 points)
     CREATE .pre-commit-config.yaml (845 bytes)
     RUN pre-commit install

  âœ… gitignore_completeness (3 points)
     MODIFY .gitignore (+15 patterns)

  âš ï¸  type_annotations (10 points) - requires AI
     MODIFY 23 Python files (add type hints)
     Use --ai to enable AI-powered fixes

Total: 3 automatic fixes, 1 AI fix available
Apply changes? [y/N]
```

**Implementation**:

```python
# src/agentready/fixers/base.py
class BaseFixer(ABC):
    """Base class for attribute fixers."""

    @abstractmethod
    def can_fix(self, finding: Finding) -> bool:
        """Check if this fixer can fix the finding."""
        pass

    @abstractmethod
    def generate_fix(self, repository: Repository, finding: Finding) -> Fix:
        """Generate fix for the finding."""
        pass

# src/agentready/fixers/template_fixer.py
class TemplateFixer(BaseFixer):
    """Fixer that generates files from templates."""

    def generate_fix(self, repository: Repository, finding: Finding) -> Fix:
        template = self.load_template(finding.attribute.id)
        content = self.render_template(template, repository)
        return FileCreationFix(path="CLAUDE.md", content=content)

# src/agentready/cli/align.py
@cli.command()
@click.argument("repository", type=click.Path(exists=True), default=".")
@click.option("--dry-run", is_flag=True, help="Preview changes without applying")
@click.option("--create-pr", is_flag=True, help="Create GitHub PR instead of direct changes")
@click.option("--interactive", is_flag=True, help="Confirm each change")
@click.option("--attributes", help="Comma-separated attribute IDs to fix")
@click.option("--ai", is_flag=True, help="Enable AI-powered fixes (requires API key)")
def align(repository, dry_run, create_pr, interactive, attributes, ai):
    """Align repository with best practices by applying automatic fixes."""

    # Run assessment first
    assessment = run_assessment(repository)

    # Identify fixable failures
    failures = [f for f in assessment.findings if f.status == "fail"]
    fixable = identify_fixable_failures(failures, enable_ai=ai)

    # Generate fixes
    fixes = [fixer.generate_fix(repo, finding) for finding in fixable]

    # Preview changes
    show_fix_preview(fixes, assessment.overall_score, projected_score)

    if dry_run:
        return

    if interactive and not confirm_each_fix(fixes):
        return

    # Apply fixes
    if create_pr:
        create_github_pr_with_fixes(fixes)
    else:
        apply_fixes(fixes)

    # Re-run assessment to show improvement
    new_assessment = run_assessment(repository)
    show_improvement(assessment.overall_score, new_assessment.overall_score)
```

**Fix Types**:

```python
class Fix(ABC):
    """Base class for fixes."""
    attribute_id: str
    description: str

class FileCreationFix(Fix):
    """Create a new file."""
    path: Path
    content: str

class FileModificationFix(Fix):
    """Modify existing file."""
    path: Path
    changes: List[TextChange]

class CommandFix(Fix):
    """Execute command."""
    command: str
    working_dir: Path

class MultiStepFix(Fix):
    """Combination of multiple fixes."""
    steps: List[Fix]
```

**GitHub PR Integration**:

```bash
# Create PR with fixes
$ agentready align . --create-pr

Creating fix branch: agentready-align-20251121
Applying 3 fixes...
  âœ… Created CLAUDE.md
  âœ… Created .pre-commit-config.yaml
  âœ… Modified .gitignore

Committing changes...
Pushing to origin...

Created PR: https://github.com/redhat/my-project/pull/42
  Title: "Improve AgentReady score from 62.4 to 84.7 (Silver â†’ Gold)"
  Score improvement: +22.3 points
  Attributes fixed: 3
```

**Configuration**:

```yaml
# .agentready-config.yaml
align:
  enabled: true

  auto_fix:
    # Attributes to automatically fix without confirmation
    - claude_md_file
    - gitignore_completeness
    - precommit_hooks

  confirm_before_fix:
    # Attributes requiring confirmation
    - type_annotations
    - cyclomatic_complexity

  never_fix:
    # Attributes to skip (user will fix manually)
    - container_setup
    - openapi_specs

  ai_fixes:
    enabled: false  # Require --ai flag
    provider: "anthropic"  # or "openai"
    model: "claude-3-5-sonnet-20241022"
    max_tokens: 4096
```

**Use Cases**:

**Use Case 1: New Repository Setup**
```bash
# Clone new project
git clone github.com/redhat/new-project
cd new-project

# Align to best practices
agentready align . --interactive

# Review and commit changes
git add .
git commit -m "chore: Align repository with AgentReady best practices"
```

**Use Case 2: Continuous Improvement**
```bash
# Weekly CI job to check and create alignment PRs
agentready align . --create-pr --dry-run
# If score < threshold, create PR automatically
```

**Use Case 3: Pre-commit Hook**
```bash
# .pre-commit-config.yaml
- repo: local
  hooks:
    - id: agentready-align
      name: AgentReady Alignment Check
      entry: agentready align --dry-run
      language: system
      pass_filenames: false
```

**Safety Features**:

- **Dry-run by default** for destructive operations
- **Git worktree** for isolated changes (optional)
- **Backup creation** before modifying files
- **Rollback support** if fixes fail
- **Validation** of generated files before writing
- **Interactive confirmation** for AI-powered fixes

**Related**: Automated remediation, repository improvement, onboarding

**Notes**:
- Start with template-based fixes (highest ROI, lowest risk)
- AI-powered fixes require API key and user consent
- Some attributes cannot be automatically fixed (requires human judgment)
- Consider integration with `git stash` for safety
- Could generate shell script of changes for manual review
- Align with Red Hat's AI-assisted development workflow

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 014-align-subcommand-(automated-remediation)

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: .github/coldstart-prompts/15-interactive-dashboard-with-automated-remediation.md
````markdown
# Coldstart Implementation Prompt: Interactive Dashboard with Automated Remediation

**Priority**: P2
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

Interactive Dashboard with Automated Remediation

**Priority**: P2 (High Value)

**Description**: Transform the static HTML report into an interactive dashboard that enables one-click remediation via automated GitHub issue creation and draft PR generation.

**Vision**: "How to Align Your Repo with Best Practices" - Click a button â†’ Open GitHub issue with draft PR containing fixes.

**Core Features**:

1. **Interactive Dashboard Mode**
   - Real-time assessment (WebSocket updates)
   - Live filtering and sorting (already have this)
   - Action buttons on each failing attribute
   - Progress tracking across multiple runs
   - Historical trend visualization

2. **One-Click Remediation Actions**
   - "Fix This" button on each failing attribute
   - Generates GitHub issue automatically
   - Creates draft PR with proposed changes
   - Links issue to PR
   - Assigns to repository owner/team

3. **Automated Fix Generation**
   - Template-based fixes for common issues:
     - Missing .gitignore â†’ Generate language-specific .gitignore
     - No CLAUDE.md â†’ Generate template with repo analysis
     - Missing pre-commit hooks â†’ Add .pre-commit-config.yaml
     - No lock files â†’ Generate appropriate lock file
     - Missing README sections â†’ Scaffold missing sections
   - AI-powered fixes for complex issues:
     - Refactor high-complexity functions
     - Add type annotations to functions
     - Generate docstrings from function signatures
     - Split large files

4. **GitHub Integration**
   - OAuth authentication with GitHub
   - gh CLI integration for seamless workflow
   - Create issues via GitHub API
   - Create draft PRs via GitHub API
   - Auto-label issues (e.g., `agentready`, `automated-fix`, `tier-1-essential`)
   - Link to AgentReady assessment report

**Technical Architecture**:

```yaml
# New components needed:

Backend (Optional - could be fully client-side with gh CLI):
  - GitHub OAuth app for authentication
  - Issue/PR template generator
  - Fix generator service (template-based + AI-powered)
  - Assessment history tracker

Frontend (Enhanced HTML report):
  - Action buttons with loading states
  - GitHub auth flow UI
  - Progress indicators
  - Toast notifications for actions
  - Modal dialogs for fix preview

CLI Extensions:
  - agentready dashboard .  # Launch local web server
  - agentready fix <attribute-id>  # Generate fix for specific attribute
  - agentready create-issue <attribute-id>  # Create GitHub issue
  - agentready create-pr <attribute-id>  # Create draft PR
```

**Use Cases**:

**Use Case 1: Quick Fixes**
```bash
# User runs assessment
agentready assess . --dashboard

# Opens dashboard in browser at http://localhost:8000
# User clicks "Fix This" on "Missing CLAUDE.md"
# â†’ Creates issue: "Add CLAUDE.md configuration file"
# â†’ Creates draft PR with generated CLAUDE.md template
# â†’ PR includes: project analysis, detected languages, suggested structure
```

**Use Case 2: Batch Remediation**
```bash
# Dashboard shows all failures
# User selects multiple attributes
# Clicks "Fix All Selected"
# â†’ Creates single issue: "Improve AgentReady Score from Silver to Gold"
# â†’ Creates draft PR with all fixes
# â†’ PR description includes before/after score projection
```

**Use Case 3: CI/CD Integration**
```bash
# GitHub Action runs assessment
# Posts comment on PR with assessment results
# Includes links to create remediation issues
# Can auto-create draft PR for improvements
```

**Implementation Approach**:

**Phase 1: Client-Side with gh CLI** (Simplest, no backend needed)
- Use JavaScript in HTML report to call gh CLI via local proxy
- Generate fix files locally
- Use `gh issue create` and `gh pr create`
- Works for users with gh CLI installed

**Phase 2: Dashboard Server** (Enhanced UX)
- Flask/FastAPI server serving dashboard
- WebSocket for live updates
- GitHub OAuth for authentication
- Background workers for fix generation

**Phase 3: Cloud Service** (SaaS offering)
- Hosted dashboard at agentready.dev
- GitHub App installation
- Webhook integration for continuous monitoring
- Team collaboration features

**Fix Templates by Attribute**:

```yaml
claude_md_file:
  type: template
  generates:
    - file: CLAUDE.md
      content: |
        # {repository_name}

        ## Overview
        {auto_generated_description}

        ## Architecture
        {detected_patterns}

        ## Development
        {build_commands}

lock_files:
  type: command
  commands:
    - condition: has_package_json
      run: npm install
    - condition: has_pyproject_toml
      run: poetry lock || pip freeze > requirements.txt

precommit_hooks:
  type: template
  generates:
    - file: .pre-commit-config.yaml
      content: {language_specific_hooks}

readme_structure:
  type: enhancement
  modifies: README.md
  adds_sections:
    - Installation
    - Usage
    - Development
    - Contributing

type_annotations:
  type: ai_powered
  uses: ast_analysis + llm
  modifies: "*.py"
  adds: type_hints
```

**Dashboard vs Report Decision**:

**Keep Both**:
- Static reports for CI/CD, documentation, archiving
- Dashboard for interactive development workflow
- Reports can link to dashboard for remediation
- Dashboard can export static reports

**Benefits of Dashboard**:
- âœ… Interactive remediation workflow
- âœ… Live assessment updates
- âœ… Progress tracking over time
- âœ… Team collaboration (comments, assignments)
- âœ… Automated fix preview before applying
- âœ… Integration with existing tools (GitHub, IDEs)

**Challenges**:
- Authentication complexity (GitHub OAuth)
- Fix generation quality (need good templates + AI)
- PR review overhead (lots of automated PRs)
- Maintaining fix templates as best practices evolve

**Recommended Approach**:

1. **Start with enhanced static report**:
   - Add "Create Issue" buttons that generate gh CLI commands
   - Users copy/paste commands to create issues
   - Include fix templates in issue descriptions

2. **Add local dashboard** (Phase 2):
   - Flask server with WebSocket updates
   - GitHub integration via gh CLI
   - Generate fixes, preview diffs, create PRs

3. **Consider hosted service** (Phase 3+):
   - If adoption is high
   - SaaS model for teams
   - Continuous monitoring and recommendations

**Related**: GitHub integration, automation, remediation, UX

**Notes**:
- This is a MAJOR feature that could become a standalone product
- Consider MVP: "Copy this gh command" buttons in HTML report
- AI-powered fix generation requires careful validation
- Some fixes (like refactoring) need human review
- Could integrate with existing tools (Dependabot, Renovate)
- May want to partner with GitHub for official integration

---


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 015-interactive-dashboard-with-automated-remediation

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: .github/coldstart-prompts/16-github-app-integration-badge-status-checks.md
````markdown
# Coldstart Implementation Prompt: GitHub App Integration (Badge & Status Checks)

**Priority**: P2
**Repository**: agentready (https://github.com/redhat/agentready)
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

GitHub App Integration (Badge & Status Checks)

**Priority**: P2 (High Value)

**Description**: Create a GitHub App that provides badge integration, PR status checks, and automated assessment comments to help Red Hat engineering teams track and improve repository quality.

**Core Features**:

1. **Repository Badge**
   - Shields.io-compatible SVG badge showing certification level
   - Endpoint: `https://agentready.redhat.com/badge/{owner}/{repo}.svg`
   - Dynamic color based on certification (Platinum=purple, Gold=yellow, Silver=silver, Bronze=brown)
   - Include score: "AgentReady: 85.2 (Gold)"
   - Click badge to view latest assessment report

2. **GitHub Actions Integration**
   - Create official `agentready/assess-action` GitHub Action
   - Run assessment on PR events (opened, synchronized, reopened)
   - Run assessment on push to main/master
   - Support custom triggers via workflow_dispatch

3. **PR Status Checks**
   - Use GitHub Commit Status API to report assessment results
   - Set check status: success (>90), warning (75-89), failure (<75)
   - Configurable thresholds via `.agentready-config.yaml`
   - Block PR merge if score below threshold (optional)
   - Link to detailed HTML report in check details

4. **PR Comments**
   - Automated bot comments on PRs with assessment summary
   - Show score delta: "Score changed: 72.4 â†’ 78.3 (+5.9)"
   - List new failures and fixes
   - Collapsible sections for full findings
   - Trend chart showing last 10 assessments (ASCII or embedded image)
   - Include remediation suggestions for new failures

**Technical Implementation**:

**Phase 1: GitHub Actions Integration**
```yaml
# .github/workflows/agentready.yml
name: AgentReady Assessment
on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches: [main, master]

jobs:
  assess:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: agentready/assess-action@v1
        with:
          threshold: 75
          post-comment: true
          update-status: true
```

**Phase 2: Badge Service**
```python
# FastAPI endpoint for badge generation
@app.get("/badge/{owner}/{repo}.svg")
async def get_badge(owner: str, repo: str):
    # Fetch latest assessment from GitHub Actions artifacts
    # Or run quick assessment on-demand
    score, level = get_latest_assessment(owner, repo)
    color = LEVEL_COLORS[level]
    return SVGResponse(generate_badge(score, level, color))
```

**Phase 3: GitHub App**
- App permissions: Contents (read), Checks (write), Pull requests (write)
- Webhook events: push, pull_request
- Installed via GitHub Marketplace (Red Hat internal)
- Dashboard at agentready.redhat.com showing:
  - Repository list with scores
  - Historical trends
  - Organization-wide statistics
  - Top repositories by improvement

**Integration Points**:

1. **GitHub Actions Artifacts**
   - Store assessment reports as workflow artifacts
   - Keep last 30 days of reports for trend analysis
   - Generate downloadable HTML/JSON/Markdown reports

2. **GitHub Status API**
   ```python
   POST /repos/{owner}/{repo}/statuses/{commit_sha}
   {
     "state": "success",  # or "pending", "failure", "error"
     "target_url": "https://agentready.redhat.com/reports/{run_id}",
     "description": "AgentReady: 85.2 (Gold)",
     "context": "agentready/assessment"
   }
   ```

3. **GitHub Checks API** (preferred over Status API)
   ```python
   POST /repos/{owner}/{repo}/check-runs
   {
     "name": "AgentReady Assessment",
     "status": "completed",
     "conclusion": "success",
     "output": {
       "title": "Score: 85.2/100 (Gold)",
       "summary": "Passed 20/25 attributes",
       "text": "Detailed findings..."
     }
   }
   ```

**Use Cases**:

**Use Case 1: Add Badge to README**
```markdown
# My Project

[![AgentReady](https://agentready.redhat.com/badge/redhat/my-project.svg)](https://agentready.redhat.com/reports/redhat/my-project)
```

**Use Case 2: Enforce Quality Gates**
```yaml
# .agentready-config.yaml
github:
  status_checks:
    enabled: true
    min_score: 75  # Block merge if score < 75
    require_improvement: true  # Block if score decreased
```

**Use Case 3: Track Organization Progress**
- Dashboard shows all repos in Red Hat org
- Filter by team, language, certification level
- Identify repos needing attention
- Celebrate improvements (score increases)

**Configuration**:

```yaml
# .agentready-config.yaml
github:
  badge:
    enabled: true
    style: flat-square  # or flat, plastic, for-the-badge
    label: "AgentReady"

  actions:
    enabled: true
    trigger_on: [pull_request, push]
    post_comment: true
    update_status: true
    upload_artifacts: true

  status_checks:
    enabled: true
    min_score: 75
    require_improvement: false

  comments:
    enabled: true
    show_delta: true
    show_trend: true
    collapse_details: true
```

**Implementation Checklist**:

- [ ] Create `agentready/assess-action` GitHub Action
- [ ] Implement badge generation service
- [ ] Add GitHub Status API integration
- [ ] Add GitHub Checks API integration
- [ ] Implement PR comment generation
- [ ] Add score delta calculation
- [ ] Create assessment artifact storage
- [ ] Build organization dashboard
- [ ] Add Red Hat SSO authentication
- [ ] Deploy to Red Hat infrastructure
- [ ] Create documentation for Red Hat teams
- [ ] Add to Red Hat developer onboarding

**Related**: CI/CD integration, automation, visibility, quality gates

**Notes**:
- Focus on internal Red Hat adoption first
- Badge service could be hosted on Red Hat infrastructure
- Dashboard should integrate with Red Hat IdM for authentication
- Consider integration with Red Hat's existing code quality tools
- GitHub App should be installable via Red Hat GitHub Enterprise
- All data stays within Red Hat infrastructure (no external services)
- Align with Red Hat's OpenShift AI strategy for agentic development
- Could become part of Red Hat's AI-assisted development workflow

---

## Backlog Metadata

**Created**: 2025-11-21
**Last Updated**: 2025-11-21
**Total Items**: 14 (11 original + 3 from code review)

## Priority Summary

- **P0 (Critical)**: 4 items - Security/Logic Bugs (FIX FIRST!), Bootstrap Command, Report Header Metadata, HTML Design Improvements
- **P1 (Critical)**: 4 items - Code Quality Fixes, Test Coverage Improvements, Align Subcommand
- **P2 (High Value)**: 3 items - Security Polish, Interactive Dashboard, GitHub App Integration
- **P3 (Important)**: 2 items - Report Schema Versioning, AgentReady Repository Agent
- **P4 (Enhancement)**: 3 items - Research Report Utility, Repomix Integration, Customizable Themes


---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/redhat/agentready.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b 016-github-app-integration-(badge-&-status-checks)

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
````

## File: .github/coldstart-prompts/README.md
````markdown
# AgentReady Coldstart Implementation Prompts

**Generated**: 2025-11-21
**Total Prompts**: 16
**Source**: BACKLOG.md

---

## Overview

This directory contains comprehensive coldstart prompts for implementing all features in the AgentReady backlog. Each prompt is a self-contained implementation guide that includes:

- Full context about AgentReady architecture
- Complete feature requirements from BACKLOG.md
- Implementation checklist with all steps
- Testing strategy and coverage requirements
- Key files to review before starting
- Getting started guide with exact commands
- Success criteria for completion

These prompts are designed to be "drop in and go" - an AI agent or developer can take any prompt and implement the feature independently without needing additional context.

---

## Priority-Based Index

### P0 (Critical) - 5 Items

| # | File | Title | Size | Status |
|---|------|-------|------|--------|
| 01 | [01-create-automated-demo.md](01-create-automated-demo.md) | Create Automated Demo | 8.7K | ðŸ”´ Not Started |
| 02 | [02-fix-critical-security-logic-bugs-from-code-review.md](02-fix-critical-security-logic-bugs-from-code-review.md) | Fix Critical Security & Logic Bugs from Code Review | 7.4K | ðŸ”´ Not Started |
| 03 | [03-bootstrap-agentready-repository-on-github.md](03-bootstrap-agentready-repository-on-github.md) | Bootstrap AgentReady Repository on GitHub | 11K | ðŸ”´ Not Started |
| 04 | [04-report-header-with-repository-metadata.md](04-report-header-with-repository-metadata.md) | Report Header with Repository Metadata | 6.8K | ðŸ”´ Not Started |
| 05 | [05-improve-html-report-design-font-size-color-scheme.md](05-improve-html-report-design-font-size-color-scheme.md) | Improve HTML Report Design (Font Size & Color Scheme) | 8.6K | ðŸ”´ Not Started |

### P1 (High) - 3 Items

| # | File | Title | Size | Status |
|---|------|-------|------|--------|
| 11 | [11-fix-code-quality-issues-from-code-review.md](11-fix-code-quality-issues-from-code-review.md) | Fix Code Quality Issues from Code Review | 6.6K | ðŸ”´ Not Started |
| 12 | [12-improve-test-coverage-and-edge-case-handling.md](12-improve-test-coverage-and-edge-case-handling.md) | Improve Test Coverage and Edge Case Handling | 6.5K | ðŸ”´ Not Started |
| 14 | [14-align-subcommand-automated-remediation.md](14-align-subcommand-automated-remediation.md) | Align Subcommand (Automated Remediation) | 12K | ðŸ”´ Not Started |

### P2 (Medium) - 3 Items

| # | File | Title | Size | Status |
|---|------|-------|------|--------|
| 13 | [13-add-security-quality-improvements-from-code-review.md](13-add-security-quality-improvements-from-code-review.md) | Add Security & Quality Improvements from Code Review | 5.7K | ðŸ”´ Not Started |
| 15 | [15-interactive-dashboard-with-automated-remediation.md](15-interactive-dashboard-with-automated-remediation.md) | Interactive Dashboard with Automated Remediation | 11K | ðŸ”´ Not Started |
| 16 | [16-github-app-integration-badge-status-checks.md](16-github-app-integration-badge-status-checks.md) | GitHub App Integration (Badge & Status Checks) | 11K | ðŸ”´ Not Started |

### P3 (Important) - 2 Items

| # | File | Title | Size | Status |
|---|------|-------|------|--------|
| 06 | [06-report-schema-versioning.md](06-report-schema-versioning.md) | Report Schema Versioning | 5.2K | ðŸ”´ Not Started |
| 09 | [09-agentready-repository-agent.md](09-agentready-repository-agent.md) | AgentReady Repository Agent | 5.8K | ðŸ”´ Not Started |

### P4 (Enhancement) - 3 Items

| # | File | Title | Size | Status |
|---|------|-------|------|--------|
| 07 | [07-research-report-generatorupdater-utility.md](07-research-report-generatorupdater-utility.md) | Research Report Generator/Updater Utility | 6.3K | ðŸ”´ Not Started |
| 08 | [08-repomix-integration.md](08-repomix-integration.md) | Repomix Integration | 5.7K | ðŸ”´ Not Started |
| 10 | [10-customizable-html-report-themes.md](10-customizable-html-report-themes.md) | Customizable HTML Report Themes | 6.1K | ðŸ”´ Not Started |

---

## Recommended Implementation Order

Based on priority and dependencies:

1. **P0 #02**: Fix Critical Security & Logic Bugs (XSS, StandardLayoutAssessor)
2. **P0 #03**: Bootstrap AgentReady Repository on GitHub (enables GitHub workflow)
3. **P0 #04**: Report Header with Repository Metadata (blocking usability)
4. **P0 #05**: Improve HTML Report Design (blocking adoption)
5. **P0 #01**: Create Automated Demo (showcase value)
6. **P1 #11**: Fix Code Quality Issues (TOCTOU, type annotations)
7. **P1 #12**: Improve Test Coverage (>80% target)
8. **P1 #14**: Align Subcommand (automated remediation)
9. **P2 #13**: Add Security & Quality Improvements (CSP, validation)
10. **P2 #15**: Interactive Dashboard (high value feature)
11. **P2 #16**: GitHub App Integration (badges, status checks)
12. **P3 #06**: Report Schema Versioning
13. **P3 #09**: AgentReady Repository Agent
14. **P4 #07**: Research Report Generator
15. **P4 #08**: Repomix Integration
16. **P4 #10**: Customizable HTML Report Themes

---

## Using These Prompts

### For AI Agents (Claude Code, etc.)

```bash
# Copy prompt content to AI agent
cat .github/coldstart-prompts/01-create-automated-demo.md

# Or open in editor
code .github/coldstart-prompts/01-create-automated-demo.md
```

### For Human Developers

1. **Pick a feature** from the priority list above
2. **Read the coldstart prompt** for complete context
3. **Create feature branch**: `git checkout -b NNN-feature-name`
4. **Follow the implementation checklist** in the prompt
5. **Run tests and linters** before committing
6. **Create PR** when complete

### Creating GitHub Issues

When the repository is on GitHub, you can create issues with:

```bash
# Generate all prompts and create GitHub issues
python scripts/backlog_to_issues.py --all --create-issues

# Create issue manually using prompt
gh issue create \
  --title "[P0] Create Automated Demo" \
  --body-file .github/coldstart-prompts/01-create-automated-demo.md \
  --label "priority:p0,enhancement"
```

---

## Prompt Structure

Each coldstart prompt follows this structure:

1. **Context** - Repository overview, structure, technologies
2. **Feature Requirements** - Complete requirements from BACKLOG.md
3. **Implementation Checklist** - Step-by-step guide
4. **Key Files to Review** - What to read before starting
5. **Testing Strategy** - Coverage requirements and test types
6. **Success Criteria** - How to know when done
7. **Getting Started** - Exact commands to begin

---

## Maintenance

**Updating Prompts**: If BACKLOG.md changes, regenerate prompts:

```bash
# Regenerate all prompts
python scripts/backlog_to_issues.py --all
```

**Tracking Progress**: Update status column in tables above as features are completed:
- ðŸ”´ Not Started
- ðŸŸ¡ In Progress
- ðŸŸ¢ Completed

---

## Statistics

- **Total Features**: 16
- **P0 (Critical)**: 5 features
- **P1 (High)**: 3 features
- **P2 (Medium)**: 3 features
- **P3 (Important)**: 2 features
- **P4 (Enhancement)**: 3 features
- **Total Prompt Size**: ~130 KB
- **Avg Prompt Size**: ~8.1 KB

---

**Last Updated**: 2025-11-21
**Generator Script**: `scripts/backlog_to_issues.py`
**Source Backlog**: `BACKLOG.md`
````

## File: docs/_config.yml
````yaml
# AgentReady Documentation Site Configuration

# Site settings
title: AgentReady
tagline: Repository Quality Assessment for AI-Assisted Development
description: >-
  Assess repositories against 25 evidence-based attributes that make codebases
  more effective for AI-assisted development. Generate actionable reports with
  specific remediation guidance.

# Repository information
repository: yourusername/agentready
github_username: yourusername

# Build settings
theme: jekyll-theme-minimal
markdown: kramdown
kramdown:
  input: GFM
  syntax_highlighter: rouge
  syntax_highlighter_opts:
    default_lang: python
    css_class: 'highlight'

# Plugins
plugins:
  - jekyll-feed
  - jekyll-seo-tag
  - jekyll-sitemap

# Navigation
navigation:
  - title: Home
    url: /
  - title: User Guide
    url: /user-guide
  - title: Developer Guide
    url: /developer-guide
  - title: Attributes
    url: /attributes
  - title: API Reference
    url: /api-reference
  - title: Examples
    url: /examples

# Collections
collections:
  attributes:
    output: true
    permalink: /attributes/:name

# Defaults
defaults:
  - scope:
      path: ""
      type: "pages"
    values:
      layout: "default"
  - scope:
      path: ""
      type: "attributes"
    values:
      layout: "attribute"

# Exclude from build
exclude:
  - Gemfile
  - Gemfile.lock
  - node_modules
  - vendor
  - .sass-cache
  - .jekyll-cache
  - README.md

# Site metadata
version: 1.0.0
certification_levels:
  platinum:
    range: "90-100"
    color: "#e5e4e2"
    emoji: "ðŸ†"
  gold:
    range: "75-89"
    color: "#ffd700"
    emoji: "ðŸ¥‡"
  silver:
    range: "60-74"
    color: "#c0c0c0"
    emoji: "ðŸ¥ˆ"
  bronze:
    range: "40-59"
    color: "#cd7f32"
    emoji: "ðŸ¥‰"
  needs_improvement:
    range: "0-39"
    color: "#8b4513"
    emoji: "ðŸ“ˆ"

# URLs
baseurl: "" # the subpath of your site, e.g. /blog
url: "" # the base hostname & protocol for your site

# Analytics (optional - uncomment and add your ID)
# google_analytics: UA-XXXXXXXXX-X
````

## File: scripts/backlog_to_issues.py
````python
#!/usr/bin/env python3
"""
Script to convert BACKLOG.md items into coldstart prompts.

This script:
1. Parses BACKLOG.md to extract individual items
2. Generates comprehensive coldstart prompts for each item
3. Saves prompts as markdown files in .github/coldstart-prompts/
4. Optionally creates GitHub issues via gh CLI (if --create-issues flag set)
5. Pauses after first item for user review
"""

import re
import subprocess
import sys
from pathlib import Path
from typing import List, Dict, Optional
import json


class BacklogItem:
    """Represents a single backlog item."""

    def __init__(self, title: str, priority: str, content: str, section_start: int):
        self.title = title
        self.priority = priority
        self.content = content
        self.section_start = section_start

    def __repr__(self):
        return f"BacklogItem(title={self.title}, priority={self.priority})"


def parse_backlog(backlog_path: Path) -> List[BacklogItem]:
    """Parse BACKLOG.md and extract all items."""

    with open(backlog_path, 'r') as f:
        content = f.read()

    items = []

    # Split into sections by ###
    sections = re.split(r'\n### ', content)

    for i, section in enumerate(sections[1:], 1):  # Skip first section (header)
        lines = section.split('\n')
        title = lines[0].strip()

        # Find priority in next few lines
        priority = "P4"  # default
        for line in lines[1:5]:
            if match := re.search(r'\*\*Priority\*\*:\s*(P\d)', line):
                priority = match.group(1)
                break

        # Get full content until next ### or end
        full_content = '\n'.join(lines)

        items.append(BacklogItem(
            title=title,
            priority=priority,
            content=full_content,
            section_start=i
        ))

    return items


def generate_coldstart_prompt(item: BacklogItem, repo_context: Dict) -> str:
    """Generate a comprehensive coldstart prompt for implementing the backlog item."""

    prompt = f"""# Coldstart Implementation Prompt: {item.title}

**Priority**: {item.priority}
**Repository**: agentready (https://github.com/{repo_context['owner']}/{repo_context['repo']})
**Branch Strategy**: Create feature branch from main

---

## Context

You are implementing a feature for AgentReady, a repository quality assessment tool for AI-assisted development.

### Repository Structure
```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”œâ”€â”€ services/            # Scanner orchestration
â”‚   â”œâ”€â”€ assessors/           # Attribute assessments
â”‚   â”œâ”€â”€ reporters/           # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ templates/           # Jinja2 templates
â”‚   â””â”€â”€ cli/                 # Click-based CLI
â”œâ”€â”€ tests/                   # Test suite (unit + integration)
â”œâ”€â”€ examples/                # Example reports
â””â”€â”€ specs/                   # Feature specifications
```

### Key Technologies
- Python 3.11+
- Click (CLI framework)
- Jinja2 (templating)
- Pytest (testing)
- Black, isort, ruff (code quality)

### Development Workflow
1. Create feature branch: `git checkout -b NNN-feature-name`
2. Implement changes with tests
3. Run linters: `black . && isort . && ruff check .`
4. Run tests: `pytest`
5. Commit with conventional commits
6. Create PR to main

---

## Feature Requirements

{item.content}

---

## Implementation Checklist

Before you begin:
- [ ] Read CLAUDE.md for project context
- [ ] Review existing similar features (if applicable)
- [ ] Understand the data model (src/agentready/models/)
- [ ] Check acceptance criteria in feature description

Implementation steps:
- [ ] Create feature branch
- [ ] Implement core functionality
- [ ] Add unit tests (target >80% coverage)
- [ ] Add integration tests (if applicable)
- [ ] Run linters and fix any issues
- [ ] Update documentation (README.md, CLAUDE.md if needed)
- [ ] Self-test the feature end-to-end
- [ ] Create PR with descriptive title and body

Code quality requirements:
- [ ] All code formatted with black (88 char lines)
- [ ] Imports sorted with isort
- [ ] No ruff violations
- [ ] All tests passing
- [ ] Type hints where appropriate
- [ ] Docstrings for public APIs

---

## Key Files to Review

Based on this feature, you should review:
- `src/agentready/models/` - Understand Assessment, Finding, Attribute models
- `src/agentready/services/scanner.py` - Scanner orchestration
- `src/agentready/assessors/base.py` - BaseAssessor pattern
- `src/agentready/reporters/` - Report generation
- `CLAUDE.md` - Project overview and guidelines
- `BACKLOG.md` - Full context of this feature

---

## Testing Strategy

For this feature, ensure:
1. **Unit tests** for core logic (80%+ coverage)
2. **Integration tests** for end-to-end workflows
3. **Edge case tests** (empty inputs, missing files, errors)
4. **Error handling tests** (graceful degradation)

Run tests:
```bash
# All tests
pytest

# With coverage
pytest --cov=src/agentready --cov-report=html

# Specific test file
pytest tests/unit/test_feature.py -v
```

---

## Success Criteria

This feature is complete when:
- âœ… All acceptance criteria from feature description are met
- âœ… Tests passing with >80% coverage for new code
- âœ… All linters passing (black, isort, ruff)
- âœ… Documentation updated
- âœ… PR created with clear description
- âœ… Self-tested end-to-end

---

## Questions to Clarify (if needed)

If anything is unclear during implementation:
1. Check CLAUDE.md for project patterns
2. Review similar existing features
3. Ask for clarification in PR comments
4. Reference the original backlog item

---

## Getting Started

```bash
# Clone and setup
git clone https://github.com/{repo_context['owner']}/{repo_context['repo']}.git
cd agentready

# Create virtual environment
uv venv && source .venv/bin/activate

# Install dependencies
uv pip install -e .
uv pip install pytest black isort ruff

# Create feature branch
git checkout -b {item.section_start:03d}-{item.title.lower().replace(' ', '-')[:50]}

# Start implementing!
```

---

**Note**: This is a coldstart prompt. You have all context needed to implement this feature independently. Read the linked files, follow the patterns, and deliver high-quality code with tests.
"""

    return prompt


def create_github_issue(item: BacklogItem, prompt: str, repo_context: Dict, dry_run: bool = False) -> Optional[str]:
    """Create GitHub issue via gh CLI and attach coldstart prompt as comment."""

    # Prepare issue title
    issue_title = f"[{item.priority}] {item.title}"

    # Prepare issue body (extract description and requirements)
    # Get content up to "Implementation" section or similar
    body_parts = []
    body_parts.append(f"**Priority**: {item.priority}\n")

    # Extract description (first paragraph after Priority)
    lines = item.content.split('\n')
    in_description = False
    description_lines = []

    for line in lines:
        if '**Description**:' in line:
            in_description = True
            continue
        if in_description:
            if line.startswith('**') and ':' in line:
                break
            description_lines.append(line)

    if description_lines:
        body_parts.append("## Description\n")
        body_parts.append('\n'.join(description_lines))

    # Add link to full context
    body_parts.append("\n\n## Full Context\n")
    body_parts.append(f"See [BACKLOG.md](https://github.com/{repo_context['owner']}/{repo_context['repo']}/blob/main/BACKLOG.md) for complete requirements.\n")

    # Add acceptance criteria if present
    if '**Acceptance Criteria**:' in item.content:
        criteria_start = item.content.find('**Acceptance Criteria**:')
        criteria_section = item.content[criteria_start:criteria_start+1000]
        body_parts.append("\n## Acceptance Criteria\n")
        body_parts.append(criteria_section.split('\n\n')[0])

    issue_body = '\n'.join(body_parts)

    # Determine labels
    labels = [f"priority:{item.priority.lower()}"]

    # Add category labels based on title/content
    if 'security' in item.title.lower() or 'xss' in item.content.lower():
        labels.append('security')
    if 'bug' in item.title.lower() or 'fix' in item.title.lower():
        labels.append('bug')
    else:
        labels.append('enhancement')
    if 'test' in item.title.lower():
        labels.append('testing')
    if 'github' in item.title.lower():
        labels.append('github-integration')
    if 'report' in item.title.lower():
        labels.append('reporting')

    labels_str = ','.join(labels)

    if dry_run:
        print(f"\n{'='*80}")
        print(f"DRY RUN: Would create issue:")
        print(f"Title: {issue_title}")
        print(f"Labels: {labels_str}")
        print(f"Body preview:\n{issue_body[:500]}...")
        print(f"\nColdstart prompt would be added as first comment")
        print(f"{'='*80}\n")
        return None

    # Create issue using gh CLI
    try:
        # Create the issue
        result = subprocess.run(
            [
                'gh', 'issue', 'create',
                '--title', issue_title,
                '--body', issue_body,
                '--label', labels_str
            ],
            capture_output=True,
            text=True,
            check=True
        )

        issue_url = result.stdout.strip()
        print(f"âœ… Created issue: {issue_url}")

        # Extract issue number from URL
        issue_number = issue_url.split('/')[-1]

        # Add coldstart prompt as first comment
        subprocess.run(
            [
                'gh', 'issue', 'comment', issue_number,
                '--body', f"## ðŸ¤– Coldstart Implementation Prompt\n\n{prompt}"
            ],
            capture_output=True,
            text=True,
            check=True
        )

        print(f"âœ… Added coldstart prompt as comment")

        return issue_url

    except subprocess.CalledProcessError as e:
        print(f"âŒ Failed to create issue: {e.stderr}")
        return None


def get_repo_context() -> Dict:
    """Get repository context (owner, repo name) from git remote."""
    try:
        result = subprocess.run(
            ['gh', 'repo', 'view', '--json', 'owner,name'],
            capture_output=True,
            text=True,
            check=True
        )
        data = json.loads(result.stdout)
        return {
            'owner': data['owner']['login'],
            'repo': data['name']
        }
    except Exception as e:
        # No git remote - ask user or use default
        print(f"âš ï¸  Warning: Could not get repo context from git remote")
        print(f"    This is expected if repository not yet on GitHub")
        print(f"    Using default values for now\n")
        # For agentready, we know the intended location
        return {'owner': 'redhat', 'repo': 'agentready'}


def save_prompt_to_file(item: BacklogItem, prompt: str, output_dir: Path, item_number: int) -> Path:
    """Save coldstart prompt to markdown file."""

    # Create output directory if it doesn't exist
    output_dir.mkdir(parents=True, exist_ok=True)

    # Generate filename from item number and title
    safe_title = re.sub(r'[^\w\s-]', '', item.title.lower())
    safe_title = re.sub(r'[-\s]+', '-', safe_title)[:50]
    filename = f"{item_number:02d}-{safe_title}.md"

    filepath = output_dir / filename

    # Write prompt to file
    with open(filepath, 'w') as f:
        f.write(prompt)

    return filepath


def main():
    """Main script execution."""

    # Parse command line args
    create_issues = '--create-issues' in sys.argv
    process_all = '--all' in sys.argv

    # Get repository root
    repo_root = Path(__file__).parent.parent
    backlog_path = repo_root / 'BACKLOG.md'

    if not backlog_path.exists():
        print(f"âŒ BACKLOG.md not found at {backlog_path}")
        sys.exit(1)

    # Create output directory
    output_dir = repo_root / '.github' / 'coldstart-prompts'

    # Get repo context
    repo_context = get_repo_context()
    print(f"ðŸ“¦ Repository: {repo_context['owner']}/{repo_context['repo']}\n")

    # Parse backlog
    print("ðŸ“– Parsing BACKLOG.md...")
    items = parse_backlog(backlog_path)
    print(f"Found {len(items)} backlog items\n")

    # Show summary
    print("Backlog Items:")
    for i, item in enumerate(items, 1):
        print(f"  {i:2d}. [{item.priority}] {item.title}")
    print()

    # Process first item (or all if --all flag)
    items_to_process = items if process_all else [items[0]]

    for idx, item in enumerate(items_to_process, 1):
        print(f"{'='*80}")
        print(f"Processing item {idx}/{len(items_to_process)}: {item.title}")
        print(f"{'='*80}\n")

        # Generate coldstart prompt
        print("ðŸ¤– Generating coldstart prompt...")
        prompt = generate_coldstart_prompt(item, repo_context)
        print(f"âœ… Generated {len(prompt)} character prompt\n")

        # Save to file
        print("ðŸ’¾ Saving prompt to file...")
        filepath = save_prompt_to_file(item, prompt, output_dir, item.section_start)
        print(f"âœ… Saved to: {filepath}\n")

        # Optionally create GitHub issue
        if create_issues:
            print("ðŸ™ Creating GitHub issue...")
            issue_url = create_github_issue(item, prompt, repo_context, dry_run=False)
            if issue_url:
                print(f"âœ… Created issue: {issue_url}\n")
            else:
                print(f"âŒ Failed to create issue\n")

        # Pause after first item unless --all specified
        if not process_all and idx == 1:
            print(f"\n{'='*80}")
            print(f"âœ… FIRST PROMPT GENERATED")
            print(f"{'='*80}\n")
            print(f"Saved to: {filepath}")
            print(f"\nPlease review the prompt file.")
            print(f"Once approved, run with --all to process remaining {len(items) - 1} items:")
            print(f"  python scripts/backlog_to_issues.py --all")
            if not create_issues:
                print(f"\nTo also create GitHub issues, add --create-issues flag:")
                print(f"  python scripts/backlog_to_issues.py --all --create-issues")
            return

    # All items processed
    print(f"\n{'='*80}")
    print(f"âœ… PROCESSED {len(items_to_process)} ITEMS")
    print(f"{'='*80}\n")
    print(f"Coldstart prompts saved to: {output_dir}/")
    if create_issues:
        print(f"GitHub issues created (check repository)")
    print(f"\nNext steps:")
    print(f"  1. Review generated prompts in {output_dir}/")
    print(f"  2. Create GitHub issues manually, or run with --create-issues")
    print(f"  3. Start implementing features using the coldstart prompts!")


if __name__ == '__main__':
    main()
````

## File: src/agentready/cli/bootstrap.py
````python
"""Bootstrap command for setting up GitHub infrastructure."""

import sys
from pathlib import Path

import click

from ..services.bootstrap import BootstrapGenerator


@click.command()
@click.argument("repository", type=click.Path(exists=True), default=".")
@click.option(
    "--dry-run",
    is_flag=True,
    help="Preview changes without creating files",
)
@click.option(
    "--language",
    type=click.Choice(["python", "javascript", "go", "auto"], case_sensitive=False),
    default="auto",
    help="Primary language (default: auto-detect)",
)
def bootstrap(repository, dry_run, language):
    """Bootstrap repository with GitHub infrastructure and best practices.

    Creates:
    - GitHub Actions workflows (tests, AgentReady assessment, security)
    - GitHub templates (issues, pull requests, CODEOWNERS)
    - Pre-commit hooks configuration
    - Dependabot configuration
    - Contributing guidelines

    REPOSITORY: Path to git repository (default: current directory)
    """
    repo_path = Path(repository).resolve()

    # Validate git repository
    if not (repo_path / ".git").exists():
        click.echo("Error: Not a git repository", err=True)
        sys.exit(1)

    click.echo("ðŸ¤– AgentReady Bootstrap")
    click.echo("=" * 50)
    click.echo(f"\nRepository: {repo_path}")
    click.echo(f"Language: {language}")
    click.echo(f"Dry run: {dry_run}\n")

    # Create generator
    generator = BootstrapGenerator(repo_path, language)

    # Generate all files
    try:
        created_files = generator.generate_all(dry_run=dry_run)
    except Exception as e:
        click.echo(f"\nError during bootstrap: {str(e)}", err=True)
        sys.exit(1)

    # Report results
    click.echo("\n" + "=" * 50)
    if dry_run:
        click.echo("\nDry run complete! The following files would be created:")
    else:
        click.echo(f"\nBootstrap complete! Created {len(created_files)} files:")

    for file_path in sorted(created_files):
        rel_path = file_path.relative_to(repo_path)
        click.echo(f"  âœ“ {rel_path}")

    if not dry_run:
        click.echo("\nâœ… Repository bootstrapped successfully!")
        click.echo("\nNext steps:")
        click.echo("  1. Review generated files")
        click.echo("  2. Commit changes: git add . && git commit -m 'chore: Bootstrap repository infrastructure'")
        click.echo("  3. Push to GitHub: git push")
        click.echo("  4. Set up branch protection rules")
        click.echo("  5. Enable GitHub Actions")
    else:
        click.echo("\nRun without --dry-run to create files")
````

## File: src/agentready/services/bootstrap.py
````python
"""Bootstrap generator for repository infrastructure."""

from pathlib import Path
from typing import List

from jinja2 import Environment, PackageLoader, select_autoescape

from .language_detector import LanguageDetector


class BootstrapGenerator:
    """Generates GitHub infrastructure files for repository."""

    def __init__(self, repo_path: Path, language: str = "auto"):
        """Initialize bootstrap generator.

        Args:
            repo_path: Path to repository
            language: Primary language or "auto" to detect
        """
        self.repo_path = repo_path
        self.language = self._detect_language(language)
        self.env = Environment(
            loader=PackageLoader("agentready", "templates/bootstrap"),
            autoescape=select_autoescape(["html", "xml", "j2", "yaml", "yml"]),
            trim_blocks=True,
            lstrip_blocks=True,
        )

    def _detect_language(self, language: str) -> str:
        """Detect primary language if auto."""
        if language != "auto":
            return language.lower()

        # Use language detector
        detector = LanguageDetector(self.repo_path)
        languages = detector.detect_languages()

        if not languages:
            return "python"  # Default fallback

        # Return most used language
        return max(languages, key=languages.get).lower()

    def generate_all(self, dry_run: bool = False) -> List[Path]:
        """Generate all bootstrap files.

        Args:
            dry_run: If True, don't create files, just return paths

        Returns:
            List of created file paths
        """
        created_files = []

        # GitHub Actions workflows
        created_files.extend(self._generate_workflows(dry_run))

        # GitHub templates
        created_files.extend(self._generate_github_templates(dry_run))

        # Pre-commit hooks
        created_files.extend(self._generate_precommit_config(dry_run))

        # Dependabot
        created_files.extend(self._generate_dependabot(dry_run))

        # Contributing guidelines
        created_files.extend(self._generate_docs(dry_run))

        return created_files

    def _generate_workflows(self, dry_run: bool) -> List[Path]:
        """Generate GitHub Actions workflows."""
        workflows_dir = self.repo_path / ".github" / "workflows"
        created = []

        # AgentReady assessment workflow
        agentready_workflow = workflows_dir / "agentready-assessment.yml"
        template = self.env.get_template("workflows/agentready-assessment.yml.j2")
        content = template.render(language=self.language)
        created.append(self._write_file(agentready_workflow, content, dry_run))

        # Tests workflow
        tests_workflow = workflows_dir / "tests.yml"
        template = self.env.get_template(f"workflows/tests-{self.language}.yml.j2")
        content = template.render()
        created.append(self._write_file(tests_workflow, content, dry_run))

        # Security workflow
        security_workflow = workflows_dir / "security.yml"
        template = self.env.get_template("workflows/security.yml.j2")
        content = template.render(language=self.language)
        created.append(self._write_file(security_workflow, content, dry_run))

        return created

    def _generate_github_templates(self, dry_run: bool) -> List[Path]:
        """Generate GitHub issue and PR templates."""
        created = []

        # Issue templates
        issue_template_dir = self.repo_path / ".github" / "ISSUE_TEMPLATE"

        bug_template = issue_template_dir / "bug_report.md"
        template = self.env.get_template("issue_templates/bug_report.md.j2")
        content = template.render()
        created.append(self._write_file(bug_template, content, dry_run))

        feature_template = issue_template_dir / "feature_request.md"
        template = self.env.get_template("issue_templates/feature_request.md.j2")
        content = template.render()
        created.append(self._write_file(feature_template, content, dry_run))

        # PR template
        pr_template = self.repo_path / ".github" / "PULL_REQUEST_TEMPLATE.md"
        template = self.env.get_template("PULL_REQUEST_TEMPLATE.md.j2")
        content = template.render()
        created.append(self._write_file(pr_template, content, dry_run))

        # CODEOWNERS
        codeowners = self.repo_path / ".github" / "CODEOWNERS"
        template = self.env.get_template("CODEOWNERS.j2")
        content = template.render()
        created.append(self._write_file(codeowners, content, dry_run))

        return created

    def _generate_precommit_config(self, dry_run: bool) -> List[Path]:
        """Generate pre-commit hooks configuration."""
        precommit_file = self.repo_path / ".pre-commit-config.yaml"
        template = self.env.get_template(f"precommit-{self.language}.yaml.j2")
        content = template.render()
        return [self._write_file(precommit_file, content, dry_run)]

    def _generate_dependabot(self, dry_run: bool) -> List[Path]:
        """Generate Dependabot configuration."""
        dependabot_file = self.repo_path / ".github" / "dependabot.yml"
        template = self.env.get_template("dependabot.yml.j2")
        content = template.render(language=self.language)
        return [self._write_file(dependabot_file, content, dry_run)]

    def _generate_docs(self, dry_run: bool) -> List[Path]:
        """Generate contributing guidelines and code of conduct."""
        created = []

        # CONTRIBUTING.md
        contributing = self.repo_path / "CONTRIBUTING.md"
        if not contributing.exists():  # Don't overwrite existing
            template = self.env.get_template("CONTRIBUTING.md.j2")
            content = template.render(language=self.language)
            created.append(self._write_file(contributing, content, dry_run))

        # CODE_OF_CONDUCT.md (Red Hat standard)
        code_of_conduct = self.repo_path / "CODE_OF_CONDUCT.md"
        if not code_of_conduct.exists():
            template = self.env.get_template("CODE_OF_CONDUCT.md.j2")
            content = template.render()
            created.append(self._write_file(code_of_conduct, content, dry_run))

        return created

    def _write_file(self, path: Path, content: str, dry_run: bool) -> Path:
        """Write file to disk or simulate for dry run."""
        if dry_run:
            return path

        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            f.write(content)

        return path
````

## File: src/agentready/templates/bootstrap/issue_templates/bug_report.md.j2
````
---
name: Bug Report
about: Create a report to help us improve
title: '[BUG] '
labels: bug
assignees: ''
---

## Bug Description

A clear and concise description of what the bug is.

## To Reproduce

Steps to reproduce the behavior:

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

## Expected Behavior

A clear and concise description of what you expected to happen.

## Actual Behavior

A clear and concise description of what actually happened.

## Environment

- OS: [e.g. macOS 14.0, Ubuntu 22.04]
- Version: [e.g. 1.0.0]
- Python Version: [e.g. 3.11]

## Additional Context

Add any other context about the problem here. Include screenshots if applicable.

## Possible Solution

If you have suggestions on how to fix the bug, please describe them here.
````

## File: src/agentready/templates/bootstrap/issue_templates/feature_request.md.j2
````
---
name: Feature Request
about: Suggest an idea for this project
title: '[FEATURE] '
labels: enhancement
assignees: ''
---

## Problem Statement

A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

## Proposed Solution

A clear and concise description of what you want to happen.

## Alternatives Considered

A clear and concise description of any alternative solutions or features you've considered.

## Use Cases

Describe specific use cases where this feature would be valuable.

1. Use case 1
2. Use case 2
3. Use case 3

## Additional Context

Add any other context, screenshots, or mockups about the feature request here.

## Acceptance Criteria

- [ ] Criterion 1
- [ ] Criterion 2
- [ ] Criterion 3

## Priority

How important is this feature to you?

- [ ] Critical (blocking my work)
- [ ] High (significantly improves workflow)
- [ ] Medium (nice to have)
- [ ] Low (minor enhancement)
````

## File: src/agentready/templates/bootstrap/workflows/agentready-assessment.yml.j2
````
name: AgentReady Assessment

on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches: [main, master]
  workflow_dispatch:

jobs:
  assess:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install AgentReady
        run: |
          pip install agentready

      - name: Run AgentReady Assessment
        run: |
          agentready assess . --verbose

      - name: Upload Assessment Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: agentready-reports
          path: .agentready/
          retention-days: 30

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const reportPath = '.agentready/report-latest.md';

            if (!fs.existsSync(reportPath)) {
              console.log('No report found');
              return;
            }

            const report = fs.readFileSync(reportPath, 'utf8');

            // Post comment with assessment results
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
````

## File: src/agentready/templates/bootstrap/workflows/security.yml.j2
````
name: Security

on:
  push:
    branches: [main, master]
  pull_request:
  schedule:
    - cron: '0 0 * * 0'  # Weekly on Sunday
  workflow_dispatch:

permissions:
  contents: read
  security-events: write

jobs:
  codeql:
    name: CodeQL Analysis
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: {{ 'python' if language == 'python' else 'javascript' }}

      - name: Autobuild
        uses: github/codeql-action/autobuild@v3

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3

{% if language == 'python' %}
  safety:
    name: Dependency Security Scan
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install safety

      - name: Run safety check
        run: |
          safety check --json || true
{% endif %}
````

## File: src/agentready/templates/bootstrap/workflows/tests-python.yml.j2
````
name: Tests

on:
  pull_request:
  push:
    branches: [main, master]
  workflow_dispatch:

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.11', '3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python {{ '${{ matrix.python-version }}' }}
        uses: actions/setup-python@v5
        with:
          python-version: {{ '${{ matrix.python-version }}' }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run black
        run: |
          black --check .

      - name: Run isort
        run: |
          isort --check .

      - name: Run ruff
        run: |
          ruff check .

      - name: Run pytest
        run: |
          pytest --cov=src --cov-report=xml --cov-report=term

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        if: matrix.python-version == '3.11'
        with:
          files: ./coverage.xml
          fail_ci_if_error: false
````

## File: src/agentready/templates/bootstrap/CODE_OF_CONDUCT.md.j2
````
# Code of Conduct

## Our Commitment

We are committed to providing a welcoming and inclusive environment for all contributors.

## Standards

Examples of behavior that contributes to a positive environment:
- Using welcoming and inclusive language
- Being respectful of differing viewpoints
- Accepting constructive criticism gracefully
- Focusing on what is best for the community

Examples of unacceptable behavior:
- Trolling or insulting comments
- Public or private harassment
- Publishing others' private information
- Other conduct which could reasonably be considered inappropriate

## Responsibilities

Project maintainers are responsible for clarifying standards of acceptable behavior and will take appropriate action in response to unacceptable behavior.

## Scope

This Code of Conduct applies within all project spaces and when representing the project in public spaces.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the project team. All complaints will be reviewed and investigated promptly and fairly.

## Attribution

This Code of Conduct is adapted from common open source community standards.
````

## File: src/agentready/templates/bootstrap/CODEOWNERS.j2
````
# CODEOWNERS file
# Define code ownership for automatic review requests

# Global owners - these users are requested for review on all PRs
# *       @owner

# Specific paths can have different owners
# /docs/  @documentation-team
# /src/   @development-team
# /.github/ @devops-team

# Default: assign to repository owner
*       @owner
````

## File: src/agentready/templates/bootstrap/CONTRIBUTING.md.j2
````
# Contributing Guidelines

Thank you for your interest in contributing to this project! This document provides guidelines and instructions for contributing.

## Code of Conduct

Please read and follow our [Code of Conduct](CODE_OF_CONDUCT.md).

## Getting Started

### Prerequisites

{% if language == 'python' %}
- Python 3.11 or higher
- pip or uv for package management
- Git
{% elif language == 'javascript' %}
- Node.js 18 or higher
- npm or yarn
- Git
{% elif language == 'go' %}
- Go 1.21 or higher
- Git
{% endif %}

### Development Setup

1. Fork the repository
2. Clone your fork:
   ```bash
   git clone https://github.com/YOUR_USERNAME/PROJECT_NAME.git
   cd PROJECT_NAME
   ```

3. Set up your development environment:
{% if language == 'python' %}
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   pip install -e ".[dev]"
   ```
{% elif language == 'javascript' %}
   ```bash
   npm install
   ```
{% elif language == 'go' %}
   ```bash
   go mod download
   ```
{% endif %}

4. Install pre-commit hooks:
   ```bash
   pre-commit install
   ```

## Development Workflow

### Creating a Feature Branch

Always create a new branch for your work:

```bash
git checkout -b feature/your-feature-name
```

Use descriptive branch names:
- `feature/add-new-api`
- `fix/resolve-crash-on-startup`
- `docs/improve-installation-guide`

### Making Changes

1. Write clear, concise code
2. Follow the project's coding standards
3. Add tests for new functionality
4. Update documentation as needed
5. Run linters and tests before committing

### Running Tests

{% if language == 'python' %}
```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=term

# Run specific test file
pytest tests/test_specific.py
```
{% elif language == 'javascript' %}
```bash
# Run all tests
npm test

# Run with coverage
npm run test:coverage

# Run specific test
npm test -- tests/specific.test.js
```
{% elif language == 'go' %}
```bash
# Run all tests
go test ./...

# Run with coverage
go test -cover ./...

# Run specific test
go test -run TestSpecific ./...
```
{% endif %}

### Code Style

{% if language == 'python' %}
This project uses:
- **black** for code formatting
- **isort** for import sorting
- **ruff** for linting

Run formatters before committing:

```bash
black .
isort .
ruff check . --fix
```

Pre-commit hooks will automatically run these tools.
{% elif language == 'javascript' %}
This project uses:
- **Prettier** for code formatting
- **ESLint** for linting

Run formatters before committing:

```bash
npm run format
npm run lint
```
{% elif language == 'go' %}
This project uses:
- **gofmt** for code formatting
- **golangci-lint** for linting

Run formatters before committing:

```bash
go fmt ./...
golangci-lint run
```
{% endif %}

### Commit Messages

We use [Conventional Commits](https://www.conventionalcommits.org/):

```
<type>(<scope>): <subject>

<body>

<footer>
```

Types:
- `feat`: New feature
- `fix`: Bug fix
- `docs`: Documentation changes
- `style`: Code style changes (formatting, etc.)
- `refactor`: Code refactoring
- `test`: Adding or updating tests
- `chore`: Maintenance tasks

Examples:
```
feat(api): add user authentication endpoint

fix(parser): resolve crash when parsing empty files

docs(readme): update installation instructions
```

### Pull Request Process

1. **Update your branch** with the latest changes:
   ```bash
   git fetch upstream
   git rebase upstream/main
   ```

2. **Push your changes**:
   ```bash
   git push origin feature/your-feature-name
   ```

3. **Create a Pull Request**:
   - Use the PR template
   - Provide a clear description
   - Link related issues
   - Request reviews from maintainers

4. **Address feedback**:
   - Respond to review comments
   - Make requested changes
   - Push updates to your branch

5. **Wait for approval**:
   - At least one maintainer approval required
   - All CI checks must pass
   - No merge conflicts

## Testing Guidelines

- Write tests for all new functionality
- Maintain or improve code coverage
- Include both positive and negative test cases
- Test edge cases and error conditions

## Documentation

- Update README.md for user-facing changes
- Add docstrings to new functions and classes
- Update API documentation if applicable
- Add examples for new features

## Issue Reporting

When reporting issues, please include:

- Clear, descriptive title
- Steps to reproduce
- Expected vs. actual behavior
- Environment details (OS, version, etc.)
- Error messages or logs
- Screenshots if applicable

## Questions?

If you have questions:
- Check existing issues and discussions
- Ask in pull request comments
- Open a new discussion
- Contact maintainers

## License

By contributing, you agree that your contributions will be licensed under the same license as the project.

Thank you for contributing! ðŸŽ‰
````

## File: src/agentready/templates/bootstrap/dependabot.yml.j2
````
version: 2
updates:
{% if language == 'python' %}
  - package-ecosystem: "pip"
    directory: "/"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 10
    labels:
      - "dependencies"
      - "python"
{% elif language == 'javascript' %}
  - package-ecosystem: "npm"
    directory: "/"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 10
    labels:
      - "dependencies"
      - "javascript"
{% elif language == 'go' %}
  - package-ecosystem: "gomod"
    directory: "/"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 10
    labels:
      - "dependencies"
      - "go"
{% endif %}

  - package-ecosystem: "github-actions"
    directory: "/"
    schedule:
      interval: "weekly"
    labels:
      - "dependencies"
      - "github-actions"
````

## File: src/agentready/templates/bootstrap/precommit-python.yaml.j2
````
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-merge-conflict
      - id: check-toml
      - id: check-json
      - id: detect-private-key

  - repo: https://github.com/psf/black
    rev: 24.1.1
    hooks:
      - id: black
        language_version: python3.11

  - repo: https://github.com/pycqa/isort
    rev: 5.13.2
    hooks:
      - id: isort
        args: ["--profile", "black"]

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.14
    hooks:
      - id: ruff
        args: ["--fix"]

  - repo: https://github.com/compilerla/conventional-pre-commit
    rev: v3.0.0
    hooks:
      - id: conventional-pre-commit
        stages: [commit-msg]
````

## File: src/agentready/templates/bootstrap/PULL_REQUEST_TEMPLATE.md.j2
````
## Description

<!-- Provide a brief description of the changes in this PR -->

## Type of Change

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] Documentation update
- [ ] Refactoring (no functional changes)
- [ ] Performance improvement
- [ ] Test coverage improvement

## Related Issues

<!-- Link related issues here using #issue_number -->

Fixes #
Relates to #

## Changes Made

<!-- Detailed list of changes made in this PR -->

-
-
-

## Testing

<!-- Describe the tests you ran to verify your changes -->

- [ ] Unit tests pass (`pytest`)
- [ ] Integration tests pass
- [ ] Manual testing performed
- [ ] No new warnings or errors

## Checklist

- [ ] My code follows the project's code style
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] Any dependent changes have been merged and published

## Screenshots (if applicable)

<!-- Add screenshots to help explain your changes -->

## Additional Notes

<!-- Any additional information that reviewers should know -->
````

## File: .repomixignore
````
# Add patterns to ignore here, one per line
# Example:
# *.log
# tmp/
````

## File: repomix.config.json
````json
{
  "$schema": "https://repomix.com/schemas/latest/schema.json",
  "input": {
    "maxFileSize": 52428800
  },
  "output": {
    "filePath": "repomix-output.md",
    "style": "markdown",
    "parsableStyle": false,
    "fileSummary": true,
    "directoryStructure": true,
    "files": true,
    "removeComments": false,
    "removeEmptyLines": false,
    "compress": false,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "truncateBase64": false,
    "copyToClipboard": false,
    "includeFullDirectoryStructure": false,
    "tokenCountTree": false,
    "git": {
      "sortByChanges": true,
      "sortByChangesMaxCommits": 100,
      "includeDiffs": false,
      "includeLogs": false,
      "includeLogsCount": 50
    }
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDotIgnore": true,
    "useDefaultPatterns": true,
    "customPatterns": []
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}
````

## File: .claude/commands/speckit.analyze.md
````markdown
---
description: Perform a non-destructive cross-artifact consistency and quality analysis across spec.md, plan.md, and tasks.md after task generation.
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Goal

Identify inconsistencies, duplications, ambiguities, and underspecified items across the three core artifacts (`spec.md`, `plan.md`, `tasks.md`) before implementation. This command MUST run only after `/speckit.tasks` has successfully produced a complete `tasks.md`.

## Operating Constraints

**STRICTLY READ-ONLY**: Do **not** modify any files. Output a structured analysis report. Offer an optional remediation plan (user must explicitly approve before any follow-up editing commands would be invoked manually).

**Constitution Authority**: The project constitution (`.specify/memory/constitution.md`) is **non-negotiable** within this analysis scope. Constitution conflicts are automatically CRITICAL and require adjustment of the spec, plan, or tasksâ€”not dilution, reinterpretation, or silent ignoring of the principle. If a principle itself needs to change, that must occur in a separate, explicit constitution update outside `/speckit.analyze`.

## Execution Steps

### 1. Initialize Analysis Context

Run `.specify/scripts/bash/check-prerequisites.sh --json --require-tasks --include-tasks` once from repo root and parse JSON for FEATURE_DIR and AVAILABLE_DOCS. Derive absolute paths:

- SPEC = FEATURE_DIR/spec.md
- PLAN = FEATURE_DIR/plan.md
- TASKS = FEATURE_DIR/tasks.md

Abort with an error message if any required file is missing (instruct the user to run missing prerequisite command).
For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

### 2. Load Artifacts (Progressive Disclosure)

Load only the minimal necessary context from each artifact:

**From spec.md:**

- Overview/Context
- Functional Requirements
- Non-Functional Requirements
- User Stories
- Edge Cases (if present)

**From plan.md:**

- Architecture/stack choices
- Data Model references
- Phases
- Technical constraints

**From tasks.md:**

- Task IDs
- Descriptions
- Phase grouping
- Parallel markers [P]
- Referenced file paths

**From constitution:**

- Load `.specify/memory/constitution.md` for principle validation

### 3. Build Semantic Models

Create internal representations (do not include raw artifacts in output):

- **Requirements inventory**: Each functional + non-functional requirement with a stable key (derive slug based on imperative phrase; e.g., "User can upload file" â†’ `user-can-upload-file`)
- **User story/action inventory**: Discrete user actions with acceptance criteria
- **Task coverage mapping**: Map each task to one or more requirements or stories (inference by keyword / explicit reference patterns like IDs or key phrases)
- **Constitution rule set**: Extract principle names and MUST/SHOULD normative statements

### 4. Detection Passes (Token-Efficient Analysis)

Focus on high-signal findings. Limit to 50 findings total; aggregate remainder in overflow summary.

#### A. Duplication Detection

- Identify near-duplicate requirements
- Mark lower-quality phrasing for consolidation

#### B. Ambiguity Detection

- Flag vague adjectives (fast, scalable, secure, intuitive, robust) lacking measurable criteria
- Flag unresolved placeholders (TODO, TKTK, ???, `<placeholder>`, etc.)

#### C. Underspecification

- Requirements with verbs but missing object or measurable outcome
- User stories missing acceptance criteria alignment
- Tasks referencing files or components not defined in spec/plan

#### D. Constitution Alignment

- Any requirement or plan element conflicting with a MUST principle
- Missing mandated sections or quality gates from constitution

#### E. Coverage Gaps

- Requirements with zero associated tasks
- Tasks with no mapped requirement/story
- Non-functional requirements not reflected in tasks (e.g., performance, security)

#### F. Inconsistency

- Terminology drift (same concept named differently across files)
- Data entities referenced in plan but absent in spec (or vice versa)
- Task ordering contradictions (e.g., integration tasks before foundational setup tasks without dependency note)
- Conflicting requirements (e.g., one requires Next.js while other specifies Vue)

### 5. Severity Assignment

Use this heuristic to prioritize findings:

- **CRITICAL**: Violates constitution MUST, missing core spec artifact, or requirement with zero coverage that blocks baseline functionality
- **HIGH**: Duplicate or conflicting requirement, ambiguous security/performance attribute, untestable acceptance criterion
- **MEDIUM**: Terminology drift, missing non-functional task coverage, underspecified edge case
- **LOW**: Style/wording improvements, minor redundancy not affecting execution order

### 6. Produce Compact Analysis Report

Output a Markdown report (no file writes) with the following structure:

## Specification Analysis Report

| ID | Category | Severity | Location(s) | Summary | Recommendation |
|----|----------|----------|-------------|---------|----------------|
| A1 | Duplication | HIGH | spec.md:L120-134 | Two similar requirements ... | Merge phrasing; keep clearer version |

(Add one row per finding; generate stable IDs prefixed by category initial.)

**Coverage Summary Table:**

| Requirement Key | Has Task? | Task IDs | Notes |
|-----------------|-----------|----------|-------|

**Constitution Alignment Issues:** (if any)

**Unmapped Tasks:** (if any)

**Metrics:**

- Total Requirements
- Total Tasks
- Coverage % (requirements with >=1 task)
- Ambiguity Count
- Duplication Count
- Critical Issues Count

### 7. Provide Next Actions

At end of report, output a concise Next Actions block:

- If CRITICAL issues exist: Recommend resolving before `/speckit.implement`
- If only LOW/MEDIUM: User may proceed, but provide improvement suggestions
- Provide explicit command suggestions: e.g., "Run /speckit.specify with refinement", "Run /speckit.plan to adjust architecture", "Manually edit tasks.md to add coverage for 'performance-metrics'"

### 8. Offer Remediation

Ask the user: "Would you like me to suggest concrete remediation edits for the top N issues?" (Do NOT apply them automatically.)

## Operating Principles

### Context Efficiency

- **Minimal high-signal tokens**: Focus on actionable findings, not exhaustive documentation
- **Progressive disclosure**: Load artifacts incrementally; don't dump all content into analysis
- **Token-efficient output**: Limit findings table to 50 rows; summarize overflow
- **Deterministic results**: Rerunning without changes should produce consistent IDs and counts

### Analysis Guidelines

- **NEVER modify files** (this is read-only analysis)
- **NEVER hallucinate missing sections** (if absent, report them accurately)
- **Prioritize constitution violations** (these are always CRITICAL)
- **Use examples over exhaustive rules** (cite specific instances, not generic patterns)
- **Report zero issues gracefully** (emit success report with coverage statistics)

## Context

$ARGUMENTS
````

## File: .claude/commands/speckit.checklist.md
````markdown
---
description: Generate a custom checklist for the current feature based on user requirements.
---

## Checklist Purpose: "Unit Tests for English"

**CRITICAL CONCEPT**: Checklists are **UNIT TESTS FOR REQUIREMENTS WRITING** - they validate the quality, clarity, and completeness of requirements in a given domain.

**NOT for verification/testing**:

- âŒ NOT "Verify the button clicks correctly"
- âŒ NOT "Test error handling works"
- âŒ NOT "Confirm the API returns 200"
- âŒ NOT checking if code/implementation matches the spec

**FOR requirements quality validation**:

- âœ… "Are visual hierarchy requirements defined for all card types?" (completeness)
- âœ… "Is 'prominent display' quantified with specific sizing/positioning?" (clarity)
- âœ… "Are hover state requirements consistent across all interactive elements?" (consistency)
- âœ… "Are accessibility requirements defined for keyboard navigation?" (coverage)
- âœ… "Does the spec define what happens when logo image fails to load?" (edge cases)

**Metaphor**: If your spec is code written in English, the checklist is its unit test suite. You're testing whether the requirements are well-written, complete, unambiguous, and ready for implementation - NOT whether the implementation works.

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Execution Steps

1. **Setup**: Run `.specify/scripts/bash/check-prerequisites.sh --json` from repo root and parse JSON for FEATURE_DIR and AVAILABLE_DOCS list.
   - All file paths must be absolute.
   - For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Clarify intent (dynamic)**: Derive up to THREE initial contextual clarifying questions (no pre-baked catalog). They MUST:
   - Be generated from the user's phrasing + extracted signals from spec/plan/tasks
   - Only ask about information that materially changes checklist content
   - Be skipped individually if already unambiguous in `$ARGUMENTS`
   - Prefer precision over breadth

   Generation algorithm:
   1. Extract signals: feature domain keywords (e.g., auth, latency, UX, API), risk indicators ("critical", "must", "compliance"), stakeholder hints ("QA", "review", "security team"), and explicit deliverables ("a11y", "rollback", "contracts").
   2. Cluster signals into candidate focus areas (max 4) ranked by relevance.
   3. Identify probable audience & timing (author, reviewer, QA, release) if not explicit.
   4. Detect missing dimensions: scope breadth, depth/rigor, risk emphasis, exclusion boundaries, measurable acceptance criteria.
   5. Formulate questions chosen from these archetypes:
      - Scope refinement (e.g., "Should this include integration touchpoints with X and Y or stay limited to local module correctness?")
      - Risk prioritization (e.g., "Which of these potential risk areas should receive mandatory gating checks?")
      - Depth calibration (e.g., "Is this a lightweight pre-commit sanity list or a formal release gate?")
      - Audience framing (e.g., "Will this be used by the author only or peers during PR review?")
      - Boundary exclusion (e.g., "Should we explicitly exclude performance tuning items this round?")
      - Scenario class gap (e.g., "No recovery flows detectedâ€”are rollback / partial failure paths in scope?")

   Question formatting rules:
   - If presenting options, generate a compact table with columns: Option | Candidate | Why It Matters
   - Limit to Aâ€“E options maximum; omit table if a free-form answer is clearer
   - Never ask the user to restate what they already said
   - Avoid speculative categories (no hallucination). If uncertain, ask explicitly: "Confirm whether X belongs in scope."

   Defaults when interaction impossible:
   - Depth: Standard
   - Audience: Reviewer (PR) if code-related; Author otherwise
   - Focus: Top 2 relevance clusters

   Output the questions (label Q1/Q2/Q3). After answers: if â‰¥2 scenario classes (Alternate / Exception / Recovery / Non-Functional domain) remain unclear, you MAY ask up to TWO more targeted followâ€‘ups (Q4/Q5) with a one-line justification each (e.g., "Unresolved recovery path risk"). Do not exceed five total questions. Skip escalation if user explicitly declines more.

3. **Understand user request**: Combine `$ARGUMENTS` + clarifying answers:
   - Derive checklist theme (e.g., security, review, deploy, ux)
   - Consolidate explicit must-have items mentioned by user
   - Map focus selections to category scaffolding
   - Infer any missing context from spec/plan/tasks (do NOT hallucinate)

4. **Load feature context**: Read from FEATURE_DIR:
   - spec.md: Feature requirements and scope
   - plan.md (if exists): Technical details, dependencies
   - tasks.md (if exists): Implementation tasks

   **Context Loading Strategy**:
   - Load only necessary portions relevant to active focus areas (avoid full-file dumping)
   - Prefer summarizing long sections into concise scenario/requirement bullets
   - Use progressive disclosure: add follow-on retrieval only if gaps detected
   - If source docs are large, generate interim summary items instead of embedding raw text

5. **Generate checklist** - Create "Unit Tests for Requirements":
   - Create `FEATURE_DIR/checklists/` directory if it doesn't exist
   - Generate unique checklist filename:
     - Use short, descriptive name based on domain (e.g., `ux.md`, `api.md`, `security.md`)
     - Format: `[domain].md`
     - If file exists, append to existing file
   - Number items sequentially starting from CHK001
   - Each `/speckit.checklist` run creates a NEW file (never overwrites existing checklists)

   **CORE PRINCIPLE - Test the Requirements, Not the Implementation**:
   Every checklist item MUST evaluate the REQUIREMENTS THEMSELVES for:
   - **Completeness**: Are all necessary requirements present?
   - **Clarity**: Are requirements unambiguous and specific?
   - **Consistency**: Do requirements align with each other?
   - **Measurability**: Can requirements be objectively verified?
   - **Coverage**: Are all scenarios/edge cases addressed?

   **Category Structure** - Group items by requirement quality dimensions:
   - **Requirement Completeness** (Are all necessary requirements documented?)
   - **Requirement Clarity** (Are requirements specific and unambiguous?)
   - **Requirement Consistency** (Do requirements align without conflicts?)
   - **Acceptance Criteria Quality** (Are success criteria measurable?)
   - **Scenario Coverage** (Are all flows/cases addressed?)
   - **Edge Case Coverage** (Are boundary conditions defined?)
   - **Non-Functional Requirements** (Performance, Security, Accessibility, etc. - are they specified?)
   - **Dependencies & Assumptions** (Are they documented and validated?)
   - **Ambiguities & Conflicts** (What needs clarification?)

   **HOW TO WRITE CHECKLIST ITEMS - "Unit Tests for English"**:

   âŒ **WRONG** (Testing implementation):
   - "Verify landing page displays 3 episode cards"
   - "Test hover states work on desktop"
   - "Confirm logo click navigates home"

   âœ… **CORRECT** (Testing requirements quality):
   - "Are the exact number and layout of featured episodes specified?" [Completeness]
   - "Is 'prominent display' quantified with specific sizing/positioning?" [Clarity]
   - "Are hover state requirements consistent across all interactive elements?" [Consistency]
   - "Are keyboard navigation requirements defined for all interactive UI?" [Coverage]
   - "Is the fallback behavior specified when logo image fails to load?" [Edge Cases]
   - "Are loading states defined for asynchronous episode data?" [Completeness]
   - "Does the spec define visual hierarchy for competing UI elements?" [Clarity]

   **ITEM STRUCTURE**:
   Each item should follow this pattern:
   - Question format asking about requirement quality
   - Focus on what's WRITTEN (or not written) in the spec/plan
   - Include quality dimension in brackets [Completeness/Clarity/Consistency/etc.]
   - Reference spec section `[Spec Â§X.Y]` when checking existing requirements
   - Use `[Gap]` marker when checking for missing requirements

   **EXAMPLES BY QUALITY DIMENSION**:

   Completeness:
   - "Are error handling requirements defined for all API failure modes? [Gap]"
   - "Are accessibility requirements specified for all interactive elements? [Completeness]"
   - "Are mobile breakpoint requirements defined for responsive layouts? [Gap]"

   Clarity:
   - "Is 'fast loading' quantified with specific timing thresholds? [Clarity, Spec Â§NFR-2]"
   - "Are 'related episodes' selection criteria explicitly defined? [Clarity, Spec Â§FR-5]"
   - "Is 'prominent' defined with measurable visual properties? [Ambiguity, Spec Â§FR-4]"

   Consistency:
   - "Do navigation requirements align across all pages? [Consistency, Spec Â§FR-10]"
   - "Are card component requirements consistent between landing and detail pages? [Consistency]"

   Coverage:
   - "Are requirements defined for zero-state scenarios (no episodes)? [Coverage, Edge Case]"
   - "Are concurrent user interaction scenarios addressed? [Coverage, Gap]"
   - "Are requirements specified for partial data loading failures? [Coverage, Exception Flow]"

   Measurability:
   - "Are visual hierarchy requirements measurable/testable? [Acceptance Criteria, Spec Â§FR-1]"
   - "Can 'balanced visual weight' be objectively verified? [Measurability, Spec Â§FR-2]"

   **Scenario Classification & Coverage** (Requirements Quality Focus):
   - Check if requirements exist for: Primary, Alternate, Exception/Error, Recovery, Non-Functional scenarios
   - For each scenario class, ask: "Are [scenario type] requirements complete, clear, and consistent?"
   - If scenario class missing: "Are [scenario type] requirements intentionally excluded or missing? [Gap]"
   - Include resilience/rollback when state mutation occurs: "Are rollback requirements defined for migration failures? [Gap]"

   **Traceability Requirements**:
   - MINIMUM: â‰¥80% of items MUST include at least one traceability reference
   - Each item should reference: spec section `[Spec Â§X.Y]`, or use markers: `[Gap]`, `[Ambiguity]`, `[Conflict]`, `[Assumption]`
   - If no ID system exists: "Is a requirement & acceptance criteria ID scheme established? [Traceability]"

   **Surface & Resolve Issues** (Requirements Quality Problems):
   Ask questions about the requirements themselves:
   - Ambiguities: "Is the term 'fast' quantified with specific metrics? [Ambiguity, Spec Â§NFR-1]"
   - Conflicts: "Do navigation requirements conflict between Â§FR-10 and Â§FR-10a? [Conflict]"
   - Assumptions: "Is the assumption of 'always available podcast API' validated? [Assumption]"
   - Dependencies: "Are external podcast API requirements documented? [Dependency, Gap]"
   - Missing definitions: "Is 'visual hierarchy' defined with measurable criteria? [Gap]"

   **Content Consolidation**:
   - Soft cap: If raw candidate items > 40, prioritize by risk/impact
   - Merge near-duplicates checking the same requirement aspect
   - If >5 low-impact edge cases, create one item: "Are edge cases X, Y, Z addressed in requirements? [Coverage]"

   **ðŸš« ABSOLUTELY PROHIBITED** - These make it an implementation test, not a requirements test:
   - âŒ Any item starting with "Verify", "Test", "Confirm", "Check" + implementation behavior
   - âŒ References to code execution, user actions, system behavior
   - âŒ "Displays correctly", "works properly", "functions as expected"
   - âŒ "Click", "navigate", "render", "load", "execute"
   - âŒ Test cases, test plans, QA procedures
   - âŒ Implementation details (frameworks, APIs, algorithms)

   **âœ… REQUIRED PATTERNS** - These test requirements quality:
   - âœ… "Are [requirement type] defined/specified/documented for [scenario]?"
   - âœ… "Is [vague term] quantified/clarified with specific criteria?"
   - âœ… "Are requirements consistent between [section A] and [section B]?"
   - âœ… "Can [requirement] be objectively measured/verified?"
   - âœ… "Are [edge cases/scenarios] addressed in requirements?"
   - âœ… "Does the spec define [missing aspect]?"

6. **Structure Reference**: Generate the checklist following the canonical template in `.specify/templates/checklist-template.md` for title, meta section, category headings, and ID formatting. If template is unavailable, use: H1 title, purpose/created meta lines, `##` category sections containing `- [ ] CHK### <requirement item>` lines with globally incrementing IDs starting at CHK001.

7. **Report**: Output full path to created checklist, item count, and remind user that each run creates a new file. Summarize:
   - Focus areas selected
   - Depth level
   - Actor/timing
   - Any explicit user-specified must-have items incorporated

**Important**: Each `/speckit.checklist` command invocation creates a checklist file using short, descriptive names unless file already exists. This allows:

- Multiple checklists of different types (e.g., `ux.md`, `test.md`, `security.md`)
- Simple, memorable filenames that indicate checklist purpose
- Easy identification and navigation in the `checklists/` folder

To avoid clutter, use descriptive types and clean up obsolete checklists when done.

## Example Checklist Types & Sample Items

**UX Requirements Quality:** `ux.md`

Sample items (testing the requirements, NOT the implementation):

- "Are visual hierarchy requirements defined with measurable criteria? [Clarity, Spec Â§FR-1]"
- "Is the number and positioning of UI elements explicitly specified? [Completeness, Spec Â§FR-1]"
- "Are interaction state requirements (hover, focus, active) consistently defined? [Consistency]"
- "Are accessibility requirements specified for all interactive elements? [Coverage, Gap]"
- "Is fallback behavior defined when images fail to load? [Edge Case, Gap]"
- "Can 'prominent display' be objectively measured? [Measurability, Spec Â§FR-4]"

**API Requirements Quality:** `api.md`

Sample items:

- "Are error response formats specified for all failure scenarios? [Completeness]"
- "Are rate limiting requirements quantified with specific thresholds? [Clarity]"
- "Are authentication requirements consistent across all endpoints? [Consistency]"
- "Are retry/timeout requirements defined for external dependencies? [Coverage, Gap]"
- "Is versioning strategy documented in requirements? [Gap]"

**Performance Requirements Quality:** `performance.md`

Sample items:

- "Are performance requirements quantified with specific metrics? [Clarity]"
- "Are performance targets defined for all critical user journeys? [Coverage]"
- "Are performance requirements under different load conditions specified? [Completeness]"
- "Can performance requirements be objectively measured? [Measurability]"
- "Are degradation requirements defined for high-load scenarios? [Edge Case, Gap]"

**Security Requirements Quality:** `security.md`

Sample items:

- "Are authentication requirements specified for all protected resources? [Coverage]"
- "Are data protection requirements defined for sensitive information? [Completeness]"
- "Is the threat model documented and requirements aligned to it? [Traceability]"
- "Are security requirements consistent with compliance obligations? [Consistency]"
- "Are security failure/breach response requirements defined? [Gap, Exception Flow]"

## Anti-Examples: What NOT To Do

**âŒ WRONG - These test implementation, not requirements:**

```markdown
- [ ] CHK001 - Verify landing page displays 3 episode cards [Spec Â§FR-001]
- [ ] CHK002 - Test hover states work correctly on desktop [Spec Â§FR-003]
- [ ] CHK003 - Confirm logo click navigates to home page [Spec Â§FR-010]
- [ ] CHK004 - Check that related episodes section shows 3-5 items [Spec Â§FR-005]
```

**âœ… CORRECT - These test requirements quality:**

```markdown
- [ ] CHK001 - Are the number and layout of featured episodes explicitly specified? [Completeness, Spec Â§FR-001]
- [ ] CHK002 - Are hover state requirements consistently defined for all interactive elements? [Consistency, Spec Â§FR-003]
- [ ] CHK003 - Are navigation requirements clear for all clickable brand elements? [Clarity, Spec Â§FR-010]
- [ ] CHK004 - Is the selection criteria for related episodes documented? [Gap, Spec Â§FR-005]
- [ ] CHK005 - Are loading state requirements defined for asynchronous episode data? [Gap]
- [ ] CHK006 - Can "visual hierarchy" requirements be objectively measured? [Measurability, Spec Â§FR-001]
```

**Key Differences:**

- Wrong: Tests if the system works correctly
- Correct: Tests if the requirements are written correctly
- Wrong: Verification of behavior
- Correct: Validation of requirement quality
- Wrong: "Does it do X?"
- Correct: "Is X clearly specified?"
````

## File: .claude/commands/speckit.clarify.md
````markdown
---
description: Identify underspecified areas in the current feature spec by asking up to 5 highly targeted clarification questions and encoding answers back into the spec.
handoffs: 
  - label: Build Technical Plan
    agent: speckit.plan
    prompt: Create a plan for the spec. I am building with...
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

Goal: Detect and reduce ambiguity or missing decision points in the active feature specification and record the clarifications directly in the spec file.

Note: This clarification workflow is expected to run (and be completed) BEFORE invoking `/speckit.plan`. If the user explicitly states they are skipping clarification (e.g., exploratory spike), you may proceed, but must warn that downstream rework risk increases.

Execution steps:

1. Run `.specify/scripts/bash/check-prerequisites.sh --json --paths-only` from repo root **once** (combined `--json --paths-only` mode / `-Json -PathsOnly`). Parse minimal JSON payload fields:
   - `FEATURE_DIR`
   - `FEATURE_SPEC`
   - (Optionally capture `IMPL_PLAN`, `TASKS` for future chained flows.)
   - If JSON parsing fails, abort and instruct user to re-run `/speckit.specify` or verify feature branch environment.
   - For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. Load the current spec file. Perform a structured ambiguity & coverage scan using this taxonomy. For each category, mark status: Clear / Partial / Missing. Produce an internal coverage map used for prioritization (do not output raw map unless no questions will be asked).

   Functional Scope & Behavior:
   - Core user goals & success criteria
   - Explicit out-of-scope declarations
   - User roles / personas differentiation

   Domain & Data Model:
   - Entities, attributes, relationships
   - Identity & uniqueness rules
   - Lifecycle/state transitions
   - Data volume / scale assumptions

   Interaction & UX Flow:
   - Critical user journeys / sequences
   - Error/empty/loading states
   - Accessibility or localization notes

   Non-Functional Quality Attributes:
   - Performance (latency, throughput targets)
   - Scalability (horizontal/vertical, limits)
   - Reliability & availability (uptime, recovery expectations)
   - Observability (logging, metrics, tracing signals)
   - Security & privacy (authN/Z, data protection, threat assumptions)
   - Compliance / regulatory constraints (if any)

   Integration & External Dependencies:
   - External services/APIs and failure modes
   - Data import/export formats
   - Protocol/versioning assumptions

   Edge Cases & Failure Handling:
   - Negative scenarios
   - Rate limiting / throttling
   - Conflict resolution (e.g., concurrent edits)

   Constraints & Tradeoffs:
   - Technical constraints (language, storage, hosting)
   - Explicit tradeoffs or rejected alternatives

   Terminology & Consistency:
   - Canonical glossary terms
   - Avoided synonyms / deprecated terms

   Completion Signals:
   - Acceptance criteria testability
   - Measurable Definition of Done style indicators

   Misc / Placeholders:
   - TODO markers / unresolved decisions
   - Ambiguous adjectives ("robust", "intuitive") lacking quantification

   For each category with Partial or Missing status, add a candidate question opportunity unless:
   - Clarification would not materially change implementation or validation strategy
   - Information is better deferred to planning phase (note internally)

3. Generate (internally) a prioritized queue of candidate clarification questions (maximum 5). Do NOT output them all at once. Apply these constraints:
    - Maximum of 10 total questions across the whole session.
    - Each question must be answerable with EITHER:
       - A short multipleâ€‘choice selection (2â€“5 distinct, mutually exclusive options), OR
       - A one-word / shortâ€‘phrase answer (explicitly constrain: "Answer in <=5 words").
    - Only include questions whose answers materially impact architecture, data modeling, task decomposition, test design, UX behavior, operational readiness, or compliance validation.
    - Ensure category coverage balance: attempt to cover the highest impact unresolved categories first; avoid asking two low-impact questions when a single high-impact area (e.g., security posture) is unresolved.
    - Exclude questions already answered, trivial stylistic preferences, or plan-level execution details (unless blocking correctness).
    - Favor clarifications that reduce downstream rework risk or prevent misaligned acceptance tests.
    - If more than 5 categories remain unresolved, select the top 5 by (Impact * Uncertainty) heuristic.

4. Sequential questioning loop (interactive):
    - Present EXACTLY ONE question at a time.
    - For multipleâ€‘choice questions:
       - **Analyze all options** and determine the **most suitable option** based on:
          - Best practices for the project type
          - Common patterns in similar implementations
          - Risk reduction (security, performance, maintainability)
          - Alignment with any explicit project goals or constraints visible in the spec
       - Present your **recommended option prominently** at the top with clear reasoning (1-2 sentences explaining why this is the best choice).
       - Format as: `**Recommended:** Option [X] - <reasoning>`
       - Then render all options as a Markdown table:

       | Option | Description |
       |--------|-------------|
       | A | <Option A description> |
       | B | <Option B description> |
       | C | <Option C description> (add D/E as needed up to 5) |
       | Short | Provide a different short answer (<=5 words) (Include only if free-form alternative is appropriate) |

       - After the table, add: `You can reply with the option letter (e.g., "A"), accept the recommendation by saying "yes" or "recommended", or provide your own short answer.`
    - For shortâ€‘answer style (no meaningful discrete options):
       - Provide your **suggested answer** based on best practices and context.
       - Format as: `**Suggested:** <your proposed answer> - <brief reasoning>`
       - Then output: `Format: Short answer (<=5 words). You can accept the suggestion by saying "yes" or "suggested", or provide your own answer.`
    - After the user answers:
       - If the user replies with "yes", "recommended", or "suggested", use your previously stated recommendation/suggestion as the answer.
       - Otherwise, validate the answer maps to one option or fits the <=5 word constraint.
       - If ambiguous, ask for a quick disambiguation (count still belongs to same question; do not advance).
       - Once satisfactory, record it in working memory (do not yet write to disk) and move to the next queued question.
    - Stop asking further questions when:
       - All critical ambiguities resolved early (remaining queued items become unnecessary), OR
       - User signals completion ("done", "good", "no more"), OR
       - You reach 5 asked questions.
    - Never reveal future queued questions in advance.
    - If no valid questions exist at start, immediately report no critical ambiguities.

5. Integration after EACH accepted answer (incremental update approach):
    - Maintain in-memory representation of the spec (loaded once at start) plus the raw file contents.
    - For the first integrated answer in this session:
       - Ensure a `## Clarifications` section exists (create it just after the highest-level contextual/overview section per the spec template if missing).
       - Under it, create (if not present) a `### Session YYYY-MM-DD` subheading for today.
    - Append a bullet line immediately after acceptance: `- Q: <question> â†’ A: <final answer>`.
    - Then immediately apply the clarification to the most appropriate section(s):
       - Functional ambiguity â†’ Update or add a bullet in Functional Requirements.
       - User interaction / actor distinction â†’ Update User Stories or Actors subsection (if present) with clarified role, constraint, or scenario.
       - Data shape / entities â†’ Update Data Model (add fields, types, relationships) preserving ordering; note added constraints succinctly.
       - Non-functional constraint â†’ Add/modify measurable criteria in Non-Functional / Quality Attributes section (convert vague adjective to metric or explicit target).
       - Edge case / negative flow â†’ Add a new bullet under Edge Cases / Error Handling (or create such subsection if template provides placeholder for it).
       - Terminology conflict â†’ Normalize term across spec; retain original only if necessary by adding `(formerly referred to as "X")` once.
    - If the clarification invalidates an earlier ambiguous statement, replace that statement instead of duplicating; leave no obsolete contradictory text.
    - Save the spec file AFTER each integration to minimize risk of context loss (atomic overwrite).
    - Preserve formatting: do not reorder unrelated sections; keep heading hierarchy intact.
    - Keep each inserted clarification minimal and testable (avoid narrative drift).

6. Validation (performed after EACH write plus final pass):
   - Clarifications session contains exactly one bullet per accepted answer (no duplicates).
   - Total asked (accepted) questions â‰¤ 5.
   - Updated sections contain no lingering vague placeholders the new answer was meant to resolve.
   - No contradictory earlier statement remains (scan for now-invalid alternative choices removed).
   - Markdown structure valid; only allowed new headings: `## Clarifications`, `### Session YYYY-MM-DD`.
   - Terminology consistency: same canonical term used across all updated sections.

7. Write the updated spec back to `FEATURE_SPEC`.

8. Report completion (after questioning loop ends or early termination):
   - Number of questions asked & answered.
   - Path to updated spec.
   - Sections touched (list names).
   - Coverage summary table listing each taxonomy category with Status: Resolved (was Partial/Missing and addressed), Deferred (exceeds question quota or better suited for planning), Clear (already sufficient), Outstanding (still Partial/Missing but low impact).
   - If any Outstanding or Deferred remain, recommend whether to proceed to `/speckit.plan` or run `/speckit.clarify` again later post-plan.
   - Suggested next command.

Behavior rules:

- If no meaningful ambiguities found (or all potential questions would be low-impact), respond: "No critical ambiguities detected worth formal clarification." and suggest proceeding.
- If spec file missing, instruct user to run `/speckit.specify` first (do not create a new spec here).
- Never exceed 5 total asked questions (clarification retries for a single question do not count as new questions).
- Avoid speculative tech stack questions unless the absence blocks functional clarity.
- Respect user early termination signals ("stop", "done", "proceed").
- If no questions asked due to full coverage, output a compact coverage summary (all categories Clear) then suggest advancing.
- If quota reached with unresolved high-impact categories remaining, explicitly flag them under Deferred with rationale.

Context for prioritization: $ARGUMENTS
````

## File: .claude/commands/speckit.constitution.md
````markdown
---
description: Create or update the project constitution from interactive or provided principle inputs, ensuring all dependent templates stay in sync.
handoffs: 
  - label: Build Specification
    agent: speckit.specify
    prompt: Implement the feature specification based on the updated constitution. I want to build...
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

You are updating the project constitution at `.specify/memory/constitution.md`. This file is a TEMPLATE containing placeholder tokens in square brackets (e.g. `[PROJECT_NAME]`, `[PRINCIPLE_1_NAME]`). Your job is to (a) collect/derive concrete values, (b) fill the template precisely, and (c) propagate any amendments across dependent artifacts.

Follow this execution flow:

1. Load the existing constitution template at `.specify/memory/constitution.md`.
   - Identify every placeholder token of the form `[ALL_CAPS_IDENTIFIER]`.
   **IMPORTANT**: The user might require less or more principles than the ones used in the template. If a number is specified, respect that - follow the general template. You will update the doc accordingly.

2. Collect/derive values for placeholders:
   - If user input (conversation) supplies a value, use it.
   - Otherwise infer from existing repo context (README, docs, prior constitution versions if embedded).
   - For governance dates: `RATIFICATION_DATE` is the original adoption date (if unknown ask or mark TODO), `LAST_AMENDED_DATE` is today if changes are made, otherwise keep previous.
   - `CONSTITUTION_VERSION` must increment according to semantic versioning rules:
     - MAJOR: Backward incompatible governance/principle removals or redefinitions.
     - MINOR: New principle/section added or materially expanded guidance.
     - PATCH: Clarifications, wording, typo fixes, non-semantic refinements.
   - If version bump type ambiguous, propose reasoning before finalizing.

3. Draft the updated constitution content:
   - Replace every placeholder with concrete text (no bracketed tokens left except intentionally retained template slots that the project has chosen not to define yetâ€”explicitly justify any left).
   - Preserve heading hierarchy and comments can be removed once replaced unless they still add clarifying guidance.
   - Ensure each Principle section: succinct name line, paragraph (or bullet list) capturing nonâ€‘negotiable rules, explicit rationale if not obvious.
   - Ensure Governance section lists amendment procedure, versioning policy, and compliance review expectations.

4. Consistency propagation checklist (convert prior checklist into active validations):
   - Read `.specify/templates/plan-template.md` and ensure any "Constitution Check" or rules align with updated principles.
   - Read `.specify/templates/spec-template.md` for scope/requirements alignmentâ€”update if constitution adds/removes mandatory sections or constraints.
   - Read `.specify/templates/tasks-template.md` and ensure task categorization reflects new or removed principle-driven task types (e.g., observability, versioning, testing discipline).
   - Read each command file in `.specify/templates/commands/*.md` (including this one) to verify no outdated references (agent-specific names like CLAUDE only) remain when generic guidance is required.
   - Read any runtime guidance docs (e.g., `README.md`, `docs/quickstart.md`, or agent-specific guidance files if present). Update references to principles changed.

5. Produce a Sync Impact Report (prepend as an HTML comment at top of the constitution file after update):
   - Version change: old â†’ new
   - List of modified principles (old title â†’ new title if renamed)
   - Added sections
   - Removed sections
   - Templates requiring updates (âœ… updated / âš  pending) with file paths
   - Follow-up TODOs if any placeholders intentionally deferred.

6. Validation before final output:
   - No remaining unexplained bracket tokens.
   - Version line matches report.
   - Dates ISO format YYYY-MM-DD.
   - Principles are declarative, testable, and free of vague language ("should" â†’ replace with MUST/SHOULD rationale where appropriate).

7. Write the completed constitution back to `.specify/memory/constitution.md` (overwrite).

8. Output a final summary to the user with:
   - New version and bump rationale.
   - Any files flagged for manual follow-up.
   - Suggested commit message (e.g., `docs: amend constitution to vX.Y.Z (principle additions + governance update)`).

Formatting & Style Requirements:

- Use Markdown headings exactly as in the template (do not demote/promote levels).
- Wrap long rationale lines to keep readability (<100 chars ideally) but do not hard enforce with awkward breaks.
- Keep a single blank line between sections.
- Avoid trailing whitespace.

If the user supplies partial updates (e.g., only one principle revision), still perform validation and version decision steps.

If critical info missing (e.g., ratification date truly unknown), insert `TODO(<FIELD_NAME>): explanation` and include in the Sync Impact Report under deferred items.

Do not create a new template; always operate on the existing `.specify/memory/constitution.md` file.
````

## File: .claude/commands/speckit.implement.md
````markdown
---
description: Execute the implementation plan by processing and executing all tasks defined in tasks.md
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. Run `.specify/scripts/bash/check-prerequisites.sh --json --require-tasks --include-tasks` from repo root and parse FEATURE_DIR and AVAILABLE_DOCS list. All paths must be absolute. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Check checklists status** (if FEATURE_DIR/checklists/ exists):
   - Scan all checklist files in the checklists/ directory
   - For each checklist, count:
     - Total items: All lines matching `- [ ]` or `- [X]` or `- [x]`
     - Completed items: Lines matching `- [X]` or `- [x]`
     - Incomplete items: Lines matching `- [ ]`
   - Create a status table:

     ```text
     | Checklist | Total | Completed | Incomplete | Status |
     |-----------|-------|-----------|------------|--------|
     | ux.md     | 12    | 12        | 0          | âœ“ PASS |
     | test.md   | 8     | 5         | 3          | âœ— FAIL |
     | security.md | 6   | 6         | 0          | âœ“ PASS |
     ```

   - Calculate overall status:
     - **PASS**: All checklists have 0 incomplete items
     - **FAIL**: One or more checklists have incomplete items

   - **If any checklist is incomplete**:
     - Display the table with incomplete item counts
     - **STOP** and ask: "Some checklists are incomplete. Do you want to proceed with implementation anyway? (yes/no)"
     - Wait for user response before continuing
     - If user says "no" or "wait" or "stop", halt execution
     - If user says "yes" or "proceed" or "continue", proceed to step 3

   - **If all checklists are complete**:
     - Display the table showing all checklists passed
     - Automatically proceed to step 3

3. Load and analyze the implementation context:
   - **REQUIRED**: Read tasks.md for the complete task list and execution plan
   - **REQUIRED**: Read plan.md for tech stack, architecture, and file structure
   - **IF EXISTS**: Read data-model.md for entities and relationships
   - **IF EXISTS**: Read contracts/ for API specifications and test requirements
   - **IF EXISTS**: Read research.md for technical decisions and constraints
   - **IF EXISTS**: Read quickstart.md for integration scenarios

4. **Project Setup Verification**:
   - **REQUIRED**: Create/verify ignore files based on actual project setup:

   **Detection & Creation Logic**:
   - Check if the following command succeeds to determine if the repository is a git repo (create/verify .gitignore if so):

     ```sh
     git rev-parse --git-dir 2>/dev/null
     ```

   - Check if Dockerfile* exists or Docker in plan.md â†’ create/verify .dockerignore
   - Check if .eslintrc* exists â†’ create/verify .eslintignore
   - Check if eslint.config.* exists â†’ ensure the config's `ignores` entries cover required patterns
   - Check if .prettierrc* exists â†’ create/verify .prettierignore
   - Check if .npmrc or package.json exists â†’ create/verify .npmignore (if publishing)
   - Check if terraform files (*.tf) exist â†’ create/verify .terraformignore
   - Check if .helmignore needed (helm charts present) â†’ create/verify .helmignore

   **If ignore file already exists**: Verify it contains essential patterns, append missing critical patterns only
   **If ignore file missing**: Create with full pattern set for detected technology

   **Common Patterns by Technology** (from plan.md tech stack):
   - **Node.js/JavaScript/TypeScript**: `node_modules/`, `dist/`, `build/`, `*.log`, `.env*`
   - **Python**: `__pycache__/`, `*.pyc`, `.venv/`, `venv/`, `dist/`, `*.egg-info/`
   - **Java**: `target/`, `*.class`, `*.jar`, `.gradle/`, `build/`
   - **C#/.NET**: `bin/`, `obj/`, `*.user`, `*.suo`, `packages/`
   - **Go**: `*.exe`, `*.test`, `vendor/`, `*.out`
   - **Ruby**: `.bundle/`, `log/`, `tmp/`, `*.gem`, `vendor/bundle/`
   - **PHP**: `vendor/`, `*.log`, `*.cache`, `*.env`
   - **Rust**: `target/`, `debug/`, `release/`, `*.rs.bk`, `*.rlib`, `*.prof*`, `.idea/`, `*.log`, `.env*`
   - **Kotlin**: `build/`, `out/`, `.gradle/`, `.idea/`, `*.class`, `*.jar`, `*.iml`, `*.log`, `.env*`
   - **C++**: `build/`, `bin/`, `obj/`, `out/`, `*.o`, `*.so`, `*.a`, `*.exe`, `*.dll`, `.idea/`, `*.log`, `.env*`
   - **C**: `build/`, `bin/`, `obj/`, `out/`, `*.o`, `*.a`, `*.so`, `*.exe`, `Makefile`, `config.log`, `.idea/`, `*.log`, `.env*`
   - **Swift**: `.build/`, `DerivedData/`, `*.swiftpm/`, `Packages/`
   - **R**: `.Rproj.user/`, `.Rhistory`, `.RData`, `.Ruserdata`, `*.Rproj`, `packrat/`, `renv/`
   - **Universal**: `.DS_Store`, `Thumbs.db`, `*.tmp`, `*.swp`, `.vscode/`, `.idea/`

   **Tool-Specific Patterns**:
   - **Docker**: `node_modules/`, `.git/`, `Dockerfile*`, `.dockerignore`, `*.log*`, `.env*`, `coverage/`
   - **ESLint**: `node_modules/`, `dist/`, `build/`, `coverage/`, `*.min.js`
   - **Prettier**: `node_modules/`, `dist/`, `build/`, `coverage/`, `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml`
   - **Terraform**: `.terraform/`, `*.tfstate*`, `*.tfvars`, `.terraform.lock.hcl`
   - **Kubernetes/k8s**: `*.secret.yaml`, `secrets/`, `.kube/`, `kubeconfig*`, `*.key`, `*.crt`

5. Parse tasks.md structure and extract:
   - **Task phases**: Setup, Tests, Core, Integration, Polish
   - **Task dependencies**: Sequential vs parallel execution rules
   - **Task details**: ID, description, file paths, parallel markers [P]
   - **Execution flow**: Order and dependency requirements

6. Execute implementation following the task plan:
   - **Phase-by-phase execution**: Complete each phase before moving to the next
   - **Respect dependencies**: Run sequential tasks in order, parallel tasks [P] can run together  
   - **Follow TDD approach**: Execute test tasks before their corresponding implementation tasks
   - **File-based coordination**: Tasks affecting the same files must run sequentially
   - **Validation checkpoints**: Verify each phase completion before proceeding

7. Implementation execution rules:
   - **Setup first**: Initialize project structure, dependencies, configuration
   - **Tests before code**: If you need to write tests for contracts, entities, and integration scenarios
   - **Core development**: Implement models, services, CLI commands, endpoints
   - **Integration work**: Database connections, middleware, logging, external services
   - **Polish and validation**: Unit tests, performance optimization, documentation

8. Progress tracking and error handling:
   - Report progress after each completed task
   - Halt execution if any non-parallel task fails
   - For parallel tasks [P], continue with successful tasks, report failed ones
   - Provide clear error messages with context for debugging
   - Suggest next steps if implementation cannot proceed
   - **IMPORTANT** For completed tasks, make sure to mark the task off as [X] in the tasks file.

9. Completion validation:
   - Verify all required tasks are completed
   - Check that implemented features match the original specification
   - Validate that tests pass and coverage meets requirements
   - Confirm the implementation follows the technical plan
   - Report final status with summary of completed work

Note: This command assumes a complete task breakdown exists in tasks.md. If tasks are incomplete or missing, suggest running `/speckit.tasks` first to regenerate the task list.
````

## File: .claude/commands/speckit.plan.md
````markdown
---
description: Execute the implementation planning workflow using the plan template to generate design artifacts.
handoffs: 
  - label: Create Tasks
    agent: speckit.tasks
    prompt: Break the plan into tasks
    send: true
  - label: Create Checklist
    agent: speckit.checklist
    prompt: Create a checklist for the following domain...
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. **Setup**: Run `.specify/scripts/bash/setup-plan.sh --json` from repo root and parse JSON for FEATURE_SPEC, IMPL_PLAN, SPECS_DIR, BRANCH. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Load context**: Read FEATURE_SPEC and `.specify/memory/constitution.md`. Load IMPL_PLAN template (already copied).

3. **Execute plan workflow**: Follow the structure in IMPL_PLAN template to:
   - Fill Technical Context (mark unknowns as "NEEDS CLARIFICATION")
   - Fill Constitution Check section from constitution
   - Evaluate gates (ERROR if violations unjustified)
   - Phase 0: Generate research.md (resolve all NEEDS CLARIFICATION)
   - Phase 1: Generate data-model.md, contracts/, quickstart.md
   - Phase 1: Update agent context by running the agent script
   - Re-evaluate Constitution Check post-design

4. **Stop and report**: Command ends after Phase 2 planning. Report branch, IMPL_PLAN path, and generated artifacts.

## Phases

### Phase 0: Outline & Research

1. **Extract unknowns from Technical Context** above:
   - For each NEEDS CLARIFICATION â†’ research task
   - For each dependency â†’ best practices task
   - For each integration â†’ patterns task

2. **Generate and dispatch research agents**:

   ```text
   For each unknown in Technical Context:
     Task: "Research {unknown} for {feature context}"
   For each technology choice:
     Task: "Find best practices for {tech} in {domain}"
   ```

3. **Consolidate findings** in `research.md` using format:
   - Decision: [what was chosen]
   - Rationale: [why chosen]
   - Alternatives considered: [what else evaluated]

**Output**: research.md with all NEEDS CLARIFICATION resolved

### Phase 1: Design & Contracts

**Prerequisites:** `research.md` complete

1. **Extract entities from feature spec** â†’ `data-model.md`:
   - Entity name, fields, relationships
   - Validation rules from requirements
   - State transitions if applicable

2. **Generate API contracts** from functional requirements:
   - For each user action â†’ endpoint
   - Use standard REST/GraphQL patterns
   - Output OpenAPI/GraphQL schema to `/contracts/`

3. **Agent context update**:
   - Run `.specify/scripts/bash/update-agent-context.sh claude`
   - These scripts detect which AI agent is in use
   - Update the appropriate agent-specific context file
   - Add only new technology from current plan
   - Preserve manual additions between markers

**Output**: data-model.md, /contracts/*, quickstart.md, agent-specific file

## Key rules

- Use absolute paths
- ERROR on gate failures or unresolved clarifications
````

## File: .claude/commands/speckit.specify.md
````markdown
---
description: Create or update the feature specification from a natural language feature description.
handoffs: 
  - label: Build Technical Plan
    agent: speckit.plan
    prompt: Create a plan for the spec. I am building with...
  - label: Clarify Spec Requirements
    agent: speckit.clarify
    prompt: Clarify specification requirements
    send: true
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

The text the user typed after `/speckit.specify` in the triggering message **is** the feature description. Assume you always have it available in this conversation even if `$ARGUMENTS` appears literally below. Do not ask the user to repeat it unless they provided an empty command.

Given that feature description, do this:

1. **Generate a concise short name** (2-4 words) for the branch:
   - Analyze the feature description and extract the most meaningful keywords
   - Create a 2-4 word short name that captures the essence of the feature
   - Use action-noun format when possible (e.g., "add-user-auth", "fix-payment-bug")
   - Preserve technical terms and acronyms (OAuth2, API, JWT, etc.)
   - Keep it concise but descriptive enough to understand the feature at a glance
   - Examples:
     - "I want to add user authentication" â†’ "user-auth"
     - "Implement OAuth2 integration for the API" â†’ "oauth2-api-integration"
     - "Create a dashboard for analytics" â†’ "analytics-dashboard"
     - "Fix payment processing timeout bug" â†’ "fix-payment-timeout"

2. **Check for existing branches before creating new one**:
   
   a. First, fetch all remote branches to ensure we have the latest information:
      ```bash
      git fetch --all --prune
      ```
   
   b. Find the highest feature number across all sources for the short-name:
      - Remote branches: `git ls-remote --heads origin | grep -E 'refs/heads/[0-9]+-<short-name>$'`
      - Local branches: `git branch | grep -E '^[* ]*[0-9]+-<short-name>$'`
      - Specs directories: Check for directories matching `specs/[0-9]+-<short-name>`
   
   c. Determine the next available number:
      - Extract all numbers from all three sources
      - Find the highest number N
      - Use N+1 for the new branch number
   
   d. Run the script `.specify/scripts/bash/create-new-feature.sh --json "$ARGUMENTS"` with the calculated number and short-name:
      - Pass `--number N+1` and `--short-name "your-short-name"` along with the feature description
      - Bash example: `.specify/scripts/bash/create-new-feature.sh --json "$ARGUMENTS" --json --number 5 --short-name "user-auth" "Add user authentication"`
      - PowerShell example: `.specify/scripts/bash/create-new-feature.sh --json "$ARGUMENTS" -Json -Number 5 -ShortName "user-auth" "Add user authentication"`
   
   **IMPORTANT**:
   - Check all three sources (remote branches, local branches, specs directories) to find the highest number
   - Only match branches/directories with the exact short-name pattern
   - If no existing branches/directories found with this short-name, start with number 1
   - You must only ever run this script once per feature
   - The JSON is provided in the terminal as output - always refer to it to get the actual content you're looking for
   - The JSON output will contain BRANCH_NAME and SPEC_FILE paths
   - For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot")

3. Load `.specify/templates/spec-template.md` to understand required sections.

4. Follow this execution flow:

    1. Parse user description from Input
       If empty: ERROR "No feature description provided"
    2. Extract key concepts from description
       Identify: actors, actions, data, constraints
    3. For unclear aspects:
       - Make informed guesses based on context and industry standards
       - Only mark with [NEEDS CLARIFICATION: specific question] if:
         - The choice significantly impacts feature scope or user experience
         - Multiple reasonable interpretations exist with different implications
         - No reasonable default exists
       - **LIMIT: Maximum 3 [NEEDS CLARIFICATION] markers total**
       - Prioritize clarifications by impact: scope > security/privacy > user experience > technical details
    4. Fill User Scenarios & Testing section
       If no clear user flow: ERROR "Cannot determine user scenarios"
    5. Generate Functional Requirements
       Each requirement must be testable
       Use reasonable defaults for unspecified details (document assumptions in Assumptions section)
    6. Define Success Criteria
       Create measurable, technology-agnostic outcomes
       Include both quantitative metrics (time, performance, volume) and qualitative measures (user satisfaction, task completion)
       Each criterion must be verifiable without implementation details
    7. Identify Key Entities (if data involved)
    8. Return: SUCCESS (spec ready for planning)

5. Write the specification to SPEC_FILE using the template structure, replacing placeholders with concrete details derived from the feature description (arguments) while preserving section order and headings.

6. **Specification Quality Validation**: After writing the initial spec, validate it against quality criteria:

   a. **Create Spec Quality Checklist**: Generate a checklist file at `FEATURE_DIR/checklists/requirements.md` using the checklist template structure with these validation items:

      ```markdown
      # Specification Quality Checklist: [FEATURE NAME]
      
      **Purpose**: Validate specification completeness and quality before proceeding to planning
      **Created**: [DATE]
      **Feature**: [Link to spec.md]
      
      ## Content Quality
      
      - [ ] No implementation details (languages, frameworks, APIs)
      - [ ] Focused on user value and business needs
      - [ ] Written for non-technical stakeholders
      - [ ] All mandatory sections completed
      
      ## Requirement Completeness
      
      - [ ] No [NEEDS CLARIFICATION] markers remain
      - [ ] Requirements are testable and unambiguous
      - [ ] Success criteria are measurable
      - [ ] Success criteria are technology-agnostic (no implementation details)
      - [ ] All acceptance scenarios are defined
      - [ ] Edge cases are identified
      - [ ] Scope is clearly bounded
      - [ ] Dependencies and assumptions identified
      
      ## Feature Readiness
      
      - [ ] All functional requirements have clear acceptance criteria
      - [ ] User scenarios cover primary flows
      - [ ] Feature meets measurable outcomes defined in Success Criteria
      - [ ] No implementation details leak into specification
      
      ## Notes
      
      - Items marked incomplete require spec updates before `/speckit.clarify` or `/speckit.plan`
      ```

   b. **Run Validation Check**: Review the spec against each checklist item:
      - For each item, determine if it passes or fails
      - Document specific issues found (quote relevant spec sections)

   c. **Handle Validation Results**:

      - **If all items pass**: Mark checklist complete and proceed to step 6

      - **If items fail (excluding [NEEDS CLARIFICATION])**:
        1. List the failing items and specific issues
        2. Update the spec to address each issue
        3. Re-run validation until all items pass (max 3 iterations)
        4. If still failing after 3 iterations, document remaining issues in checklist notes and warn user

      - **If [NEEDS CLARIFICATION] markers remain**:
        1. Extract all [NEEDS CLARIFICATION: ...] markers from the spec
        2. **LIMIT CHECK**: If more than 3 markers exist, keep only the 3 most critical (by scope/security/UX impact) and make informed guesses for the rest
        3. For each clarification needed (max 3), present options to user in this format:

           ```markdown
           ## Question [N]: [Topic]
           
           **Context**: [Quote relevant spec section]
           
           **What we need to know**: [Specific question from NEEDS CLARIFICATION marker]
           
           **Suggested Answers**:
           
           | Option | Answer | Implications |
           |--------|--------|--------------|
           | A      | [First suggested answer] | [What this means for the feature] |
           | B      | [Second suggested answer] | [What this means for the feature] |
           | C      | [Third suggested answer] | [What this means for the feature] |
           | Custom | Provide your own answer | [Explain how to provide custom input] |
           
           **Your choice**: _[Wait for user response]_
           ```

        4. **CRITICAL - Table Formatting**: Ensure markdown tables are properly formatted:
           - Use consistent spacing with pipes aligned
           - Each cell should have spaces around content: `| Content |` not `|Content|`
           - Header separator must have at least 3 dashes: `|--------|`
           - Test that the table renders correctly in markdown preview
        5. Number questions sequentially (Q1, Q2, Q3 - max 3 total)
        6. Present all questions together before waiting for responses
        7. Wait for user to respond with their choices for all questions (e.g., "Q1: A, Q2: Custom - [details], Q3: B")
        8. Update the spec by replacing each [NEEDS CLARIFICATION] marker with the user's selected or provided answer
        9. Re-run validation after all clarifications are resolved

   d. **Update Checklist**: After each validation iteration, update the checklist file with current pass/fail status

7. Report completion with branch name, spec file path, checklist results, and readiness for the next phase (`/speckit.clarify` or `/speckit.plan`).

**NOTE:** The script creates and checks out the new branch and initializes the spec file before writing.

## General Guidelines

## Quick Guidelines

- Focus on **WHAT** users need and **WHY**.
- Avoid HOW to implement (no tech stack, APIs, code structure).
- Written for business stakeholders, not developers.
- DO NOT create any checklists that are embedded in the spec. That will be a separate command.

### Section Requirements

- **Mandatory sections**: Must be completed for every feature
- **Optional sections**: Include only when relevant to the feature
- When a section doesn't apply, remove it entirely (don't leave as "N/A")

### For AI Generation

When creating this spec from a user prompt:

1. **Make informed guesses**: Use context, industry standards, and common patterns to fill gaps
2. **Document assumptions**: Record reasonable defaults in the Assumptions section
3. **Limit clarifications**: Maximum 3 [NEEDS CLARIFICATION] markers - use only for critical decisions that:
   - Significantly impact feature scope or user experience
   - Have multiple reasonable interpretations with different implications
   - Lack any reasonable default
4. **Prioritize clarifications**: scope > security/privacy > user experience > technical details
5. **Think like a tester**: Every vague requirement should fail the "testable and unambiguous" checklist item
6. **Common areas needing clarification** (only if no reasonable default exists):
   - Feature scope and boundaries (include/exclude specific use cases)
   - User types and permissions (if multiple conflicting interpretations possible)
   - Security/compliance requirements (when legally/financially significant)

**Examples of reasonable defaults** (don't ask about these):

- Data retention: Industry-standard practices for the domain
- Performance targets: Standard web/mobile app expectations unless specified
- Error handling: User-friendly messages with appropriate fallbacks
- Authentication method: Standard session-based or OAuth2 for web apps
- Integration patterns: RESTful APIs unless specified otherwise

### Success Criteria Guidelines

Success criteria must be:

1. **Measurable**: Include specific metrics (time, percentage, count, rate)
2. **Technology-agnostic**: No mention of frameworks, languages, databases, or tools
3. **User-focused**: Describe outcomes from user/business perspective, not system internals
4. **Verifiable**: Can be tested/validated without knowing implementation details

**Good examples**:

- "Users can complete checkout in under 3 minutes"
- "System supports 10,000 concurrent users"
- "95% of searches return results in under 1 second"
- "Task completion rate improves by 40%"

**Bad examples** (implementation-focused):

- "API response time is under 200ms" (too technical, use "Users see results instantly")
- "Database can handle 1000 TPS" (implementation detail, use user-facing metric)
- "React components render efficiently" (framework-specific)
- "Redis cache hit rate above 80%" (technology-specific)
````

## File: .claude/commands/speckit.tasks.md
````markdown
---
description: Generate an actionable, dependency-ordered tasks.md for the feature based on available design artifacts.
handoffs: 
  - label: Analyze For Consistency
    agent: speckit.analyze
    prompt: Run a project analysis for consistency
    send: true
  - label: Implement Project
    agent: speckit.implement
    prompt: Start the implementation in phases
    send: true
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. **Setup**: Run `.specify/scripts/bash/check-prerequisites.sh --json` from repo root and parse FEATURE_DIR and AVAILABLE_DOCS list. All paths must be absolute. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Load design documents**: Read from FEATURE_DIR:
   - **Required**: plan.md (tech stack, libraries, structure), spec.md (user stories with priorities)
   - **Optional**: data-model.md (entities), contracts/ (API endpoints), research.md (decisions), quickstart.md (test scenarios)
   - Note: Not all projects have all documents. Generate tasks based on what's available.

3. **Execute task generation workflow**:
   - Load plan.md and extract tech stack, libraries, project structure
   - Load spec.md and extract user stories with their priorities (P1, P2, P3, etc.)
   - If data-model.md exists: Extract entities and map to user stories
   - If contracts/ exists: Map endpoints to user stories
   - If research.md exists: Extract decisions for setup tasks
   - Generate tasks organized by user story (see Task Generation Rules below)
   - Generate dependency graph showing user story completion order
   - Create parallel execution examples per user story
   - Validate task completeness (each user story has all needed tasks, independently testable)

4. **Generate tasks.md**: Use `.specify.specify/templates/tasks-template.md` as structure, fill with:
   - Correct feature name from plan.md
   - Phase 1: Setup tasks (project initialization)
   - Phase 2: Foundational tasks (blocking prerequisites for all user stories)
   - Phase 3+: One phase per user story (in priority order from spec.md)
   - Each phase includes: story goal, independent test criteria, tests (if requested), implementation tasks
   - Final Phase: Polish & cross-cutting concerns
   - All tasks must follow the strict checklist format (see Task Generation Rules below)
   - Clear file paths for each task
   - Dependencies section showing story completion order
   - Parallel execution examples per story
   - Implementation strategy section (MVP first, incremental delivery)

5. **Report**: Output path to generated tasks.md and summary:
   - Total task count
   - Task count per user story
   - Parallel opportunities identified
   - Independent test criteria for each story
   - Suggested MVP scope (typically just User Story 1)
   - Format validation: Confirm ALL tasks follow the checklist format (checkbox, ID, labels, file paths)

Context for task generation: $ARGUMENTS

The tasks.md should be immediately executable - each task must be specific enough that an LLM can complete it without additional context.

## Task Generation Rules

**CRITICAL**: Tasks MUST be organized by user story to enable independent implementation and testing.

**Tests are OPTIONAL**: Only generate test tasks if explicitly requested in the feature specification or if user requests TDD approach.

### Checklist Format (REQUIRED)

Every task MUST strictly follow this format:

```text
- [ ] [TaskID] [P?] [Story?] Description with file path
```

**Format Components**:

1. **Checkbox**: ALWAYS start with `- [ ]` (markdown checkbox)
2. **Task ID**: Sequential number (T001, T002, T003...) in execution order
3. **[P] marker**: Include ONLY if task is parallelizable (different files, no dependencies on incomplete tasks)
4. **[Story] label**: REQUIRED for user story phase tasks only
   - Format: [US1], [US2], [US3], etc. (maps to user stories from spec.md)
   - Setup phase: NO story label
   - Foundational phase: NO story label  
   - User Story phases: MUST have story label
   - Polish phase: NO story label
5. **Description**: Clear action with exact file path

**Examples**:

- âœ… CORRECT: `- [ ] T001 Create project structure per implementation plan`
- âœ… CORRECT: `- [ ] T005 [P] Implement authentication middleware in src/middleware/auth.py`
- âœ… CORRECT: `- [ ] T012 [P] [US1] Create User model in src/models/user.py`
- âœ… CORRECT: `- [ ] T014 [US1] Implement UserService in src/services/user_service.py`
- âŒ WRONG: `- [ ] Create User model` (missing ID and Story label)
- âŒ WRONG: `T001 [US1] Create model` (missing checkbox)
- âŒ WRONG: `- [ ] [US1] Create User model` (missing Task ID)
- âŒ WRONG: `- [ ] T001 [US1] Create model` (missing file path)

### Task Organization

1. **From User Stories (spec.md)** - PRIMARY ORGANIZATION:
   - Each user story (P1, P2, P3...) gets its own phase
   - Map all related components to their story:
     - Models needed for that story
     - Services needed for that story
     - Endpoints/UI needed for that story
     - If tests requested: Tests specific to that story
   - Mark story dependencies (most stories should be independent)

2. **From Contracts**:
   - Map each contract/endpoint â†’ to the user story it serves
   - If tests requested: Each contract â†’ contract test task [P] before implementation in that story's phase

3. **From Data Model**:
   - Map each entity to the user story(ies) that need it
   - If entity serves multiple stories: Put in earliest story or Setup phase
   - Relationships â†’ service layer tasks in appropriate story phase

4. **From Setup/Infrastructure**:
   - Shared infrastructure â†’ Setup phase (Phase 1)
   - Foundational/blocking tasks â†’ Foundational phase (Phase 2)
   - Story-specific setup â†’ within that story's phase

### Phase Structure

- **Phase 1**: Setup (project initialization)
- **Phase 2**: Foundational (blocking prerequisites - MUST complete before user stories)
- **Phase 3+**: User Stories in priority order (P1, P2, P3...)
  - Within each story: Tests (if requested) â†’ Models â†’ Services â†’ Endpoints â†’ Integration
  - Each phase should be a complete, independently testable increment
- **Final Phase**: Polish & Cross-Cutting Concerns
````

## File: .claude/commands/speckit.taskstoissues.md
````markdown
---
description: Convert existing tasks into actionable, dependency-ordered GitHub issues for the feature based on available design artifacts.
tools: ['github/github-mcp-server/issue_write']
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. Run `.specify/scripts/bash/check-prerequisites.sh --json --require-tasks --include-tasks` from repo root and parse FEATURE_DIR and AVAILABLE_DOCS list. All paths must be absolute. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").
1. From the executed script, extract the path to **tasks**.
1. Get the Git remote by running:

```bash
git config --get remote.origin.url
```

**ONLY PROCEED TO NEXT STEPS IF THE REMOTE IS A GITHUB URL**

1. For each task in the list, use the GitHub MCP server to create a new issue in the repository that is representative of the Git remote.

**UNDER NO CIRCUMSTANCES EVER CREATE ISSUES IN REPOSITORIES THAT DO NOT MATCH THE REMOTE URL**
````

## File: .specify/scripts/bash/check-prerequisites.sh
````bash
#!/usr/bin/env bash

# Consolidated prerequisite checking script
#
# This script provides unified prerequisite checking for Spec-Driven Development workflow.
# It replaces the functionality previously spread across multiple scripts.
#
# Usage: ./check-prerequisites.sh [OPTIONS]
#
# OPTIONS:
#   --json              Output in JSON format
#   --require-tasks     Require tasks.md to exist (for implementation phase)
#   --include-tasks     Include tasks.md in AVAILABLE_DOCS list
#   --paths-only        Only output path variables (no validation)
#   --help, -h          Show help message
#
# OUTPUTS:
#   JSON mode: {"FEATURE_DIR":"...", "AVAILABLE_DOCS":["..."]}
#   Text mode: FEATURE_DIR:... \n AVAILABLE_DOCS: \n âœ“/âœ— file.md
#   Paths only: REPO_ROOT: ... \n BRANCH: ... \n FEATURE_DIR: ... etc.

set -e

# Parse command line arguments
JSON_MODE=false
REQUIRE_TASKS=false
INCLUDE_TASKS=false
PATHS_ONLY=false

for arg in "$@"; do
    case "$arg" in
        --json)
            JSON_MODE=true
            ;;
        --require-tasks)
            REQUIRE_TASKS=true
            ;;
        --include-tasks)
            INCLUDE_TASKS=true
            ;;
        --paths-only)
            PATHS_ONLY=true
            ;;
        --help|-h)
            cat << 'EOF'
Usage: check-prerequisites.sh [OPTIONS]

Consolidated prerequisite checking for Spec-Driven Development workflow.

OPTIONS:
  --json              Output in JSON format
  --require-tasks     Require tasks.md to exist (for implementation phase)
  --include-tasks     Include tasks.md in AVAILABLE_DOCS list
  --paths-only        Only output path variables (no prerequisite validation)
  --help, -h          Show this help message

EXAMPLES:
  # Check task prerequisites (plan.md required)
  ./check-prerequisites.sh --json
  
  # Check implementation prerequisites (plan.md + tasks.md required)
  ./check-prerequisites.sh --json --require-tasks --include-tasks
  
  # Get feature paths only (no validation)
  ./check-prerequisites.sh --paths-only
  
EOF
            exit 0
            ;;
        *)
            echo "ERROR: Unknown option '$arg'. Use --help for usage information." >&2
            exit 1
            ;;
    esac
done

# Source common functions
SCRIPT_DIR="$(CDPATH="" cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/common.sh"

# Get feature paths and validate branch
eval $(get_feature_paths)
check_feature_branch "$CURRENT_BRANCH" "$HAS_GIT" || exit 1

# If paths-only mode, output paths and exit (support JSON + paths-only combined)
if $PATHS_ONLY; then
    if $JSON_MODE; then
        # Minimal JSON paths payload (no validation performed)
        printf '{"REPO_ROOT":"%s","BRANCH":"%s","FEATURE_DIR":"%s","FEATURE_SPEC":"%s","IMPL_PLAN":"%s","TASKS":"%s"}\n' \
            "$REPO_ROOT" "$CURRENT_BRANCH" "$FEATURE_DIR" "$FEATURE_SPEC" "$IMPL_PLAN" "$TASKS"
    else
        echo "REPO_ROOT: $REPO_ROOT"
        echo "BRANCH: $CURRENT_BRANCH"
        echo "FEATURE_DIR: $FEATURE_DIR"
        echo "FEATURE_SPEC: $FEATURE_SPEC"
        echo "IMPL_PLAN: $IMPL_PLAN"
        echo "TASKS: $TASKS"
    fi
    exit 0
fi

# Validate required directories and files
if [[ ! -d "$FEATURE_DIR" ]]; then
    echo "ERROR: Feature directory not found: $FEATURE_DIR" >&2
    echo "Run /speckit.specify first to create the feature structure." >&2
    exit 1
fi

if [[ ! -f "$IMPL_PLAN" ]]; then
    echo "ERROR: plan.md not found in $FEATURE_DIR" >&2
    echo "Run /speckit.plan first to create the implementation plan." >&2
    exit 1
fi

# Check for tasks.md if required
if $REQUIRE_TASKS && [[ ! -f "$TASKS" ]]; then
    echo "ERROR: tasks.md not found in $FEATURE_DIR" >&2
    echo "Run /speckit.tasks first to create the task list." >&2
    exit 1
fi

# Build list of available documents
docs=()

# Always check these optional docs
[[ -f "$RESEARCH" ]] && docs+=("research.md")
[[ -f "$DATA_MODEL" ]] && docs+=("data-model.md")

# Check contracts directory (only if it exists and has files)
if [[ -d "$CONTRACTS_DIR" ]] && [[ -n "$(ls -A "$CONTRACTS_DIR" 2>/dev/null)" ]]; then
    docs+=("contracts/")
fi

[[ -f "$QUICKSTART" ]] && docs+=("quickstart.md")

# Include tasks.md if requested and it exists
if $INCLUDE_TASKS && [[ -f "$TASKS" ]]; then
    docs+=("tasks.md")
fi

# Output results
if $JSON_MODE; then
    # Build JSON array of documents
    if [[ ${#docs[@]} -eq 0 ]]; then
        json_docs="[]"
    else
        json_docs=$(printf '"%s",' "${docs[@]}")
        json_docs="[${json_docs%,}]"
    fi
    
    printf '{"FEATURE_DIR":"%s","AVAILABLE_DOCS":%s}\n' "$FEATURE_DIR" "$json_docs"
else
    # Text output
    echo "FEATURE_DIR:$FEATURE_DIR"
    echo "AVAILABLE_DOCS:"
    
    # Show status of each potential document
    check_file "$RESEARCH" "research.md"
    check_file "$DATA_MODEL" "data-model.md"
    check_dir "$CONTRACTS_DIR" "contracts/"
    check_file "$QUICKSTART" "quickstart.md"
    
    if $INCLUDE_TASKS; then
        check_file "$TASKS" "tasks.md"
    fi
fi
````

## File: .specify/scripts/bash/common.sh
````bash
#!/usr/bin/env bash
# Common functions and variables for all scripts

# Get repository root, with fallback for non-git repositories
get_repo_root() {
    if git rev-parse --show-toplevel >/dev/null 2>&1; then
        git rev-parse --show-toplevel
    else
        # Fall back to script location for non-git repos
        local script_dir="$(CDPATH="" cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
        (cd "$script_dir/../../.." && pwd)
    fi
}

# Get current branch, with fallback for non-git repositories
get_current_branch() {
    # First check if SPECIFY_FEATURE environment variable is set
    if [[ -n "${SPECIFY_FEATURE:-}" ]]; then
        echo "$SPECIFY_FEATURE"
        return
    fi

    # Then check git if available
    if git rev-parse --abbrev-ref HEAD >/dev/null 2>&1; then
        git rev-parse --abbrev-ref HEAD
        return
    fi

    # For non-git repos, try to find the latest feature directory
    local repo_root=$(get_repo_root)
    local specs_dir="$repo_root/specs"

    if [[ -d "$specs_dir" ]]; then
        local latest_feature=""
        local highest=0

        for dir in "$specs_dir"/*; do
            if [[ -d "$dir" ]]; then
                local dirname=$(basename "$dir")
                if [[ "$dirname" =~ ^([0-9]{3})- ]]; then
                    local number=${BASH_REMATCH[1]}
                    number=$((10#$number))
                    if [[ "$number" -gt "$highest" ]]; then
                        highest=$number
                        latest_feature=$dirname
                    fi
                fi
            fi
        done

        if [[ -n "$latest_feature" ]]; then
            echo "$latest_feature"
            return
        fi
    fi

    echo "main"  # Final fallback
}

# Check if we have git available
has_git() {
    git rev-parse --show-toplevel >/dev/null 2>&1
}

check_feature_branch() {
    local branch="$1"
    local has_git_repo="$2"

    # For non-git repos, we can't enforce branch naming but still provide output
    if [[ "$has_git_repo" != "true" ]]; then
        echo "[specify] Warning: Git repository not detected; skipped branch validation" >&2
        return 0
    fi

    if [[ ! "$branch" =~ ^[0-9]{3}- ]]; then
        echo "ERROR: Not on a feature branch. Current branch: $branch" >&2
        echo "Feature branches should be named like: 001-feature-name" >&2
        return 1
    fi

    return 0
}

get_feature_dir() { echo "$1/specs/$2"; }

# Find feature directory by numeric prefix instead of exact branch match
# This allows multiple branches to work on the same spec (e.g., 004-fix-bug, 004-add-feature)
find_feature_dir_by_prefix() {
    local repo_root="$1"
    local branch_name="$2"
    local specs_dir="$repo_root/specs"

    # Extract numeric prefix from branch (e.g., "004" from "004-whatever")
    if [[ ! "$branch_name" =~ ^([0-9]{3})- ]]; then
        # If branch doesn't have numeric prefix, fall back to exact match
        echo "$specs_dir/$branch_name"
        return
    fi

    local prefix="${BASH_REMATCH[1]}"

    # Search for directories in specs/ that start with this prefix
    local matches=()
    if [[ -d "$specs_dir" ]]; then
        for dir in "$specs_dir"/"$prefix"-*; do
            if [[ -d "$dir" ]]; then
                matches+=("$(basename "$dir")")
            fi
        done
    fi

    # Handle results
    if [[ ${#matches[@]} -eq 0 ]]; then
        # No match found - return the branch name path (will fail later with clear error)
        echo "$specs_dir/$branch_name"
    elif [[ ${#matches[@]} -eq 1 ]]; then
        # Exactly one match - perfect!
        echo "$specs_dir/${matches[0]}"
    else
        # Multiple matches - this shouldn't happen with proper naming convention
        echo "ERROR: Multiple spec directories found with prefix '$prefix': ${matches[*]}" >&2
        echo "Please ensure only one spec directory exists per numeric prefix." >&2
        echo "$specs_dir/$branch_name"  # Return something to avoid breaking the script
    fi
}

get_feature_paths() {
    local repo_root=$(get_repo_root)
    local current_branch=$(get_current_branch)
    local has_git_repo="false"

    if has_git; then
        has_git_repo="true"
    fi

    # Use prefix-based lookup to support multiple branches per spec
    local feature_dir=$(find_feature_dir_by_prefix "$repo_root" "$current_branch")

    cat <<EOF
REPO_ROOT='$repo_root'
CURRENT_BRANCH='$current_branch'
HAS_GIT='$has_git_repo'
FEATURE_DIR='$feature_dir'
FEATURE_SPEC='$feature_dir/spec.md'
IMPL_PLAN='$feature_dir/plan.md'
TASKS='$feature_dir/tasks.md'
RESEARCH='$feature_dir/research.md'
DATA_MODEL='$feature_dir/data-model.md'
QUICKSTART='$feature_dir/quickstart.md'
CONTRACTS_DIR='$feature_dir/contracts'
EOF
}

check_file() { [[ -f "$1" ]] && echo "  âœ“ $2" || echo "  âœ— $2"; }
check_dir() { [[ -d "$1" && -n $(ls -A "$1" 2>/dev/null) ]] && echo "  âœ“ $2" || echo "  âœ— $2"; }
````

## File: .specify/scripts/bash/create-new-feature.sh
````bash
#!/usr/bin/env bash

set -e

JSON_MODE=false
SHORT_NAME=""
BRANCH_NUMBER=""
ARGS=()
i=1
while [ $i -le $# ]; do
    arg="${!i}"
    case "$arg" in
        --json) 
            JSON_MODE=true 
            ;;
        --short-name)
            if [ $((i + 1)) -gt $# ]; then
                echo 'Error: --short-name requires a value' >&2
                exit 1
            fi
            i=$((i + 1))
            next_arg="${!i}"
            # Check if the next argument is another option (starts with --)
            if [[ "$next_arg" == --* ]]; then
                echo 'Error: --short-name requires a value' >&2
                exit 1
            fi
            SHORT_NAME="$next_arg"
            ;;
        --number)
            if [ $((i + 1)) -gt $# ]; then
                echo 'Error: --number requires a value' >&2
                exit 1
            fi
            i=$((i + 1))
            next_arg="${!i}"
            if [[ "$next_arg" == --* ]]; then
                echo 'Error: --number requires a value' >&2
                exit 1
            fi
            BRANCH_NUMBER="$next_arg"
            ;;
        --help|-h) 
            echo "Usage: $0 [--json] [--short-name <name>] [--number N] <feature_description>"
            echo ""
            echo "Options:"
            echo "  --json              Output in JSON format"
            echo "  --short-name <name> Provide a custom short name (2-4 words) for the branch"
            echo "  --number N          Specify branch number manually (overrides auto-detection)"
            echo "  --help, -h          Show this help message"
            echo ""
            echo "Examples:"
            echo "  $0 'Add user authentication system' --short-name 'user-auth'"
            echo "  $0 'Implement OAuth2 integration for API' --number 5"
            exit 0
            ;;
        *) 
            ARGS+=("$arg") 
            ;;
    esac
    i=$((i + 1))
done

FEATURE_DESCRIPTION="${ARGS[*]}"
if [ -z "$FEATURE_DESCRIPTION" ]; then
    echo "Usage: $0 [--json] [--short-name <name>] [--number N] <feature_description>" >&2
    exit 1
fi

# Function to find the repository root by searching for existing project markers
find_repo_root() {
    local dir="$1"
    while [ "$dir" != "/" ]; do
        if [ -d "$dir/.git" ] || [ -d "$dir/.specify" ]; then
            echo "$dir"
            return 0
        fi
        dir="$(dirname "$dir")"
    done
    return 1
}

# Function to get highest number from specs directory
get_highest_from_specs() {
    local specs_dir="$1"
    local highest=0
    
    if [ -d "$specs_dir" ]; then
        for dir in "$specs_dir"/*; do
            [ -d "$dir" ] || continue
            dirname=$(basename "$dir")
            number=$(echo "$dirname" | grep -o '^[0-9]\+' || echo "0")
            number=$((10#$number))
            if [ "$number" -gt "$highest" ]; then
                highest=$number
            fi
        done
    fi
    
    echo "$highest"
}

# Function to get highest number from git branches
get_highest_from_branches() {
    local highest=0
    
    # Get all branches (local and remote)
    branches=$(git branch -a 2>/dev/null || echo "")
    
    if [ -n "$branches" ]; then
        while IFS= read -r branch; do
            # Clean branch name: remove leading markers and remote prefixes
            clean_branch=$(echo "$branch" | sed 's/^[* ]*//; s|^remotes/[^/]*/||')
            
            # Extract feature number if branch matches pattern ###-*
            if echo "$clean_branch" | grep -q '^[0-9]\{3\}-'; then
                number=$(echo "$clean_branch" | grep -o '^[0-9]\{3\}' || echo "0")
                number=$((10#$number))
                if [ "$number" -gt "$highest" ]; then
                    highest=$number
                fi
            fi
        done <<< "$branches"
    fi
    
    echo "$highest"
}

# Function to check existing branches (local and remote) and return next available number
check_existing_branches() {
    local short_name="$1"
    local specs_dir="$2"
    
    # Fetch all remotes to get latest branch info (suppress errors if no remotes)
    git fetch --all --prune 2>/dev/null || true
    
    # Find all branches matching the pattern using git ls-remote (more reliable)
    local remote_branches=$(git ls-remote --heads origin 2>/dev/null | grep -E "refs/heads/[0-9]+-${short_name}$" | sed 's/.*\/\([0-9]*\)-.*/\1/' | sort -n)
    
    # Also check local branches
    local local_branches=$(git branch 2>/dev/null | grep -E "^[* ]*[0-9]+-${short_name}$" | sed 's/^[* ]*//' | sed 's/-.*//' | sort -n)
    
    # Check specs directory as well
    local spec_dirs=""
    if [ -d "$specs_dir" ]; then
        spec_dirs=$(find "$specs_dir" -maxdepth 1 -type d -name "[0-9]*-${short_name}" 2>/dev/null | xargs -n1 basename 2>/dev/null | sed 's/-.*//' | sort -n)
    fi
    
    # Combine all sources and get the highest number
    local max_num=0
    for num in $remote_branches $local_branches $spec_dirs; do
        if [ "$num" -gt "$max_num" ]; then
            max_num=$num
        fi
    done
    
    # Return next number
    echo $((max_num + 1))
}

# Function to clean and format a branch name
clean_branch_name() {
    local name="$1"
    echo "$name" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/-\+/-/g' | sed 's/^-//' | sed 's/-$//'
}

# Resolve repository root. Prefer git information when available, but fall back
# to searching for repository markers so the workflow still functions in repositories that
# were initialised with --no-git.
SCRIPT_DIR="$(CDPATH="" cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

if git rev-parse --show-toplevel >/dev/null 2>&1; then
    REPO_ROOT=$(git rev-parse --show-toplevel)
    HAS_GIT=true
else
    REPO_ROOT="$(find_repo_root "$SCRIPT_DIR")"
    if [ -z "$REPO_ROOT" ]; then
        echo "Error: Could not determine repository root. Please run this script from within the repository." >&2
        exit 1
    fi
    HAS_GIT=false
fi

cd "$REPO_ROOT"

SPECS_DIR="$REPO_ROOT/specs"
mkdir -p "$SPECS_DIR"

# Function to generate branch name with stop word filtering and length filtering
generate_branch_name() {
    local description="$1"
    
    # Common stop words to filter out
    local stop_words="^(i|a|an|the|to|for|of|in|on|at|by|with|from|is|are|was|were|be|been|being|have|has|had|do|does|did|will|would|should|could|can|may|might|must|shall|this|that|these|those|my|your|our|their|want|need|add|get|set)$"
    
    # Convert to lowercase and split into words
    local clean_name=$(echo "$description" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/ /g')
    
    # Filter words: remove stop words and words shorter than 3 chars (unless they're uppercase acronyms in original)
    local meaningful_words=()
    for word in $clean_name; do
        # Skip empty words
        [ -z "$word" ] && continue
        
        # Keep words that are NOT stop words AND (length >= 3 OR are potential acronyms)
        if ! echo "$word" | grep -qiE "$stop_words"; then
            if [ ${#word} -ge 3 ]; then
                meaningful_words+=("$word")
            elif echo "$description" | grep -q "\b${word^^}\b"; then
                # Keep short words if they appear as uppercase in original (likely acronyms)
                meaningful_words+=("$word")
            fi
        fi
    done
    
    # If we have meaningful words, use first 3-4 of them
    if [ ${#meaningful_words[@]} -gt 0 ]; then
        local max_words=3
        if [ ${#meaningful_words[@]} -eq 4 ]; then max_words=4; fi
        
        local result=""
        local count=0
        for word in "${meaningful_words[@]}"; do
            if [ $count -ge $max_words ]; then break; fi
            if [ -n "$result" ]; then result="$result-"; fi
            result="$result$word"
            count=$((count + 1))
        done
        echo "$result"
    else
        # Fallback to original logic if no meaningful words found
        local cleaned=$(clean_branch_name "$description")
        echo "$cleaned" | tr '-' '\n' | grep -v '^$' | head -3 | tr '\n' '-' | sed 's/-$//'
    fi
}

# Generate branch name
if [ -n "$SHORT_NAME" ]; then
    # Use provided short name, just clean it up
    BRANCH_SUFFIX=$(clean_branch_name "$SHORT_NAME")
else
    # Generate from description with smart filtering
    BRANCH_SUFFIX=$(generate_branch_name "$FEATURE_DESCRIPTION")
fi

# Determine branch number
if [ -z "$BRANCH_NUMBER" ]; then
    if [ "$HAS_GIT" = true ]; then
        # Check existing branches on remotes
        BRANCH_NUMBER=$(check_existing_branches "$BRANCH_SUFFIX" "$SPECS_DIR")
    else
        # Fall back to local directory check
        HIGHEST=$(get_highest_from_specs "$SPECS_DIR")
        BRANCH_NUMBER=$((HIGHEST + 1))
    fi
fi

FEATURE_NUM=$(printf "%03d" "$BRANCH_NUMBER")
BRANCH_NAME="${FEATURE_NUM}-${BRANCH_SUFFIX}"

# GitHub enforces a 244-byte limit on branch names
# Validate and truncate if necessary
MAX_BRANCH_LENGTH=244
if [ ${#BRANCH_NAME} -gt $MAX_BRANCH_LENGTH ]; then
    # Calculate how much we need to trim from suffix
    # Account for: feature number (3) + hyphen (1) = 4 chars
    MAX_SUFFIX_LENGTH=$((MAX_BRANCH_LENGTH - 4))
    
    # Truncate suffix at word boundary if possible
    TRUNCATED_SUFFIX=$(echo "$BRANCH_SUFFIX" | cut -c1-$MAX_SUFFIX_LENGTH)
    # Remove trailing hyphen if truncation created one
    TRUNCATED_SUFFIX=$(echo "$TRUNCATED_SUFFIX" | sed 's/-$//')
    
    ORIGINAL_BRANCH_NAME="$BRANCH_NAME"
    BRANCH_NAME="${FEATURE_NUM}-${TRUNCATED_SUFFIX}"
    
    >&2 echo "[specify] Warning: Branch name exceeded GitHub's 244-byte limit"
    >&2 echo "[specify] Original: $ORIGINAL_BRANCH_NAME (${#ORIGINAL_BRANCH_NAME} bytes)"
    >&2 echo "[specify] Truncated to: $BRANCH_NAME (${#BRANCH_NAME} bytes)"
fi

if [ "$HAS_GIT" = true ]; then
    git checkout -b "$BRANCH_NAME"
else
    >&2 echo "[specify] Warning: Git repository not detected; skipped branch creation for $BRANCH_NAME"
fi

FEATURE_DIR="$SPECS_DIR/$BRANCH_NAME"
mkdir -p "$FEATURE_DIR"

TEMPLATE="$REPO_ROOT/.specify/templates/spec-template.md"
SPEC_FILE="$FEATURE_DIR/spec.md"
if [ -f "$TEMPLATE" ]; then cp "$TEMPLATE" "$SPEC_FILE"; else touch "$SPEC_FILE"; fi

# Set the SPECIFY_FEATURE environment variable for the current session
export SPECIFY_FEATURE="$BRANCH_NAME"

if $JSON_MODE; then
    printf '{"BRANCH_NAME":"%s","SPEC_FILE":"%s","FEATURE_NUM":"%s"}\n' "$BRANCH_NAME" "$SPEC_FILE" "$FEATURE_NUM"
else
    echo "BRANCH_NAME: $BRANCH_NAME"
    echo "SPEC_FILE: $SPEC_FILE"
    echo "FEATURE_NUM: $FEATURE_NUM"
    echo "SPECIFY_FEATURE environment variable set to: $BRANCH_NAME"
fi
````

## File: .specify/scripts/bash/setup-plan.sh
````bash
#!/usr/bin/env bash

set -e

# Parse command line arguments
JSON_MODE=false
ARGS=()

for arg in "$@"; do
    case "$arg" in
        --json) 
            JSON_MODE=true 
            ;;
        --help|-h) 
            echo "Usage: $0 [--json]"
            echo "  --json    Output results in JSON format"
            echo "  --help    Show this help message"
            exit 0 
            ;;
        *) 
            ARGS+=("$arg") 
            ;;
    esac
done

# Get script directory and load common functions
SCRIPT_DIR="$(CDPATH="" cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/common.sh"

# Get all paths and variables from common functions
eval $(get_feature_paths)

# Check if we're on a proper feature branch (only for git repos)
check_feature_branch "$CURRENT_BRANCH" "$HAS_GIT" || exit 1

# Ensure the feature directory exists
mkdir -p "$FEATURE_DIR"

# Copy plan template if it exists
TEMPLATE="$REPO_ROOT/.specify/templates/plan-template.md"
if [[ -f "$TEMPLATE" ]]; then
    cp "$TEMPLATE" "$IMPL_PLAN"
    echo "Copied plan template to $IMPL_PLAN"
else
    echo "Warning: Plan template not found at $TEMPLATE"
    # Create a basic plan file if template doesn't exist
    touch "$IMPL_PLAN"
fi

# Output results
if $JSON_MODE; then
    printf '{"FEATURE_SPEC":"%s","IMPL_PLAN":"%s","SPECS_DIR":"%s","BRANCH":"%s","HAS_GIT":"%s"}\n' \
        "$FEATURE_SPEC" "$IMPL_PLAN" "$FEATURE_DIR" "$CURRENT_BRANCH" "$HAS_GIT"
else
    echo "FEATURE_SPEC: $FEATURE_SPEC"
    echo "IMPL_PLAN: $IMPL_PLAN" 
    echo "SPECS_DIR: $FEATURE_DIR"
    echo "BRANCH: $CURRENT_BRANCH"
    echo "HAS_GIT: $HAS_GIT"
fi
````

## File: .specify/scripts/bash/update-agent-context.sh
````bash
#!/usr/bin/env bash

# Update agent context files with information from plan.md
#
# This script maintains AI agent context files by parsing feature specifications 
# and updating agent-specific configuration files with project information.
#
# MAIN FUNCTIONS:
# 1. Environment Validation
#    - Verifies git repository structure and branch information
#    - Checks for required plan.md files and templates
#    - Validates file permissions and accessibility
#
# 2. Plan Data Extraction
#    - Parses plan.md files to extract project metadata
#    - Identifies language/version, frameworks, databases, and project types
#    - Handles missing or incomplete specification data gracefully
#
# 3. Agent File Management
#    - Creates new agent context files from templates when needed
#    - Updates existing agent files with new project information
#    - Preserves manual additions and custom configurations
#    - Supports multiple AI agent formats and directory structures
#
# 4. Content Generation
#    - Generates language-specific build/test commands
#    - Creates appropriate project directory structures
#    - Updates technology stacks and recent changes sections
#    - Maintains consistent formatting and timestamps
#
# 5. Multi-Agent Support
#    - Handles agent-specific file paths and naming conventions
#    - Supports: Claude, Gemini, Copilot, Cursor, Qwen, opencode, Codex, Windsurf, Kilo Code, Auggie CLI, Roo Code, CodeBuddy CLI, Amp, SHAI, or Amazon Q Developer CLI
#    - Can update single agents or all existing agent files
#    - Creates default Claude file if no agent files exist
#
# Usage: ./update-agent-context.sh [agent_type]
# Agent types: claude|gemini|copilot|cursor-agent|qwen|opencode|codex|windsurf|kilocode|auggie|shai|q
# Leave empty to update all existing agent files

set -e

# Enable strict error handling
set -u
set -o pipefail

#==============================================================================
# Configuration and Global Variables
#==============================================================================

# Get script directory and load common functions
SCRIPT_DIR="$(CDPATH="" cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/common.sh"

# Get all paths and variables from common functions
eval $(get_feature_paths)

NEW_PLAN="$IMPL_PLAN"  # Alias for compatibility with existing code
AGENT_TYPE="${1:-}"

# Agent-specific file paths  
CLAUDE_FILE="$REPO_ROOT/CLAUDE.md"
GEMINI_FILE="$REPO_ROOT/GEMINI.md"
COPILOT_FILE="$REPO_ROOT/.github/agents/copilot-instructions.md"
CURSOR_FILE="$REPO_ROOT/.cursor/rules/specify-rules.mdc"
QWEN_FILE="$REPO_ROOT/QWEN.md"
AGENTS_FILE="$REPO_ROOT/AGENTS.md"
WINDSURF_FILE="$REPO_ROOT/.windsurf/rules/specify-rules.md"
KILOCODE_FILE="$REPO_ROOT/.kilocode/rules/specify-rules.md"
AUGGIE_FILE="$REPO_ROOT/.augment/rules/specify-rules.md"
ROO_FILE="$REPO_ROOT/.roo/rules/specify-rules.md"
CODEBUDDY_FILE="$REPO_ROOT/CODEBUDDY.md"
AMP_FILE="$REPO_ROOT/AGENTS.md"
SHAI_FILE="$REPO_ROOT/SHAI.md"
Q_FILE="$REPO_ROOT/AGENTS.md"

# Template file
TEMPLATE_FILE="$REPO_ROOT/.specify/templates/agent-file-template.md"

# Global variables for parsed plan data
NEW_LANG=""
NEW_FRAMEWORK=""
NEW_DB=""
NEW_PROJECT_TYPE=""

#==============================================================================
# Utility Functions
#==============================================================================

log_info() {
    echo "INFO: $1"
}

log_success() {
    echo "âœ“ $1"
}

log_error() {
    echo "ERROR: $1" >&2
}

log_warning() {
    echo "WARNING: $1" >&2
}

# Cleanup function for temporary files
cleanup() {
    local exit_code=$?
    rm -f /tmp/agent_update_*_$$
    rm -f /tmp/manual_additions_$$
    exit $exit_code
}

# Set up cleanup trap
trap cleanup EXIT INT TERM

#==============================================================================
# Validation Functions
#==============================================================================

validate_environment() {
    # Check if we have a current branch/feature (git or non-git)
    if [[ -z "$CURRENT_BRANCH" ]]; then
        log_error "Unable to determine current feature"
        if [[ "$HAS_GIT" == "true" ]]; then
            log_info "Make sure you're on a feature branch"
        else
            log_info "Set SPECIFY_FEATURE environment variable or create a feature first"
        fi
        exit 1
    fi
    
    # Check if plan.md exists
    if [[ ! -f "$NEW_PLAN" ]]; then
        log_error "No plan.md found at $NEW_PLAN"
        log_info "Make sure you're working on a feature with a corresponding spec directory"
        if [[ "$HAS_GIT" != "true" ]]; then
            log_info "Use: export SPECIFY_FEATURE=your-feature-name or create a new feature first"
        fi
        exit 1
    fi
    
    # Check if template exists (needed for new files)
    if [[ ! -f "$TEMPLATE_FILE" ]]; then
        log_warning "Template file not found at $TEMPLATE_FILE"
        log_warning "Creating new agent files will fail"
    fi
}

#==============================================================================
# Plan Parsing Functions
#==============================================================================

extract_plan_field() {
    local field_pattern="$1"
    local plan_file="$2"
    
    grep "^\*\*${field_pattern}\*\*: " "$plan_file" 2>/dev/null | \
        head -1 | \
        sed "s|^\*\*${field_pattern}\*\*: ||" | \
        sed 's/^[ \t]*//;s/[ \t]*$//' | \
        grep -v "NEEDS CLARIFICATION" | \
        grep -v "^N/A$" || echo ""
}

parse_plan_data() {
    local plan_file="$1"
    
    if [[ ! -f "$plan_file" ]]; then
        log_error "Plan file not found: $plan_file"
        return 1
    fi
    
    if [[ ! -r "$plan_file" ]]; then
        log_error "Plan file is not readable: $plan_file"
        return 1
    fi
    
    log_info "Parsing plan data from $plan_file"
    
    NEW_LANG=$(extract_plan_field "Language/Version" "$plan_file")
    NEW_FRAMEWORK=$(extract_plan_field "Primary Dependencies" "$plan_file")
    NEW_DB=$(extract_plan_field "Storage" "$plan_file")
    NEW_PROJECT_TYPE=$(extract_plan_field "Project Type" "$plan_file")
    
    # Log what we found
    if [[ -n "$NEW_LANG" ]]; then
        log_info "Found language: $NEW_LANG"
    else
        log_warning "No language information found in plan"
    fi
    
    if [[ -n "$NEW_FRAMEWORK" ]]; then
        log_info "Found framework: $NEW_FRAMEWORK"
    fi
    
    if [[ -n "$NEW_DB" ]] && [[ "$NEW_DB" != "N/A" ]]; then
        log_info "Found database: $NEW_DB"
    fi
    
    if [[ -n "$NEW_PROJECT_TYPE" ]]; then
        log_info "Found project type: $NEW_PROJECT_TYPE"
    fi
}

format_technology_stack() {
    local lang="$1"
    local framework="$2"
    local parts=()
    
    # Add non-empty parts
    [[ -n "$lang" && "$lang" != "NEEDS CLARIFICATION" ]] && parts+=("$lang")
    [[ -n "$framework" && "$framework" != "NEEDS CLARIFICATION" && "$framework" != "N/A" ]] && parts+=("$framework")
    
    # Join with proper formatting
    if [[ ${#parts[@]} -eq 0 ]]; then
        echo ""
    elif [[ ${#parts[@]} -eq 1 ]]; then
        echo "${parts[0]}"
    else
        # Join multiple parts with " + "
        local result="${parts[0]}"
        for ((i=1; i<${#parts[@]}; i++)); do
            result="$result + ${parts[i]}"
        done
        echo "$result"
    fi
}

#==============================================================================
# Template and Content Generation Functions
#==============================================================================

get_project_structure() {
    local project_type="$1"
    
    if [[ "$project_type" == *"web"* ]]; then
        echo "backend/\\nfrontend/\\ntests/"
    else
        echo "src/\\ntests/"
    fi
}

get_commands_for_language() {
    local lang="$1"
    
    case "$lang" in
        *"Python"*)
            echo "cd src && pytest && ruff check ."
            ;;
        *"Rust"*)
            echo "cargo test && cargo clippy"
            ;;
        *"JavaScript"*|*"TypeScript"*)
            echo "npm test \\&\\& npm run lint"
            ;;
        *)
            echo "# Add commands for $lang"
            ;;
    esac
}

get_language_conventions() {
    local lang="$1"
    echo "$lang: Follow standard conventions"
}

create_new_agent_file() {
    local target_file="$1"
    local temp_file="$2"
    local project_name="$3"
    local current_date="$4"
    
    if [[ ! -f "$TEMPLATE_FILE" ]]; then
        log_error "Template not found at $TEMPLATE_FILE"
        return 1
    fi
    
    if [[ ! -r "$TEMPLATE_FILE" ]]; then
        log_error "Template file is not readable: $TEMPLATE_FILE"
        return 1
    fi
    
    log_info "Creating new agent context file from template..."
    
    if ! cp "$TEMPLATE_FILE" "$temp_file"; then
        log_error "Failed to copy template file"
        return 1
    fi
    
    # Replace template placeholders
    local project_structure
    project_structure=$(get_project_structure "$NEW_PROJECT_TYPE")
    
    local commands
    commands=$(get_commands_for_language "$NEW_LANG")
    
    local language_conventions
    language_conventions=$(get_language_conventions "$NEW_LANG")
    
    # Perform substitutions with error checking using safer approach
    # Escape special characters for sed by using a different delimiter or escaping
    local escaped_lang=$(printf '%s\n' "$NEW_LANG" | sed 's/[\[\.*^$()+{}|]/\\&/g')
    local escaped_framework=$(printf '%s\n' "$NEW_FRAMEWORK" | sed 's/[\[\.*^$()+{}|]/\\&/g')
    local escaped_branch=$(printf '%s\n' "$CURRENT_BRANCH" | sed 's/[\[\.*^$()+{}|]/\\&/g')
    
    # Build technology stack and recent change strings conditionally
    local tech_stack
    if [[ -n "$escaped_lang" && -n "$escaped_framework" ]]; then
        tech_stack="- $escaped_lang + $escaped_framework ($escaped_branch)"
    elif [[ -n "$escaped_lang" ]]; then
        tech_stack="- $escaped_lang ($escaped_branch)"
    elif [[ -n "$escaped_framework" ]]; then
        tech_stack="- $escaped_framework ($escaped_branch)"
    else
        tech_stack="- ($escaped_branch)"
    fi

    local recent_change
    if [[ -n "$escaped_lang" && -n "$escaped_framework" ]]; then
        recent_change="- $escaped_branch: Added $escaped_lang + $escaped_framework"
    elif [[ -n "$escaped_lang" ]]; then
        recent_change="- $escaped_branch: Added $escaped_lang"
    elif [[ -n "$escaped_framework" ]]; then
        recent_change="- $escaped_branch: Added $escaped_framework"
    else
        recent_change="- $escaped_branch: Added"
    fi

    local substitutions=(
        "s|\[PROJECT NAME\]|$project_name|"
        "s|\[DATE\]|$current_date|"
        "s|\[EXTRACTED FROM ALL PLAN.MD FILES\]|$tech_stack|"
        "s|\[ACTUAL STRUCTURE FROM PLANS\]|$project_structure|g"
        "s|\[ONLY COMMANDS FOR ACTIVE TECHNOLOGIES\]|$commands|"
        "s|\[LANGUAGE-SPECIFIC, ONLY FOR LANGUAGES IN USE\]|$language_conventions|"
        "s|\[LAST 3 FEATURES AND WHAT THEY ADDED\]|$recent_change|"
    )
    
    for substitution in "${substitutions[@]}"; do
        if ! sed -i.bak -e "$substitution" "$temp_file"; then
            log_error "Failed to perform substitution: $substitution"
            rm -f "$temp_file" "$temp_file.bak"
            return 1
        fi
    done
    
    # Convert \n sequences to actual newlines
    newline=$(printf '\n')
    sed -i.bak2 "s/\\\\n/${newline}/g" "$temp_file"
    
    # Clean up backup files
    rm -f "$temp_file.bak" "$temp_file.bak2"
    
    return 0
}




update_existing_agent_file() {
    local target_file="$1"
    local current_date="$2"
    
    log_info "Updating existing agent context file..."
    
    # Use a single temporary file for atomic update
    local temp_file
    temp_file=$(mktemp) || {
        log_error "Failed to create temporary file"
        return 1
    }
    
    # Process the file in one pass
    local tech_stack=$(format_technology_stack "$NEW_LANG" "$NEW_FRAMEWORK")
    local new_tech_entries=()
    local new_change_entry=""
    
    # Prepare new technology entries
    if [[ -n "$tech_stack" ]] && ! grep -q "$tech_stack" "$target_file"; then
        new_tech_entries+=("- $tech_stack ($CURRENT_BRANCH)")
    fi
    
    if [[ -n "$NEW_DB" ]] && [[ "$NEW_DB" != "N/A" ]] && [[ "$NEW_DB" != "NEEDS CLARIFICATION" ]] && ! grep -q "$NEW_DB" "$target_file"; then
        new_tech_entries+=("- $NEW_DB ($CURRENT_BRANCH)")
    fi
    
    # Prepare new change entry
    if [[ -n "$tech_stack" ]]; then
        new_change_entry="- $CURRENT_BRANCH: Added $tech_stack"
    elif [[ -n "$NEW_DB" ]] && [[ "$NEW_DB" != "N/A" ]] && [[ "$NEW_DB" != "NEEDS CLARIFICATION" ]]; then
        new_change_entry="- $CURRENT_BRANCH: Added $NEW_DB"
    fi
    
    # Check if sections exist in the file
    local has_active_technologies=0
    local has_recent_changes=0
    
    if grep -q "^## Active Technologies" "$target_file" 2>/dev/null; then
        has_active_technologies=1
    fi
    
    if grep -q "^## Recent Changes" "$target_file" 2>/dev/null; then
        has_recent_changes=1
    fi
    
    # Process file line by line
    local in_tech_section=false
    local in_changes_section=false
    local tech_entries_added=false
    local changes_entries_added=false
    local existing_changes_count=0
    local file_ended=false
    
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Handle Active Technologies section
        if [[ "$line" == "## Active Technologies" ]]; then
            echo "$line" >> "$temp_file"
            in_tech_section=true
            continue
        elif [[ $in_tech_section == true ]] && [[ "$line" =~ ^##[[:space:]] ]]; then
            # Add new tech entries before closing the section
            if [[ $tech_entries_added == false ]] && [[ ${#new_tech_entries[@]} -gt 0 ]]; then
                printf '%s\n' "${new_tech_entries[@]}" >> "$temp_file"
                tech_entries_added=true
            fi
            echo "$line" >> "$temp_file"
            in_tech_section=false
            continue
        elif [[ $in_tech_section == true ]] && [[ -z "$line" ]]; then
            # Add new tech entries before empty line in tech section
            if [[ $tech_entries_added == false ]] && [[ ${#new_tech_entries[@]} -gt 0 ]]; then
                printf '%s\n' "${new_tech_entries[@]}" >> "$temp_file"
                tech_entries_added=true
            fi
            echo "$line" >> "$temp_file"
            continue
        fi
        
        # Handle Recent Changes section
        if [[ "$line" == "## Recent Changes" ]]; then
            echo "$line" >> "$temp_file"
            # Add new change entry right after the heading
            if [[ -n "$new_change_entry" ]]; then
                echo "$new_change_entry" >> "$temp_file"
            fi
            in_changes_section=true
            changes_entries_added=true
            continue
        elif [[ $in_changes_section == true ]] && [[ "$line" =~ ^##[[:space:]] ]]; then
            echo "$line" >> "$temp_file"
            in_changes_section=false
            continue
        elif [[ $in_changes_section == true ]] && [[ "$line" == "- "* ]]; then
            # Keep only first 2 existing changes
            if [[ $existing_changes_count -lt 2 ]]; then
                echo "$line" >> "$temp_file"
                ((existing_changes_count++))
            fi
            continue
        fi
        
        # Update timestamp
        if [[ "$line" =~ \*\*Last\ updated\*\*:.*[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9] ]]; then
            echo "$line" | sed "s/[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]/$current_date/" >> "$temp_file"
        else
            echo "$line" >> "$temp_file"
        fi
    done < "$target_file"
    
    # Post-loop check: if we're still in the Active Technologies section and haven't added new entries
    if [[ $in_tech_section == true ]] && [[ $tech_entries_added == false ]] && [[ ${#new_tech_entries[@]} -gt 0 ]]; then
        printf '%s\n' "${new_tech_entries[@]}" >> "$temp_file"
        tech_entries_added=true
    fi
    
    # If sections don't exist, add them at the end of the file
    if [[ $has_active_technologies -eq 0 ]] && [[ ${#new_tech_entries[@]} -gt 0 ]]; then
        echo "" >> "$temp_file"
        echo "## Active Technologies" >> "$temp_file"
        printf '%s\n' "${new_tech_entries[@]}" >> "$temp_file"
        tech_entries_added=true
    fi
    
    if [[ $has_recent_changes -eq 0 ]] && [[ -n "$new_change_entry" ]]; then
        echo "" >> "$temp_file"
        echo "## Recent Changes" >> "$temp_file"
        echo "$new_change_entry" >> "$temp_file"
        changes_entries_added=true
    fi
    
    # Move temp file to target atomically
    if ! mv "$temp_file" "$target_file"; then
        log_error "Failed to update target file"
        rm -f "$temp_file"
        return 1
    fi
    
    return 0
}
#==============================================================================
# Main Agent File Update Function
#==============================================================================

update_agent_file() {
    local target_file="$1"
    local agent_name="$2"
    
    if [[ -z "$target_file" ]] || [[ -z "$agent_name" ]]; then
        log_error "update_agent_file requires target_file and agent_name parameters"
        return 1
    fi
    
    log_info "Updating $agent_name context file: $target_file"
    
    local project_name
    project_name=$(basename "$REPO_ROOT")
    local current_date
    current_date=$(date +%Y-%m-%d)
    
    # Create directory if it doesn't exist
    local target_dir
    target_dir=$(dirname "$target_file")
    if [[ ! -d "$target_dir" ]]; then
        if ! mkdir -p "$target_dir"; then
            log_error "Failed to create directory: $target_dir"
            return 1
        fi
    fi
    
    if [[ ! -f "$target_file" ]]; then
        # Create new file from template
        local temp_file
        temp_file=$(mktemp) || {
            log_error "Failed to create temporary file"
            return 1
        }
        
        if create_new_agent_file "$target_file" "$temp_file" "$project_name" "$current_date"; then
            if mv "$temp_file" "$target_file"; then
                log_success "Created new $agent_name context file"
            else
                log_error "Failed to move temporary file to $target_file"
                rm -f "$temp_file"
                return 1
            fi
        else
            log_error "Failed to create new agent file"
            rm -f "$temp_file"
            return 1
        fi
    else
        # Update existing file
        if [[ ! -r "$target_file" ]]; then
            log_error "Cannot read existing file: $target_file"
            return 1
        fi
        
        if [[ ! -w "$target_file" ]]; then
            log_error "Cannot write to existing file: $target_file"
            return 1
        fi
        
        if update_existing_agent_file "$target_file" "$current_date"; then
            log_success "Updated existing $agent_name context file"
        else
            log_error "Failed to update existing agent file"
            return 1
        fi
    fi
    
    return 0
}

#==============================================================================
# Agent Selection and Processing
#==============================================================================

update_specific_agent() {
    local agent_type="$1"
    
    case "$agent_type" in
        claude)
            update_agent_file "$CLAUDE_FILE" "Claude Code"
            ;;
        gemini)
            update_agent_file "$GEMINI_FILE" "Gemini CLI"
            ;;
        copilot)
            update_agent_file "$COPILOT_FILE" "GitHub Copilot"
            ;;
        cursor-agent)
            update_agent_file "$CURSOR_FILE" "Cursor IDE"
            ;;
        qwen)
            update_agent_file "$QWEN_FILE" "Qwen Code"
            ;;
        opencode)
            update_agent_file "$AGENTS_FILE" "opencode"
            ;;
        codex)
            update_agent_file "$AGENTS_FILE" "Codex CLI"
            ;;
        windsurf)
            update_agent_file "$WINDSURF_FILE" "Windsurf"
            ;;
        kilocode)
            update_agent_file "$KILOCODE_FILE" "Kilo Code"
            ;;
        auggie)
            update_agent_file "$AUGGIE_FILE" "Auggie CLI"
            ;;
        roo)
            update_agent_file "$ROO_FILE" "Roo Code"
            ;;
        codebuddy)
            update_agent_file "$CODEBUDDY_FILE" "CodeBuddy CLI"
            ;;
        amp)
            update_agent_file "$AMP_FILE" "Amp"
            ;;
        shai)
            update_agent_file "$SHAI_FILE" "SHAI"
            ;;
        q)
            update_agent_file "$Q_FILE" "Amazon Q Developer CLI"
            ;;
        *)
            log_error "Unknown agent type '$agent_type'"
            log_error "Expected: claude|gemini|copilot|cursor-agent|qwen|opencode|codex|windsurf|kilocode|auggie|roo|amp|shai|q"
            exit 1
            ;;
    esac
}

update_all_existing_agents() {
    local found_agent=false
    
    # Check each possible agent file and update if it exists
    if [[ -f "$CLAUDE_FILE" ]]; then
        update_agent_file "$CLAUDE_FILE" "Claude Code"
        found_agent=true
    fi
    
    if [[ -f "$GEMINI_FILE" ]]; then
        update_agent_file "$GEMINI_FILE" "Gemini CLI"
        found_agent=true
    fi
    
    if [[ -f "$COPILOT_FILE" ]]; then
        update_agent_file "$COPILOT_FILE" "GitHub Copilot"
        found_agent=true
    fi
    
    if [[ -f "$CURSOR_FILE" ]]; then
        update_agent_file "$CURSOR_FILE" "Cursor IDE"
        found_agent=true
    fi
    
    if [[ -f "$QWEN_FILE" ]]; then
        update_agent_file "$QWEN_FILE" "Qwen Code"
        found_agent=true
    fi
    
    if [[ -f "$AGENTS_FILE" ]]; then
        update_agent_file "$AGENTS_FILE" "Codex/opencode"
        found_agent=true
    fi
    
    if [[ -f "$WINDSURF_FILE" ]]; then
        update_agent_file "$WINDSURF_FILE" "Windsurf"
        found_agent=true
    fi
    
    if [[ -f "$KILOCODE_FILE" ]]; then
        update_agent_file "$KILOCODE_FILE" "Kilo Code"
        found_agent=true
    fi

    if [[ -f "$AUGGIE_FILE" ]]; then
        update_agent_file "$AUGGIE_FILE" "Auggie CLI"
        found_agent=true
    fi
    
    if [[ -f "$ROO_FILE" ]]; then
        update_agent_file "$ROO_FILE" "Roo Code"
        found_agent=true
    fi

    if [[ -f "$CODEBUDDY_FILE" ]]; then
        update_agent_file "$CODEBUDDY_FILE" "CodeBuddy CLI"
        found_agent=true
    fi

    if [[ -f "$SHAI_FILE" ]]; then
        update_agent_file "$SHAI_FILE" "SHAI"
        found_agent=true
    fi

    if [[ -f "$Q_FILE" ]]; then
        update_agent_file "$Q_FILE" "Amazon Q Developer CLI"
        found_agent=true
    fi
    
    # If no agent files exist, create a default Claude file
    if [[ "$found_agent" == false ]]; then
        log_info "No existing agent files found, creating default Claude file..."
        update_agent_file "$CLAUDE_FILE" "Claude Code"
    fi
}
print_summary() {
    echo
    log_info "Summary of changes:"
    
    if [[ -n "$NEW_LANG" ]]; then
        echo "  - Added language: $NEW_LANG"
    fi
    
    if [[ -n "$NEW_FRAMEWORK" ]]; then
        echo "  - Added framework: $NEW_FRAMEWORK"
    fi
    
    if [[ -n "$NEW_DB" ]] && [[ "$NEW_DB" != "N/A" ]]; then
        echo "  - Added database: $NEW_DB"
    fi
    
    echo

    log_info "Usage: $0 [claude|gemini|copilot|cursor-agent|qwen|opencode|codex|windsurf|kilocode|auggie|codebuddy|shai|q]"
}

#==============================================================================
# Main Execution
#==============================================================================

main() {
    # Validate environment before proceeding
    validate_environment
    
    log_info "=== Updating agent context files for feature $CURRENT_BRANCH ==="
    
    # Parse the plan file to extract project information
    if ! parse_plan_data "$NEW_PLAN"; then
        log_error "Failed to parse plan data"
        exit 1
    fi
    
    # Process based on agent type argument
    local success=true
    
    if [[ -z "$AGENT_TYPE" ]]; then
        # No specific agent provided - update all existing agent files
        log_info "No agent specified, updating all existing agent files..."
        if ! update_all_existing_agents; then
            success=false
        fi
    else
        # Specific agent provided - update only that agent
        log_info "Updating specific agent: $AGENT_TYPE"
        if ! update_specific_agent "$AGENT_TYPE"; then
            success=false
        fi
    fi
    
    # Print summary
    print_summary
    
    if [[ "$success" == true ]]; then
        log_success "Agent context update completed successfully"
        exit 0
    else
        log_error "Agent context update completed with errors"
        exit 1
    fi
}

# Execute main function if script is run directly
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
````

## File: .specify/templates/agent-file-template.md
````markdown
# [PROJECT NAME] Development Guidelines

Auto-generated from all feature plans. Last updated: [DATE]

## Active Technologies

[EXTRACTED FROM ALL PLAN.MD FILES]

## Project Structure

```text
[ACTUAL STRUCTURE FROM PLANS]
```

## Commands

[ONLY COMMANDS FOR ACTIVE TECHNOLOGIES]

## Code Style

[LANGUAGE-SPECIFIC, ONLY FOR LANGUAGES IN USE]

## Recent Changes

[LAST 3 FEATURES AND WHAT THEY ADDED]

<!-- MANUAL ADDITIONS START -->
<!-- MANUAL ADDITIONS END -->
````

## File: .specify/templates/checklist-template.md
````markdown
# [CHECKLIST TYPE] Checklist: [FEATURE NAME]

**Purpose**: [Brief description of what this checklist covers]
**Created**: [DATE]
**Feature**: [Link to spec.md or relevant documentation]

**Note**: This checklist is generated by the `/speckit.checklist` command based on feature context and requirements.

<!-- 
  ============================================================================
  IMPORTANT: The checklist items below are SAMPLE ITEMS for illustration only.
  
  The /speckit.checklist command MUST replace these with actual items based on:
  - User's specific checklist request
  - Feature requirements from spec.md
  - Technical context from plan.md
  - Implementation details from tasks.md
  
  DO NOT keep these sample items in the generated checklist file.
  ============================================================================
-->

## [Category 1]

- [ ] CHK001 First checklist item with clear action
- [ ] CHK002 Second checklist item
- [ ] CHK003 Third checklist item

## [Category 2]

- [ ] CHK004 Another category item
- [ ] CHK005 Item with specific criteria
- [ ] CHK006 Final item in this category

## Notes

- Check items off as completed: `[x]`
- Add comments or findings inline
- Link to relevant resources or documentation
- Items are numbered sequentially for easy reference
````

## File: .specify/templates/plan-template.md
````markdown
# Implementation Plan: [FEATURE]

**Branch**: `[###-feature-name]` | **Date**: [DATE] | **Spec**: [link]
**Input**: Feature specification from `/specs/[###-feature-name]/spec.md`

**Note**: This template is filled in by the `/speckit.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.

## Summary

[Extract from feature spec: primary requirement + technical approach from research]

## Technical Context

<!--
  ACTION REQUIRED: Replace the content in this section with the technical details
  for the project. The structure here is presented in advisory capacity to guide
  the iteration process.
-->

**Language/Version**: [e.g., Python 3.11, Swift 5.9, Rust 1.75 or NEEDS CLARIFICATION]  
**Primary Dependencies**: [e.g., FastAPI, UIKit, LLVM or NEEDS CLARIFICATION]  
**Storage**: [if applicable, e.g., PostgreSQL, CoreData, files or N/A]  
**Testing**: [e.g., pytest, XCTest, cargo test or NEEDS CLARIFICATION]  
**Target Platform**: [e.g., Linux server, iOS 15+, WASM or NEEDS CLARIFICATION]
**Project Type**: [single/web/mobile - determines source structure]  
**Performance Goals**: [domain-specific, e.g., 1000 req/s, 10k lines/sec, 60 fps or NEEDS CLARIFICATION]  
**Constraints**: [domain-specific, e.g., <200ms p95, <100MB memory, offline-capable or NEEDS CLARIFICATION]  
**Scale/Scope**: [domain-specific, e.g., 10k users, 1M LOC, 50 screens or NEEDS CLARIFICATION]

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

[Gates determined based on constitution file]

## Project Structure

### Documentation (this feature)

```text
specs/[###-feature]/
â”œâ”€â”€ plan.md              # This file (/speckit.plan command output)
â”œâ”€â”€ research.md          # Phase 0 output (/speckit.plan command)
â”œâ”€â”€ data-model.md        # Phase 1 output (/speckit.plan command)
â”œâ”€â”€ quickstart.md        # Phase 1 output (/speckit.plan command)
â”œâ”€â”€ contracts/           # Phase 1 output (/speckit.plan command)
â””â”€â”€ tasks.md             # Phase 2 output (/speckit.tasks command - NOT created by /speckit.plan)
```

### Source Code (repository root)
<!--
  ACTION REQUIRED: Replace the placeholder tree below with the concrete layout
  for this feature. Delete unused options and expand the chosen structure with
  real paths (e.g., apps/admin, packages/something). The delivered plan must
  not include Option labels.
-->

```text
# [REMOVE IF UNUSED] Option 1: Single project (DEFAULT)
src/
â”œâ”€â”€ models/
â”œâ”€â”€ services/
â”œâ”€â”€ cli/
â””â”€â”€ lib/

tests/
â”œâ”€â”€ contract/
â”œâ”€â”€ integration/
â””â”€â”€ unit/

# [REMOVE IF UNUSED] Option 2: Web application (when "frontend" + "backend" detected)
backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ services/
â”‚   â””â”€â”€ api/
â””â”€â”€ tests/

frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ pages/
â”‚   â””â”€â”€ services/
â””â”€â”€ tests/

# [REMOVE IF UNUSED] Option 3: Mobile + API (when "iOS/Android" detected)
api/
â””â”€â”€ [same as backend above]

ios/ or android/
â””â”€â”€ [platform-specific structure: feature modules, UI flows, platform tests]
```

**Structure Decision**: [Document the selected structure and reference the real
directories captured above]

## Complexity Tracking

> **Fill ONLY if Constitution Check has violations that must be justified**

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| [e.g., 4th project] | [current need] | [why 3 projects insufficient] |
| [e.g., Repository pattern] | [specific problem] | [why direct DB access insufficient] |
````

## File: .specify/templates/spec-template.md
````markdown
# Feature Specification: [FEATURE NAME]

**Feature Branch**: `[###-feature-name]`  
**Created**: [DATE]  
**Status**: Draft  
**Input**: User description: "$ARGUMENTS"

## User Scenarios & Testing *(mandatory)*

<!--
  IMPORTANT: User stories should be PRIORITIZED as user journeys ordered by importance.
  Each user story/journey must be INDEPENDENTLY TESTABLE - meaning if you implement just ONE of them,
  you should still have a viable MVP (Minimum Viable Product) that delivers value.
  
  Assign priorities (P1, P2, P3, etc.) to each story, where P1 is the most critical.
  Think of each story as a standalone slice of functionality that can be:
  - Developed independently
  - Tested independently
  - Deployed independently
  - Demonstrated to users independently
-->

### User Story 1 - [Brief Title] (Priority: P1)

[Describe this user journey in plain language]

**Why this priority**: [Explain the value and why it has this priority level]

**Independent Test**: [Describe how this can be tested independently - e.g., "Can be fully tested by [specific action] and delivers [specific value]"]

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]
2. **Given** [initial state], **When** [action], **Then** [expected outcome]

---

### User Story 2 - [Brief Title] (Priority: P2)

[Describe this user journey in plain language]

**Why this priority**: [Explain the value and why it has this priority level]

**Independent Test**: [Describe how this can be tested independently]

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]

---

### User Story 3 - [Brief Title] (Priority: P3)

[Describe this user journey in plain language]

**Why this priority**: [Explain the value and why it has this priority level]

**Independent Test**: [Describe how this can be tested independently]

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]

---

[Add more user stories as needed, each with an assigned priority]

### Edge Cases

<!--
  ACTION REQUIRED: The content in this section represents placeholders.
  Fill them out with the right edge cases.
-->

- What happens when [boundary condition]?
- How does system handle [error scenario]?

## Requirements *(mandatory)*

<!--
  ACTION REQUIRED: The content in this section represents placeholders.
  Fill them out with the right functional requirements.
-->

### Functional Requirements

- **FR-001**: System MUST [specific capability, e.g., "allow users to create accounts"]
- **FR-002**: System MUST [specific capability, e.g., "validate email addresses"]  
- **FR-003**: Users MUST be able to [key interaction, e.g., "reset their password"]
- **FR-004**: System MUST [data requirement, e.g., "persist user preferences"]
- **FR-005**: System MUST [behavior, e.g., "log all security events"]

*Example of marking unclear requirements:*

- **FR-006**: System MUST authenticate users via [NEEDS CLARIFICATION: auth method not specified - email/password, SSO, OAuth?]
- **FR-007**: System MUST retain user data for [NEEDS CLARIFICATION: retention period not specified]

### Key Entities *(include if feature involves data)*

- **[Entity 1]**: [What it represents, key attributes without implementation]
- **[Entity 2]**: [What it represents, relationships to other entities]

## Success Criteria *(mandatory)*

<!--
  ACTION REQUIRED: Define measurable success criteria.
  These must be technology-agnostic and measurable.
-->

### Measurable Outcomes

- **SC-001**: [Measurable metric, e.g., "Users can complete account creation in under 2 minutes"]
- **SC-002**: [Measurable metric, e.g., "System handles 1000 concurrent users without degradation"]
- **SC-003**: [User satisfaction metric, e.g., "90% of users successfully complete primary task on first attempt"]
- **SC-004**: [Business metric, e.g., "Reduce support tickets related to [X] by 50%"]
````

## File: .specify/templates/tasks-template.md
````markdown
---

description: "Task list template for feature implementation"
---

# Tasks: [FEATURE NAME]

**Input**: Design documents from `/specs/[###-feature-name]/`
**Prerequisites**: plan.md (required), spec.md (required for user stories), research.md, data-model.md, contracts/

**Tests**: The examples below include test tasks. Tests are OPTIONAL - only include them if explicitly requested in the feature specification.

**Organization**: Tasks are grouped by user story to enable independent implementation and testing of each story.

## Format: `[ID] [P?] [Story] Description`

- **[P]**: Can run in parallel (different files, no dependencies)
- **[Story]**: Which user story this task belongs to (e.g., US1, US2, US3)
- Include exact file paths in descriptions

## Path Conventions

- **Single project**: `src/`, `tests/` at repository root
- **Web app**: `backend/src/`, `frontend/src/`
- **Mobile**: `api/src/`, `ios/src/` or `android/src/`
- Paths shown below assume single project - adjust based on plan.md structure

<!-- 
  ============================================================================
  IMPORTANT: The tasks below are SAMPLE TASKS for illustration purposes only.
  
  The /speckit.tasks command MUST replace these with actual tasks based on:
  - User stories from spec.md (with their priorities P1, P2, P3...)
  - Feature requirements from plan.md
  - Entities from data-model.md
  - Endpoints from contracts/
  
  Tasks MUST be organized by user story so each story can be:
  - Implemented independently
  - Tested independently
  - Delivered as an MVP increment
  
  DO NOT keep these sample tasks in the generated tasks.md file.
  ============================================================================
-->

## Phase 1: Setup (Shared Infrastructure)

**Purpose**: Project initialization and basic structure

- [ ] T001 Create project structure per implementation plan
- [ ] T002 Initialize [language] project with [framework] dependencies
- [ ] T003 [P] Configure linting and formatting tools

---

## Phase 2: Foundational (Blocking Prerequisites)

**Purpose**: Core infrastructure that MUST be complete before ANY user story can be implemented

**âš ï¸ CRITICAL**: No user story work can begin until this phase is complete

Examples of foundational tasks (adjust based on your project):

- [ ] T004 Setup database schema and migrations framework
- [ ] T005 [P] Implement authentication/authorization framework
- [ ] T006 [P] Setup API routing and middleware structure
- [ ] T007 Create base models/entities that all stories depend on
- [ ] T008 Configure error handling and logging infrastructure
- [ ] T009 Setup environment configuration management

**Checkpoint**: Foundation ready - user story implementation can now begin in parallel

---

## Phase 3: User Story 1 - [Title] (Priority: P1) ðŸŽ¯ MVP

**Goal**: [Brief description of what this story delivers]

**Independent Test**: [How to verify this story works on its own]

### Tests for User Story 1 (OPTIONAL - only if tests requested) âš ï¸

> **NOTE: Write these tests FIRST, ensure they FAIL before implementation**

- [ ] T010 [P] [US1] Contract test for [endpoint] in tests/contract/test_[name].py
- [ ] T011 [P] [US1] Integration test for [user journey] in tests/integration/test_[name].py

### Implementation for User Story 1

- [ ] T012 [P] [US1] Create [Entity1] model in src/models/[entity1].py
- [ ] T013 [P] [US1] Create [Entity2] model in src/models/[entity2].py
- [ ] T014 [US1] Implement [Service] in src/services/[service].py (depends on T012, T013)
- [ ] T015 [US1] Implement [endpoint/feature] in src/[location]/[file].py
- [ ] T016 [US1] Add validation and error handling
- [ ] T017 [US1] Add logging for user story 1 operations

**Checkpoint**: At this point, User Story 1 should be fully functional and testable independently

---

## Phase 4: User Story 2 - [Title] (Priority: P2)

**Goal**: [Brief description of what this story delivers]

**Independent Test**: [How to verify this story works on its own]

### Tests for User Story 2 (OPTIONAL - only if tests requested) âš ï¸

- [ ] T018 [P] [US2] Contract test for [endpoint] in tests/contract/test_[name].py
- [ ] T019 [P] [US2] Integration test for [user journey] in tests/integration/test_[name].py

### Implementation for User Story 2

- [ ] T020 [P] [US2] Create [Entity] model in src/models/[entity].py
- [ ] T021 [US2] Implement [Service] in src/services/[service].py
- [ ] T022 [US2] Implement [endpoint/feature] in src/[location]/[file].py
- [ ] T023 [US2] Integrate with User Story 1 components (if needed)

**Checkpoint**: At this point, User Stories 1 AND 2 should both work independently

---

## Phase 5: User Story 3 - [Title] (Priority: P3)

**Goal**: [Brief description of what this story delivers]

**Independent Test**: [How to verify this story works on its own]

### Tests for User Story 3 (OPTIONAL - only if tests requested) âš ï¸

- [ ] T024 [P] [US3] Contract test for [endpoint] in tests/contract/test_[name].py
- [ ] T025 [P] [US3] Integration test for [user journey] in tests/integration/test_[name].py

### Implementation for User Story 3

- [ ] T026 [P] [US3] Create [Entity] model in src/models/[entity].py
- [ ] T027 [US3] Implement [Service] in src/services/[service].py
- [ ] T028 [US3] Implement [endpoint/feature] in src/[location]/[file].py

**Checkpoint**: All user stories should now be independently functional

---

[Add more user story phases as needed, following the same pattern]

---

## Phase N: Polish & Cross-Cutting Concerns

**Purpose**: Improvements that affect multiple user stories

- [ ] TXXX [P] Documentation updates in docs/
- [ ] TXXX Code cleanup and refactoring
- [ ] TXXX Performance optimization across all stories
- [ ] TXXX [P] Additional unit tests (if requested) in tests/unit/
- [ ] TXXX Security hardening
- [ ] TXXX Run quickstart.md validation

---

## Dependencies & Execution Order

### Phase Dependencies

- **Setup (Phase 1)**: No dependencies - can start immediately
- **Foundational (Phase 2)**: Depends on Setup completion - BLOCKS all user stories
- **User Stories (Phase 3+)**: All depend on Foundational phase completion
  - User stories can then proceed in parallel (if staffed)
  - Or sequentially in priority order (P1 â†’ P2 â†’ P3)
- **Polish (Final Phase)**: Depends on all desired user stories being complete

### User Story Dependencies

- **User Story 1 (P1)**: Can start after Foundational (Phase 2) - No dependencies on other stories
- **User Story 2 (P2)**: Can start after Foundational (Phase 2) - May integrate with US1 but should be independently testable
- **User Story 3 (P3)**: Can start after Foundational (Phase 2) - May integrate with US1/US2 but should be independently testable

### Within Each User Story

- Tests (if included) MUST be written and FAIL before implementation
- Models before services
- Services before endpoints
- Core implementation before integration
- Story complete before moving to next priority

### Parallel Opportunities

- All Setup tasks marked [P] can run in parallel
- All Foundational tasks marked [P] can run in parallel (within Phase 2)
- Once Foundational phase completes, all user stories can start in parallel (if team capacity allows)
- All tests for a user story marked [P] can run in parallel
- Models within a story marked [P] can run in parallel
- Different user stories can be worked on in parallel by different team members

---

## Parallel Example: User Story 1

```bash
# Launch all tests for User Story 1 together (if tests requested):
Task: "Contract test for [endpoint] in tests/contract/test_[name].py"
Task: "Integration test for [user journey] in tests/integration/test_[name].py"

# Launch all models for User Story 1 together:
Task: "Create [Entity1] model in src/models/[entity1].py"
Task: "Create [Entity2] model in src/models/[entity2].py"
```

---

## Implementation Strategy

### MVP First (User Story 1 Only)

1. Complete Phase 1: Setup
2. Complete Phase 2: Foundational (CRITICAL - blocks all stories)
3. Complete Phase 3: User Story 1
4. **STOP and VALIDATE**: Test User Story 1 independently
5. Deploy/demo if ready

### Incremental Delivery

1. Complete Setup + Foundational â†’ Foundation ready
2. Add User Story 1 â†’ Test independently â†’ Deploy/Demo (MVP!)
3. Add User Story 2 â†’ Test independently â†’ Deploy/Demo
4. Add User Story 3 â†’ Test independently â†’ Deploy/Demo
5. Each story adds value without breaking previous stories

### Parallel Team Strategy

With multiple developers:

1. Team completes Setup + Foundational together
2. Once Foundational is done:
   - Developer A: User Story 1
   - Developer B: User Story 2
   - Developer C: User Story 3
3. Stories complete and integrate independently

---

## Notes

- [P] tasks = different files, no dependencies
- [Story] label maps task to specific user story for traceability
- Each user story should be independently completable and testable
- Verify tests fail before implementing
- Commit after each task or logical group
- Stop at any checkpoint to validate story independently
- Avoid: vague tasks, same file conflicts, cross-story dependencies that break independence
````

## File: examples/self-assessment/assessment-20251121.json
````json
{
  "repository": {
    "path": "/Users/jeder/repos/sk/agentready",
    "name": "agentready",
    "url": null,
    "branch": "001-agentready-scorer",
    "commit_hash": "adfc4c823a7b25d5b1404c9b994461e290257dce",
    "languages": {
      "Markdown": 36,
      "Shell": 5,
      "Python": 31
    },
    "total_files": 79,
    "total_lines": 13852
  },
  "timestamp": "2025-11-21T02:11:05.766923",
  "overall_score": 75.4,
  "certification_level": "Gold",
  "attributes_assessed": 10,
  "attributes_skipped": 15,
  "attributes_total": 25,
  "findings": [
    {
      "attribute": {
        "id": "claude_md_file",
        "name": "CLAUDE.md Configuration Files",
        "category": "Context Window Optimization",
        "tier": 1,
        "description": "Project-specific configuration for Claude Code",
        "criteria": "CLAUDE.md file exists in repository root",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "present",
      "threshold": "present",
      "evidence": [
        "CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "readme_structure",
        "name": "README Structure",
        "category": "Documentation Standards",
        "tier": 1,
        "description": "Well-structured README with key sections",
        "criteria": "README.md with installation, usage, and development sections",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "3/3 sections",
      "threshold": "3/3 sections",
      "evidence": [
        "Found 3/3 essential sections",
        "Installation: \u2713",
        "Usage: \u2713",
        "Development: \u2713"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "type_annotations",
        "name": "Type Annotations",
        "category": "Code Quality",
        "tier": 1,
        "description": "Type hints in function signatures",
        "criteria": ">80% of functions have type annotations",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "95.9%",
      "threshold": "\u226580%",
      "evidence": [
        "Typed functions: 142/148",
        "Coverage: 95.9%"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "standard_layout",
        "name": "Standard Project Layouts",
        "category": "Repository Structure",
        "tier": 1,
        "description": "Follows standard project structure for language",
        "criteria": "Standard directories (src/, tests/, docs/) present",
        "default_weight": 0.1
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "2/2 directories",
      "threshold": "2/2 directories",
      "evidence": [
        "Found 2/2 standard directories",
        "src/: \u2713",
        "tests/: \u2713"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "lock_files",
        "name": "Lock Files for Reproducibility",
        "category": "Dependency Management",
        "tier": 1,
        "description": "Lock files present for dependency pinning",
        "criteria": "package-lock.json, yarn.lock, poetry.lock, or requirements.txt with versions",
        "default_weight": 0.1
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "none",
      "threshold": "at least one lock file",
      "evidence": [
        "No lock files found"
      ],
      "remediation": {
        "summary": "Add lock file for dependency reproducibility",
        "steps": [
          "Use npm install, poetry lock, or equivalent to generate lock file"
        ],
        "tools": [],
        "commands": [
          "npm install  # generates package-lock.json"
        ],
        "examples": [],
        "citations": []
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "test_coverage",
        "name": "Test Coverage Requirements",
        "category": "Testing & CI/CD",
        "tier": 2,
        "description": "Test coverage thresholds configured and enforced",
        "criteria": ">80% code coverage",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "configured",
      "threshold": "configured with >80% threshold",
      "evidence": [
        "Coverage configuration found",
        "pytest-cov dependency present"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "precommit_hooks",
        "name": "Pre-commit Hooks & CI/CD Linting",
        "category": "Testing & CI/CD",
        "tier": 2,
        "description": "Pre-commit hooks configured for linting and formatting",
        "criteria": ".pre-commit-config.yaml exists",
        "default_weight": 0.03
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "not configured",
      "threshold": "configured",
      "evidence": [
        ".pre-commit-config.yaml not found"
      ],
      "remediation": {
        "summary": "Configure pre-commit hooks for automated code quality checks",
        "steps": [
          "Install pre-commit framework",
          "Create .pre-commit-config.yaml",
          "Add hooks for linting and formatting",
          "Install hooks: pre-commit install",
          "Run on all files: pre-commit run --all-files"
        ],
        "tools": [
          "pre-commit"
        ],
        "commands": [
          "pip install pre-commit",
          "pre-commit install",
          "pre-commit run --all-files"
        ],
        "examples": [
          "# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n\n  - repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n      - id: black\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n"
        ],
        "citations": [
          {
            "source": "pre-commit.com",
            "title": "Pre-commit Framework",
            "url": "https://pre-commit.com/",
            "relevance": "Official pre-commit documentation"
          }
        ]
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "conventional_commits",
        "name": "Conventional Commit Messages",
        "category": "Git & Version Control",
        "tier": 2,
        "description": "Follows conventional commit format",
        "criteria": "\u226580% of recent commits follow convention",
        "default_weight": 0.03
      },
      "status": "fail",
      "score": 0.0,
      "measured_value": "not configured",
      "threshold": "configured",
      "evidence": [
        "No commitlint or husky configuration"
      ],
      "remediation": {
        "summary": "Configure conventional commits with commitlint",
        "steps": [
          "Install commitlint",
          "Configure husky for commit-msg hook"
        ],
        "tools": [
          "commitlint",
          "husky"
        ],
        "commands": [
          "npm install --save-dev @commitlint/cli @commitlint/config-conventional husky"
        ],
        "examples": [],
        "citations": []
      },
      "error_message": null
    },
    {
      "attribute": {
        "id": "gitignore_completeness",
        "name": ".gitignore Completeness",
        "category": "Git & Version Control",
        "tier": 2,
        "description": "Comprehensive .gitignore file",
        "criteria": ".gitignore exists and covers common patterns",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "580 bytes",
      "threshold": ">50 bytes",
      "evidence": [
        ".gitignore found (580 bytes)"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "cyclomatic_complexity",
        "name": "Cyclomatic Complexity Thresholds",
        "category": "Code Quality",
        "tier": 3,
        "description": "Cyclomatic complexity thresholds enforced",
        "criteria": "Average complexity <10, no functions >15",
        "default_weight": 0.03
      },
      "status": "pass",
      "score": 100.0,
      "measured_value": "3.1",
      "threshold": "<10.0",
      "evidence": [
        "Average cyclomatic complexity: 3.1"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "one_command_setup",
        "name": "One-Command Build/Setup",
        "category": "Build & Development",
        "tier": 2,
        "description": "Assessment for One-Command Build/Setup",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "One-Command Build/Setup assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "concise_documentation",
        "name": "Concise Structured Documentation",
        "category": "Context Window Optimization",
        "tier": 2,
        "description": "Assessment for Concise Structured Documentation",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Concise Structured Documentation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "inline_documentation",
        "name": "Inline Documentation",
        "category": "Documentation Standards",
        "tier": 2,
        "description": "Assessment for Inline Documentation",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Inline Documentation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "file_size_limits",
        "name": "File Size Limits",
        "category": "Context Window Optimization",
        "tier": 2,
        "description": "Assessment for File Size Limits",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "File Size Limits assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "dependency_freshness",
        "name": "Dependency Freshness & Security",
        "category": "Dependency Management",
        "tier": 2,
        "description": "Assessment for Dependency Freshness & Security",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Dependency Freshness & Security assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "separation_concerns",
        "name": "Separation of Concerns",
        "category": "Repository Structure",
        "tier": 2,
        "description": "Assessment for Separation of Concerns",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Separation of Concerns assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "structured_logging",
        "name": "Structured Logging",
        "category": "Error Handling",
        "tier": 3,
        "description": "Assessment for Structured Logging",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Structured Logging assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "openapi_specs",
        "name": "OpenAPI/Swagger Specifications",
        "category": "API Documentation",
        "tier": 3,
        "description": "Assessment for OpenAPI/Swagger Specifications",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "OpenAPI/Swagger Specifications assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "architecture_decisions",
        "name": "Architecture Decision Records",
        "category": "Documentation Standards",
        "tier": 3,
        "description": "Assessment for Architecture Decision Records",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Architecture Decision Records assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "semantic_naming",
        "name": "Semantic File & Directory Naming",
        "category": "Modularity",
        "tier": 3,
        "description": "Assessment for Semantic File & Directory Naming",
        "criteria": "To be implemented",
        "default_weight": 0.03
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Semantic File & Directory Naming assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "security_scanning",
        "name": "Security Scanning Automation",
        "category": "Security",
        "tier": 4,
        "description": "Assessment for Security Scanning Automation",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Security Scanning Automation assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "performance_benchmarks",
        "name": "Performance Benchmarks",
        "category": "Performance",
        "tier": 4,
        "description": "Assessment for Performance Benchmarks",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Performance Benchmarks assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "code_smells",
        "name": "Code Smell Elimination",
        "category": "Code Quality",
        "tier": 4,
        "description": "Assessment for Code Smell Elimination",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Code Smell Elimination assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "issue_pr_templates",
        "name": "Issue & Pull Request Templates",
        "category": "Git & Version Control",
        "tier": 4,
        "description": "Assessment for Issue & Pull Request Templates",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Issue & Pull Request Templates assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    },
    {
      "attribute": {
        "id": "container_setup",
        "name": "Container/Virtualization Setup",
        "category": "Build & Development",
        "tier": 4,
        "description": "Assessment for Container/Virtualization Setup",
        "criteria": "To be implemented",
        "default_weight": 0.01
      },
      "status": "not_applicable",
      "score": null,
      "measured_value": null,
      "threshold": null,
      "evidence": [
        "Container/Virtualization Setup assessment not yet implemented"
      ],
      "remediation": null,
      "error_message": null
    }
  ],
  "config": null,
  "duration_seconds": 0.2
}
````

## File: examples/self-assessment/README.md
````markdown
# AgentReady Self-Assessment

This directory contains the AgentReady tool's own assessment reports, demonstrating the output formats and serving as a reference implementation.

## Assessment Results

**Date**: 2025-11-21
**Score**: 75.4/100
**Certification Level**: ðŸ¥‡ Gold

### Summary

- **Attributes Assessed**: 10/25
- **Attributes Skipped**: 15 (not yet implemented)
- **Duration**: 0.2 seconds

### Passing Attributes (7)

1. âœ… **CLAUDE.md File** (Tier 1, 10%) - Present and comprehensive
2. âœ… **README Structure** (Tier 1, 10%) - Well-structured documentation
3. âœ… **Type Annotations** (Tier 1, 10%) - Python type hints coverage
4. âœ… **Standard Layout** (Tier 2, 3%) - Follows src/ layout convention
5. âœ… **Test Coverage** (Tier 2, 3%) - Pytest suite with unit and integration tests
6. âœ… **Gitignore Completeness** (Tier 2, 3%) - Comprehensive .gitignore
7. âœ… **Cyclomatic Complexity** (Tier 3, 1.5%) - Low complexity in codebase

### Failing Attributes (3)

1. âŒ **Lock Files** (Tier 2, 3%) - No lock file present (intentional for library)
2. âŒ **Pre-commit Hooks** (Tier 2, 3%) - No .pre-commit-config.yaml
3. âŒ **Conventional Commits** (Tier 3, 1.5%) - Not enforced via tooling

### Not Applicable (15)

These attributes are stub implementations returning "not_applicable" - they will be implemented in future iterations.

## Report Formats

This directory contains three report formats generated by AgentReady:

### 1. JSON Report (`assessment-20251121.json`)

Machine-readable format for:
- CI/CD integration
- Programmatic processing
- Historical tracking
- Trend analysis

**Use Case**: Parse assessment data in automation scripts or dashboards.

### 2. HTML Report (`report-20251121.html`)

Interactive web report with:
- ðŸŽ¨ Color-coded certification levels
- ðŸ” Search and filter functionality
- ðŸ“Š Visual score indicators
- ðŸ“‹ Collapsible finding details
- ðŸŒ Works offline (no CDN dependencies)

**Use Case**: Share with stakeholders or view in browser for detailed analysis.

**To view**: Open `report-20251121.html` in any web browser.

### 3. Markdown Report (`report-20251121.md`)

Version-control friendly format with:
- ðŸ“ GitHub-Flavored Markdown
- âœ… Status indicators (âœ…âŒâŠ˜)
- ðŸ“Š ASCII tables
- ðŸŽ–ï¸ Certification ladder
- ðŸ”§ Prioritized next steps

**Use Case**: Commit to git for tracking improvements over time.

**To view**: View on GitHub or any Markdown renderer.

## How to Run Your Own Assessment

```bash
# Install AgentReady
pip install agentready

# Run assessment on current directory
agentready assess .

# Run with verbose output
agentready assess . --verbose

# Specify output directory
agentready assess /path/to/repo --output-dir ./reports
```

## Next Steps for AgentReady Repository

Based on this assessment, the highest-priority improvements are:

1. **Add Pre-commit Hooks** (+3 points) - Create `.pre-commit-config.yaml` with formatters and linters
2. **Enforce Conventional Commits** (+1.5 points) - Add commitlint to pre-commit hooks

These two changes would bring the score from **75.4** to **79.9** (still Gold, but closer to Platinum threshold of 90).

## About This Assessment

This self-assessment demonstrates:

- âœ… The tool successfully assesses itself
- âœ… All three report formats are generated
- âœ… Assessment completes in under 1 second
- âœ… Provides actionable remediation guidance
- âœ… Accurately reflects repository state

The fact that AgentReady achieves **Gold certification** when assessing itself validates that the scoring algorithm and attribute definitions are reasonable and achievable for real-world repositories.
````

## File: examples/self-assessment/report-20251121.html
````html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AgentReady Assessment - agentready</title>
    <style>
        /* Reset and Base Styles */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        /* Header */
        header {
            border-bottom: 3px solid #2563eb;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }

        h1 {
            font-size: 2rem;
            color: #1e293b;
            margin-bottom: 10px;
        }

        .subtitle {
            color: #64748b;
            font-size: 1rem;
        }

        /* Summary Cards */
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .summary-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
        }

        .summary-card.score {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        }

        .summary-card.certification {
            background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        }

        .summary-card.assessed {
            background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
        }

        .summary-card.duration {
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
        }

        .summary-card h3 {
            font-size: 0.9rem;
            font-weight: 500;
            opacity: 0.9;
            margin-bottom: 10px;
        }

        .summary-card .value {
            font-size: 2rem;
            font-weight: 700;
        }

        /* Controls */
        .controls {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin: 30px 0;
            padding: 20px;
            background: #f8fafc;
            border-radius: 8px;
        }

        .control-group {
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .control-group label {
            font-weight: 600;
            color: #475569;
            font-size: 0.9rem;
        }

        select, input[type="search"] {
            padding: 8px 12px;
            border: 1px solid #cbd5e1;
            border-radius: 6px;
            font-size: 0.9rem;
            background: white;
        }

        input[type="search"] {
            min-width: 250px;
        }

        .filter-btn {
            padding: 8px 16px;
            border: 2px solid #e2e8f0;
            background: white;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: all 0.2s;
        }

        .filter-btn:hover {
            border-color: #2563eb;
            background: #eff6ff;
        }

        .filter-btn.active {
            background: #2563eb;
            color: white;
            border-color: #2563eb;
        }

        .badge {
            display: inline-block;
            padding: 2px 8px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
            margin-left: 5px;
        }

        /* Certification Ladder */
        .cert-ladder {
            display: flex;
            justify-content: space-between;
            margin: 30px 0;
            padding: 20px;
            background: #f8fafc;
            border-radius: 8px;
        }

        .cert-level {
            flex: 1;
            text-align: center;
            padding: 15px 10px;
            border-radius: 6px;
            margin: 0 5px;
            transition: all 0.3s;
        }

        .cert-level.platinum { background: linear-gradient(135deg, #e0e7ff, #c7d2fe); }
        .cert-level.gold { background: linear-gradient(135deg, #fef3c7, #fde68a); }
        .cert-level.silver { background: linear-gradient(135deg, #e5e7eb, #d1d5db); }
        .cert-level.bronze { background: linear-gradient(135deg, #fed7aa, #fdba74); }
        .cert-level.needs-improvement { background: linear-gradient(135deg, #fecaca, #fca5a5); }

        .cert-level.active {
            transform: scale(1.1);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            border: 3px solid #2563eb;
        }

        .cert-level h4 {
            font-size: 0.9rem;
            margin-bottom: 5px;
        }

        .cert-level .range {
            font-size: 0.8rem;
            opacity: 0.7;
        }

        /* Findings */
        .findings {
            margin: 30px 0;
        }

        .finding {
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            margin-bottom: 15px;
            overflow: hidden;
            transition: all 0.2s;
        }

        .finding:hover {
            box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        }

        .finding-header {
            padding: 15px 20px;
            background: #f8fafc;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .finding-header:hover {
            background: #f1f5f9;
        }

        .finding-title {
            display: flex;
            align-items: center;
            gap: 15px;
            flex: 1;
        }

        .status-icon {
            font-size: 1.5rem;
        }

        .finding-info h3 {
            font-size: 1rem;
            color: #1e293b;
            margin-bottom: 3px;
        }

        .finding-meta {
            font-size: 0.85rem;
            color: #64748b;
        }

        .tier-badge {
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
        }

        .tier-1 { background: #fee2e2; color: #991b1b; }
        .tier-2 { background: #fed7aa; color: #9a3412; }
        .tier-3 { background: #fef3c7; color: #92400e; }
        .tier-4 { background: #d1fae5; color: #065f46; }

        .score-display {
            font-size: 1.5rem;
            font-weight: 700;
            min-width: 80px;
            text-align: right;
        }

        .score-pass { color: #059669; }
        .score-fail { color: #dc2626; }
        .score-skip { color: #6b7280; }

        .finding-body {
            padding: 20px;
            display: none;
        }

        .finding.expanded .finding-body {
            display: block;
        }

        .finding-section {
            margin-bottom: 20px;
        }

        .finding-section h4 {
            font-size: 0.9rem;
            color: #475569;
            margin-bottom: 10px;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        .evidence-list, .remediation-steps {
            list-style: none;
            padding-left: 0;
        }

        .evidence-list li {
            padding: 8px 0;
            border-bottom: 1px solid #f1f5f9;
        }

        .evidence-list li:before {
            content: "â–¸ ";
            color: #2563eb;
            font-weight: 700;
            margin-right: 8px;
        }

        .remediation-steps li {
            padding: 10px 0 10px 30px;
            position: relative;
        }

        .remediation-steps li:before {
            content: counter(step);
            counter-increment: step;
            position: absolute;
            left: 0;
            top: 10px;
            background: #2563eb;
            color: white;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.75rem;
            font-weight: 700;
        }

        .remediation-steps {
            counter-reset: step;
        }

        code {
            background: #f1f5f9;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: #1e293b;
            color: #e2e8f0;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            margin: 10px 0;
        }

        pre code {
            background: none;
            color: inherit;
            padding: 0;
        }

        /* Footer */
        footer {
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #e2e8f0;
            text-align: center;
            color: #64748b;
            font-size: 0.9rem;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }

            .summary-grid {
                grid-template-columns: 1fr;
            }

            .controls {
                flex-direction: column;
            }

            input[type="search"] {
                min-width: 100%;
            }

            .cert-ladder {
                flex-direction: column;
            }

            .cert-level {
                margin: 5px 0;
            }
        }

        /* Hidden class */
        .hidden {
            display: none !important;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ðŸ¤– AgentReady Assessment Report</h1>
            <p class="subtitle">agentready â€¢ November 21, 2025 at 02:11</p>
        </header>

        <!-- Summary Cards -->
        <div class="summary-grid">
            <div class="summary-card score">
                <h3>Overall Score</h3>
                <div class="value">75.4</div>
            </div>
            <div class="summary-card certification">
                <h3>Certification</h3>
                <div class="value">Gold</div>
            </div>
            <div class="summary-card assessed">
                <h3>Assessed</h3>
                <div class="value">10/25</div>
            </div>
            <div class="summary-card duration">
                <h3>Duration</h3>
                <div class="value">0.2s</div>
            </div>
        </div>

        <!-- Certification Ladder -->
        <div class="cert-ladder">
            <div class="cert-level platinum ">
                <h4>ðŸ’Ž Platinum</h4>
                <p class="range">90-100</p>
            </div>
            <div class="cert-level gold active">
                <h4>ðŸ¥‡ Gold</h4>
                <p class="range">75-89</p>
            </div>
            <div class="cert-level silver ">
                <h4>ðŸ¥ˆ Silver</h4>
                <p class="range">60-74</p>
            </div>
            <div class="cert-level bronze ">
                <h4>ðŸ¥‰ Bronze</h4>
                <p class="range">40-59</p>
            </div>
            <div class="cert-level needs-improvement ">
                <h4>âš ï¸ Needs Work</h4>
                <p class="range">0-39</p>
            </div>
        </div>

        <!-- Controls -->
        <div class="controls">
            <div class="control-group">
                <label>Filter:</label>
                <button class="filter-btn active" data-filter="all">All <span class="badge">25</span></button>
                <button class="filter-btn" data-filter="pass">Pass <span class="badge">7</span></button>
                <button class="filter-btn" data-filter="fail">Fail <span class="badge">3</span></button>
                <button class="filter-btn" data-filter="skipped">Skipped <span class="badge">15</span></button>
            </div>

            <div class="control-group">
                <label>Sort:</label>
                <select id="sort-select">
                    <option value="category">Category</option>
                    <option value="score-desc">Score (High to Low)</option>
                    <option value="score-asc">Score (Low to High)</option>
                    <option value="tier">Tier (Essential First)</option>
                </select>
            </div>

            <div class="control-group">
                <input type="search" id="search-box" placeholder="Search attributes...">
            </div>
        </div>

        <!-- Findings -->
        <div class="findings" id="findings-container">
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Context Window Optimization"
                 data-score="100.0"
                 data-name="claude.md configuration files">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âœ…
                            
                        </span>
                        <div class="finding-info">
                            <h3>CLAUDE.md Configuration Files</h3>
                            <div class="finding-meta">
                                Context Window Optimization â€¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 â€¢ present
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Documentation Standards"
                 data-score="100.0"
                 data-name="readme structure">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âœ…
                            
                        </span>
                        <div class="finding-info">
                            <h3>README Structure</h3>
                            <div class="finding-meta">
                                Documentation Standards â€¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 â€¢ 3/3 sections
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Found 3/3 essential sections</li>
                            
                            <li>Installation: âœ“</li>
                            
                            <li>Usage: âœ“</li>
                            
                            <li>Development: âœ“</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Code Quality"
                 data-score="100.0"
                 data-name="type annotations">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âœ…
                            
                        </span>
                        <div class="finding-info">
                            <h3>Type Annotations</h3>
                            <div class="finding-meta">
                                Code Quality â€¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 â€¢ 95.9%
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Typed functions: 142/148</li>
                            
                            <li>Coverage: 95.9%</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="1"
                 data-category="Repository Structure"
                 data-score="100.0"
                 data-name="standard project layouts">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âœ…
                            
                        </span>
                        <div class="finding-info">
                            <h3>Standard Project Layouts</h3>
                            <div class="finding-meta">
                                Repository Structure â€¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 â€¢ 2/2 directories
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Found 2/2 standard directories</li>
                            
                            <li>src/: âœ“</li>
                            
                            <li>tests/: âœ“</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="1"
                 data-category="Dependency Management"
                 data-score="0"
                 data-name="lock files for reproducibility">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŒ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Lock Files for Reproducibility</h3>
                            <div class="finding-meta">
                                Dependency Management â€¢
                                <span class="tier-badge tier-1">Tier 1</span>
                                 â€¢ none
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>No lock files found</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Add lock file for dependency reproducibility</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Use npm install, poetry lock, or equivalent to generate lock file</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>npm install  # generates package-lock.json</code></pre>
                        

                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="2"
                 data-category="Testing & CI/CD"
                 data-score="100.0"
                 data-name="test coverage requirements">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âœ…
                            
                        </span>
                        <div class="finding-info">
                            <h3>Test Coverage Requirements</h3>
                            <div class="finding-meta">
                                Testing & CI/CD â€¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 â€¢ configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Coverage configuration found</li>
                            
                            <li>pytest-cov dependency present</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="2"
                 data-category="Testing & CI/CD"
                 data-score="0"
                 data-name="pre-commit hooks & ci/cd linting">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŒ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Pre-commit Hooks & CI/CD Linting</h3>
                            <div class="finding-meta">
                                Testing & CI/CD â€¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 â€¢ not configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>.pre-commit-config.yaml not found</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Configure pre-commit hooks for automated code quality checks</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Install pre-commit framework</li>
                            
                            <li>Create .pre-commit-config.yaml</li>
                            
                            <li>Add hooks for linting and formatting</li>
                            
                            <li>Install hooks: pre-commit install</li>
                            
                            <li>Run on all files: pre-commit run --all-files</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>pip install pre-commit
pre-commit install
pre-commit run --all-files</code></pre>
                        

                        
                        <h4 style="margin-top: 15px;">Examples</h4>
                        
                        <pre><code># .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
</code></pre>
                        
                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="fail"
                 data-tier="2"
                 data-category="Git & Version Control"
                 data-score="0"
                 data-name="conventional commit messages">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŒ
                            
                        </span>
                        <div class="finding-info">
                            <h3>Conventional Commit Messages</h3>
                            <div class="finding-meta">
                                Git & Version Control â€¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 â€¢ not configured
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-fail">
                        0
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>No commitlint or husky configuration</li>
                            
                        </ul>
                    </div>
                    

                    
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>Configure conventional commits with commitlint</strong></p>

                        
                        <ol class="remediation-steps">
                            
                            <li>Install commitlint</li>
                            
                            <li>Configure husky for commit-msg hook</li>
                            
                        </ol>
                        

                        
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>npm install --save-dev @commitlint/cli @commitlint/config-conventional husky</code></pre>
                        

                        
                    </div>
                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="2"
                 data-category="Git & Version Control"
                 data-score="100.0"
                 data-name=".gitignore completeness">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âœ…
                            
                        </span>
                        <div class="finding-info">
                            <h3>.gitignore Completeness</h3>
                            <div class="finding-meta">
                                Git & Version Control â€¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                 â€¢ 580 bytes
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>.gitignore found (580 bytes)</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="pass"
                 data-tier="3"
                 data-category="Code Quality"
                 data-score="100.0"
                 data-name="cyclomatic complexity thresholds">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âœ…
                            
                        </span>
                        <div class="finding-info">
                            <h3>Cyclomatic Complexity Thresholds</h3>
                            <div class="finding-meta">
                                Code Quality â€¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                 â€¢ 3.1
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-pass">
                        100
                    </div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Average cyclomatic complexity: 3.1</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Build & Development"
                 data-score="0"
                 data-name="one-command build/setup">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŠ˜
                            
                        </span>
                        <div class="finding-info">
                            <h3>One-Command Build/Setup</h3>
                            <div class="finding-meta">
                                Build & Development â€¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">â€”</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>One-Command Build/Setup assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Context Window Optimization"
                 data-score="0"
                 data-name="concise structured documentation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŠ˜
                            
                        </span>
                        <div class="finding-info">
                            <h3>Concise Structured Documentation</h3>
                            <div class="finding-meta">
                                Context Window Optimization â€¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">â€”</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Concise Structured Documentation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Documentation Standards"
                 data-score="0"
                 data-name="inline documentation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŠ˜
                            
                        </span>
                        <div class="finding-info">
                            <h3>Inline Documentation</h3>
                            <div class="finding-meta">
                                Documentation Standards â€¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">â€”</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Inline Documentation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Context Window Optimization"
                 data-score="0"
                 data-name="file size limits">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŠ˜
                            
                        </span>
                        <div class="finding-info">
                            <h3>File Size Limits</h3>
                            <div class="finding-meta">
                                Context Window Optimization â€¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">â€”</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>File Size Limits assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Dependency Management"
                 data-score="0"
                 data-name="dependency freshness & security">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŠ˜
                            
                        </span>
                        <div class="finding-info">
                            <h3>Dependency Freshness & Security</h3>
                            <div class="finding-meta">
                                Dependency Management â€¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">â€”</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Dependency Freshness & Security assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="2"
                 data-category="Repository Structure"
                 data-score="0"
                 data-name="separation of concerns">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŠ˜
                            
                        </span>
                        <div class="finding-info">
                            <h3>Separation of Concerns</h3>
                            <div class="finding-meta">
                                Repository Structure â€¢
                                <span class="tier-badge tier-2">Tier 2</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">â€”</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Separation of Concerns assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Error Handling"
                 data-score="0"
                 data-name="structured logging">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŠ˜
                            
                        </span>
                        <div class="finding-info">
                            <h3>Structured Logging</h3>
                            <div class="finding-meta">
                                Error Handling â€¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">â€”</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Structured Logging assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="API Documentation"
                 data-score="0"
                 data-name="openapi/swagger specifications">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŠ˜
                            
                        </span>
                        <div class="finding-info">
                            <h3>OpenAPI/Swagger Specifications</h3>
                            <div class="finding-meta">
                                API Documentation â€¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">â€”</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>OpenAPI/Swagger Specifications assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Documentation Standards"
                 data-score="0"
                 data-name="architecture decision records">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŠ˜
                            
                        </span>
                        <div class="finding-info">
                            <h3>Architecture Decision Records</h3>
                            <div class="finding-meta">
                                Documentation Standards â€¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">â€”</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Architecture Decision Records assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="3"
                 data-category="Modularity"
                 data-score="0"
                 data-name="semantic file & directory naming">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŠ˜
                            
                        </span>
                        <div class="finding-info">
                            <h3>Semantic File & Directory Naming</h3>
                            <div class="finding-meta">
                                Modularity â€¢
                                <span class="tier-badge tier-3">Tier 3</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">â€”</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Semantic File & Directory Naming assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Security"
                 data-score="0"
                 data-name="security scanning automation">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŠ˜
                            
                        </span>
                        <div class="finding-info">
                            <h3>Security Scanning Automation</h3>
                            <div class="finding-meta">
                                Security â€¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">â€”</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Security Scanning Automation assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Performance"
                 data-score="0"
                 data-name="performance benchmarks">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŠ˜
                            
                        </span>
                        <div class="finding-info">
                            <h3>Performance Benchmarks</h3>
                            <div class="finding-meta">
                                Performance â€¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">â€”</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Performance Benchmarks assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Code Quality"
                 data-score="0"
                 data-name="code smell elimination">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŠ˜
                            
                        </span>
                        <div class="finding-info">
                            <h3>Code Smell Elimination</h3>
                            <div class="finding-meta">
                                Code Quality â€¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">â€”</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Code Smell Elimination assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Git & Version Control"
                 data-score="0"
                 data-name="issue & pull request templates">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŠ˜
                            
                        </span>
                        <div class="finding-info">
                            <h3>Issue & Pull Request Templates</h3>
                            <div class="finding-meta">
                                Git & Version Control â€¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">â€”</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Issue & Pull Request Templates assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
            <div class="finding"
                 data-status="not_applicable"
                 data-tier="4"
                 data-category="Build & Development"
                 data-score="0"
                 data-name="container/virtualization setup">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            âŠ˜
                            
                        </span>
                        <div class="finding-info">
                            <h3>Container/Virtualization Setup</h3>
                            <div class="finding-meta">
                                Build & Development â€¢
                                <span class="tier-badge tier-4">Tier 4</span>
                                
                            </div>
                        </div>
                    </div>
                    
                    <div class="score-display score-skip">â€”</div>
                    
                </div>

                <div class="finding-body">
                    
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            
                            <li>Container/Virtualization Setup assessment not yet implemented</li>
                            
                        </ul>
                    </div>
                    

                    

                    
                </div>
            </div>
            
        </div>

        <footer>
            <p>Generated by <strong>AgentReady v1.0.0</strong></p>
            <p>Repository: /Users/jeder/repos/sk/agentready â€¢ Branch: 001-agentready-scorer â€¢ Commit: adfc4c82</p>
            <p style="margin-top: 10px; font-size: 0.85rem;">
                ðŸ¤– Generated with <a href="https://claude.com/claude-code" style="color: #2563eb;">Claude Code</a>
            </p>
        </footer>
    </div>

    <script>
        // Embedded assessment data
        const ASSESSMENT = {"repository": {"path": "/Users/jeder/repos/sk/agentready", "name": "agentready", "url": null, "branch": "001-agentready-scorer", "commit_hash": "adfc4c823a7b25d5b1404c9b994461e290257dce", "languages": {"Markdown": 36, "Shell": 5, "Python": 31}, "total_files": 79, "total_lines": 13852}, "timestamp": "2025-11-21T02:11:05.766923", "overall_score": 75.4, "certification_level": "Gold", "attributes_assessed": 10, "attributes_skipped": 15, "attributes_total": 25, "findings": [{"attribute": {"id": "claude_md_file", "name": "CLAUDE.md Configuration Files", "category": "Context Window Optimization", "tier": 1, "description": "Project-specific configuration for Claude Code", "criteria": "CLAUDE.md file exists in repository root", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "present", "threshold": "present", "evidence": ["CLAUDE.md found at /Users/jeder/repos/sk/agentready/CLAUDE.md"], "remediation": null, "error_message": null}, {"attribute": {"id": "readme_structure", "name": "README Structure", "category": "Documentation Standards", "tier": 1, "description": "Well-structured README with key sections", "criteria": "README.md with installation, usage, and development sections", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "3/3 sections", "threshold": "3/3 sections", "evidence": ["Found 3/3 essential sections", "Installation: \u2713", "Usage: \u2713", "Development: \u2713"], "remediation": null, "error_message": null}, {"attribute": {"id": "type_annotations", "name": "Type Annotations", "category": "Code Quality", "tier": 1, "description": "Type hints in function signatures", "criteria": ">80% of functions have type annotations", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "95.9%", "threshold": "\u226580%", "evidence": ["Typed functions: 142/148", "Coverage: 95.9%"], "remediation": null, "error_message": null}, {"attribute": {"id": "standard_layout", "name": "Standard Project Layouts", "category": "Repository Structure", "tier": 1, "description": "Follows standard project structure for language", "criteria": "Standard directories (src/, tests/, docs/) present", "default_weight": 0.1}, "status": "pass", "score": 100.0, "measured_value": "2/2 directories", "threshold": "2/2 directories", "evidence": ["Found 2/2 standard directories", "src/: \u2713", "tests/: \u2713"], "remediation": null, "error_message": null}, {"attribute": {"id": "lock_files", "name": "Lock Files for Reproducibility", "category": "Dependency Management", "tier": 1, "description": "Lock files present for dependency pinning", "criteria": "package-lock.json, yarn.lock, poetry.lock, or requirements.txt with versions", "default_weight": 0.1}, "status": "fail", "score": 0.0, "measured_value": "none", "threshold": "at least one lock file", "evidence": ["No lock files found"], "remediation": {"summary": "Add lock file for dependency reproducibility", "steps": ["Use npm install, poetry lock, or equivalent to generate lock file"], "tools": [], "commands": ["npm install  # generates package-lock.json"], "examples": [], "citations": []}, "error_message": null}, {"attribute": {"id": "test_coverage", "name": "Test Coverage Requirements", "category": "Testing & CI/CD", "tier": 2, "description": "Test coverage thresholds configured and enforced", "criteria": ">80% code coverage", "default_weight": 0.03}, "status": "pass", "score": 100.0, "measured_value": "configured", "threshold": "configured with >80% threshold", "evidence": ["Coverage configuration found", "pytest-cov dependency present"], "remediation": null, "error_message": null}, {"attribute": {"id": "precommit_hooks", "name": "Pre-commit Hooks & CI/CD Linting", "category": "Testing & CI/CD", "tier": 2, "description": "Pre-commit hooks configured for linting and formatting", "criteria": ".pre-commit-config.yaml exists", "default_weight": 0.03}, "status": "fail", "score": 0.0, "measured_value": "not configured", "threshold": "configured", "evidence": [".pre-commit-config.yaml not found"], "remediation": {"summary": "Configure pre-commit hooks for automated code quality checks", "steps": ["Install pre-commit framework", "Create .pre-commit-config.yaml", "Add hooks for linting and formatting", "Install hooks: pre-commit install", "Run on all files: pre-commit run --all-files"], "tools": ["pre-commit"], "commands": ["pip install pre-commit", "pre-commit install", "pre-commit run --all-files"], "examples": ["# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n\n  - repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n      - id: black\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n"], "citations": [{"source": "pre-commit.com", "title": "Pre-commit Framework", "url": "https://pre-commit.com/", "relevance": "Official pre-commit documentation"}]}, "error_message": null}, {"attribute": {"id": "conventional_commits", "name": "Conventional Commit Messages", "category": "Git & Version Control", "tier": 2, "description": "Follows conventional commit format", "criteria": "\u226580% of recent commits follow convention", "default_weight": 0.03}, "status": "fail", "score": 0.0, "measured_value": "not configured", "threshold": "configured", "evidence": ["No commitlint or husky configuration"], "remediation": {"summary": "Configure conventional commits with commitlint", "steps": ["Install commitlint", "Configure husky for commit-msg hook"], "tools": ["commitlint", "husky"], "commands": ["npm install --save-dev @commitlint/cli @commitlint/config-conventional husky"], "examples": [], "citations": []}, "error_message": null}, {"attribute": {"id": "gitignore_completeness", "name": ".gitignore Completeness", "category": "Git & Version Control", "tier": 2, "description": "Comprehensive .gitignore file", "criteria": ".gitignore exists and covers common patterns", "default_weight": 0.03}, "status": "pass", "score": 100.0, "measured_value": "580 bytes", "threshold": ">50 bytes", "evidence": [".gitignore found (580 bytes)"], "remediation": null, "error_message": null}, {"attribute": {"id": "cyclomatic_complexity", "name": "Cyclomatic Complexity Thresholds", "category": "Code Quality", "tier": 3, "description": "Cyclomatic complexity thresholds enforced", "criteria": "Average complexity <10, no functions >15", "default_weight": 0.03}, "status": "pass", "score": 100.0, "measured_value": "3.1", "threshold": "<10.0", "evidence": ["Average cyclomatic complexity: 3.1"], "remediation": null, "error_message": null}, {"attribute": {"id": "one_command_setup", "name": "One-Command Build/Setup", "category": "Build & Development", "tier": 2, "description": "Assessment for One-Command Build/Setup", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["One-Command Build/Setup assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "concise_documentation", "name": "Concise Structured Documentation", "category": "Context Window Optimization", "tier": 2, "description": "Assessment for Concise Structured Documentation", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Concise Structured Documentation assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "inline_documentation", "name": "Inline Documentation", "category": "Documentation Standards", "tier": 2, "description": "Assessment for Inline Documentation", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Inline Documentation assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "file_size_limits", "name": "File Size Limits", "category": "Context Window Optimization", "tier": 2, "description": "Assessment for File Size Limits", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["File Size Limits assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "dependency_freshness", "name": "Dependency Freshness & Security", "category": "Dependency Management", "tier": 2, "description": "Assessment for Dependency Freshness & Security", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Dependency Freshness & Security assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "separation_concerns", "name": "Separation of Concerns", "category": "Repository Structure", "tier": 2, "description": "Assessment for Separation of Concerns", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Separation of Concerns assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "structured_logging", "name": "Structured Logging", "category": "Error Handling", "tier": 3, "description": "Assessment for Structured Logging", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Structured Logging assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "openapi_specs", "name": "OpenAPI/Swagger Specifications", "category": "API Documentation", "tier": 3, "description": "Assessment for OpenAPI/Swagger Specifications", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["OpenAPI/Swagger Specifications assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "architecture_decisions", "name": "Architecture Decision Records", "category": "Documentation Standards", "tier": 3, "description": "Assessment for Architecture Decision Records", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Architecture Decision Records assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "semantic_naming", "name": "Semantic File & Directory Naming", "category": "Modularity", "tier": 3, "description": "Assessment for Semantic File & Directory Naming", "criteria": "To be implemented", "default_weight": 0.03}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Semantic File & Directory Naming assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "security_scanning", "name": "Security Scanning Automation", "category": "Security", "tier": 4, "description": "Assessment for Security Scanning Automation", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Security Scanning Automation assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "performance_benchmarks", "name": "Performance Benchmarks", "category": "Performance", "tier": 4, "description": "Assessment for Performance Benchmarks", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Performance Benchmarks assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "code_smells", "name": "Code Smell Elimination", "category": "Code Quality", "tier": 4, "description": "Assessment for Code Smell Elimination", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Code Smell Elimination assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "issue_pr_templates", "name": "Issue & Pull Request Templates", "category": "Git & Version Control", "tier": 4, "description": "Assessment for Issue & Pull Request Templates", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Issue & Pull Request Templates assessment not yet implemented"], "remediation": null, "error_message": null}, {"attribute": {"id": "container_setup", "name": "Container/Virtualization Setup", "category": "Build & Development", "tier": 4, "description": "Assessment for Container/Virtualization Setup", "criteria": "To be implemented", "default_weight": 0.01}, "status": "not_applicable", "score": null, "measured_value": null, "threshold": null, "evidence": ["Container/Virtualization Setup assessment not yet implemented"], "remediation": null, "error_message": null}], "config": null, "duration_seconds": 0.2};

        // Toggle finding expansion
        function toggleFinding(header) {
            const finding = header.parentElement;
            finding.classList.toggle('expanded');
        }

        // Filter functionality
        document.querySelectorAll('.filter-btn').forEach(btn => {
            btn.addEventListener('click', () => {
                // Update active button
                document.querySelectorAll('.filter-btn').forEach(b => b.classList.remove('active'));
                btn.classList.add('active');

                const filter = btn.dataset.filter;
                const findings = document.querySelectorAll('.finding');

                findings.forEach(finding => {
                    const status = finding.dataset.status;

                    if (filter === 'all') {
                        finding.classList.remove('hidden');
                    } else if (filter === 'skipped') {
                        if (status === 'skipped' || status === 'not_applicable' || status === 'error') {
                            finding.classList.remove('hidden');
                        } else {
                            finding.classList.add('hidden');
                        }
                    } else {
                        if (status === filter) {
                            finding.classList.remove('hidden');
                        } else {
                            finding.classList.add('hidden');
                        }
                    }
                });
            });
        });

        // Sort functionality
        document.getElementById('sort-select').addEventListener('change', (e) => {
            const sortBy = e.target.value;
            const container = document.getElementById('findings-container');
            const findings = Array.from(container.querySelectorAll('.finding'));

            findings.sort((a, b) => {
                if (sortBy === 'category') {
                    return a.dataset.category.localeCompare(b.dataset.category);
                } else if (sortBy === 'score-desc') {
                    return parseFloat(b.dataset.score) - parseFloat(a.dataset.score);
                } else if (sortBy === 'score-asc') {
                    return parseFloat(a.dataset.score) - parseFloat(b.dataset.score);
                } else if (sortBy === 'tier') {
                    return parseInt(a.dataset.tier) - parseInt(b.dataset.tier);
                }
            });

            findings.forEach(finding => container.appendChild(finding));
        });

        // Search functionality
        document.getElementById('search-box').addEventListener('input', (e) => {
            const query = e.target.value.toLowerCase();
            const findings = document.querySelectorAll('.finding');

            findings.forEach(finding => {
                const name = finding.dataset.name;
                const category = finding.dataset.category.toLowerCase();

                if (name.includes(query) || category.includes(query)) {
                    finding.classList.remove('hidden');
                } else {
                    finding.classList.add('hidden');
                }
            });
        });
    </script>
</body>
</html>
````

## File: examples/self-assessment/report-20251121.md
````markdown
# ðŸ¤– AgentReady Assessment Report

**Repository**: agentready
**Path**: `/Users/jeder/repos/sk/agentready`
**Branch**: 001-agentready-scorer
**Commit**: adfc4c82
**Date**: 2025-11-21 02:11:05

---

## ðŸ“Š Summary

| Metric | Value |
|--------|-------|
| **Overall Score** | **75.4/100** |
| **Certification Level** | **Gold** |
| **Attributes Assessed** | 10/25 |
| **Attributes Skipped** | 15 |
| **Assessment Duration** | 0.2s |

### Languages Detected

- **Markdown**: 36 files
- **Python**: 31 files
- **Shell**: 5 files

### Repository Stats

- **Total Files**: 79
- **Total Lines**: 13,852

## ðŸŽ–ï¸ Certification Ladder

- ðŸ’Ž **Platinum** (90-100) 
- ðŸ¥‡ **Gold** (75-89) **â†’ YOUR LEVEL â†**
- ðŸ¥ˆ **Silver** (60-74) 
- ðŸ¥‰ **Bronze** (40-59) 
- âš ï¸ **Needs Improvement** (0-39) 

## ðŸ“‹ Detailed Findings

### API Documentation

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| OpenAPI/Swagger Specifications | T3 | âŠ˜ not_applicable | â€” |

### Build & Development

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| One-Command Build/Setup | T2 | âŠ˜ not_applicable | â€” |
| Container/Virtualization Setup | T4 | âŠ˜ not_applicable | â€” |

### Code Quality

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Type Annotations | T1 | âœ… pass | 100 |
| Cyclomatic Complexity Thresholds | T3 | âœ… pass | 100 |
| Code Smell Elimination | T4 | âŠ˜ not_applicable | â€” |

### Context Window Optimization

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| CLAUDE.md Configuration Files | T1 | âœ… pass | 100 |
| Concise Structured Documentation | T2 | âŠ˜ not_applicable | â€” |
| File Size Limits | T2 | âŠ˜ not_applicable | â€” |

### Dependency Management

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Lock Files for Reproducibility | T1 | âŒ fail | 0 |
| Dependency Freshness & Security | T2 | âŠ˜ not_applicable | â€” |

#### âŒ Lock Files for Reproducibility

**Measured**: none (Threshold: at least one lock file)

**Evidence**:
- No lock files found

<details><summary><strong>ðŸ“ Remediation Steps</strong></summary>


Add lock file for dependency reproducibility

1. Use npm install, poetry lock, or equivalent to generate lock file

**Commands**:

```bash
npm install  # generates package-lock.json
```

</details>

### Documentation Standards

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| README Structure | T1 | âœ… pass | 100 |
| Inline Documentation | T2 | âŠ˜ not_applicable | â€” |
| Architecture Decision Records | T3 | âŠ˜ not_applicable | â€” |

### Error Handling

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Structured Logging | T3 | âŠ˜ not_applicable | â€” |

### Git & Version Control

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Conventional Commit Messages | T2 | âŒ fail | 0 |
| .gitignore Completeness | T2 | âœ… pass | 100 |
| Issue & Pull Request Templates | T4 | âŠ˜ not_applicable | â€” |

#### âŒ Conventional Commit Messages

**Measured**: not configured (Threshold: configured)

**Evidence**:
- No commitlint or husky configuration

<details><summary><strong>ðŸ“ Remediation Steps</strong></summary>


Configure conventional commits with commitlint

1. Install commitlint
2. Configure husky for commit-msg hook

**Commands**:

```bash
npm install --save-dev @commitlint/cli @commitlint/config-conventional husky
```

</details>

### Modularity

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Semantic File & Directory Naming | T3 | âŠ˜ not_applicable | â€” |

### Performance

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Performance Benchmarks | T4 | âŠ˜ not_applicable | â€” |

### Repository Structure

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Standard Project Layouts | T1 | âœ… pass | 100 |
| Separation of Concerns | T2 | âŠ˜ not_applicable | â€” |

### Security

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Security Scanning Automation | T4 | âŠ˜ not_applicable | â€” |

### Testing & CI/CD

| Attribute | Tier | Status | Score |
|-----------|------|--------|-------|
| Test Coverage Requirements | T2 | âœ… pass | 100 |
| Pre-commit Hooks & CI/CD Linting | T2 | âŒ fail | 0 |

#### âŒ Pre-commit Hooks & CI/CD Linting

**Measured**: not configured (Threshold: configured)

**Evidence**:
- .pre-commit-config.yaml not found

<details><summary><strong>ðŸ“ Remediation Steps</strong></summary>


Configure pre-commit hooks for automated code quality checks

1. Install pre-commit framework
2. Create .pre-commit-config.yaml
3. Add hooks for linting and formatting
4. Install hooks: pre-commit install
5. Run on all files: pre-commit run --all-files

**Commands**:

```bash
pip install pre-commit
pre-commit install
pre-commit run --all-files
```

**Examples**:

```
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort

```

</details>

## ðŸŽ¯ Next Steps

**Priority Improvements** (highest impact first):

1. **Lock Files for Reproducibility** (Tier 1) - +10.0 points potential
   - Add lock file for dependency reproducibility
2. **Pre-commit Hooks & CI/CD Linting** (Tier 2) - +3.0 points potential
   - Configure pre-commit hooks for automated code quality checks
3. **Conventional Commit Messages** (Tier 2) - +3.0 points potential
   - Configure conventional commits with commitlint

---

## ðŸ“ Assessment Metadata

- **Tool Version**: AgentReady v1.0.0
- **Research Report**: Bundled version
- **Repository Snapshot**: adfc4c823a7b25d5b1404c9b994461e290257dce
- **Assessment Duration**: 0.2s

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)
````

## File: specs/001-agentready-scorer/.remediation/A1-proportional-scoring.md
````markdown
# Remediation A1: Proportional Scoring Formula

**Issue**: FR-014 mentions proportional scoring but doesn't define the algorithm.

**Target File**: `specs/001-agentready-scorer/research.md`

**Action**: Add new section after existing technical decisions (after Decision 10)

---

## Content to Add

### Decision 11: Proportional Scoring Algorithm

**Context**: Many attributes have measurable thresholds that can be partially met (e.g., 65% test coverage when 80% is required). Linear proportional scoring provides deterministic, understandable results.

**Decision**: Use linear proportional scoring with the following formula:

```python
def calculate_proportional_score(measured_value, threshold, attribute_type):
    """
    Calculate proportional score for partial compliance.

    Args:
        measured_value: The measured value (numeric or parseable)
        threshold: The target threshold
        attribute_type: 'higher_is_better' or 'lower_is_better'

    Returns:
        Score from 0-100
    """
    if attribute_type == 'higher_is_better':
        # Example: test coverage (want higher values)
        if measured_value >= threshold:
            return 100
        elif measured_value <= 0:
            return 0
        else:
            return (measured_value / threshold) * 100

    elif attribute_type == 'lower_is_better':
        # Example: file length (want lower values)
        if measured_value <= threshold:
            return 100
        elif threshold == 0:
            return 0  # Avoid division by zero
        else:
            # Degrade linearly, cap at 0
            return max(0, 100 - ((measured_value - threshold) / threshold) * 100)
```

**Edge Cases**:
- Division by zero: Return 0 score
- Negative values: Clamp to 0
- Values exceeding 2x threshold (lower_is_better): Cap at 0
- Values exceeding 2x threshold (higher_is_better): Cap at 100

**Examples**:
- Test coverage: 65% measured, 80% threshold â†’ 65/80 * 100 = 81.25 score
- File length: 450 lines measured, 300 threshold â†’ 100 - ((450-300)/300)*100 = 50 score
- Cyclomatic complexity: 5 measured, 10 threshold â†’ 100 (meets threshold)

**Rationale**: Linear proportional scoring is:
- **Simple**: Easy to understand and explain to users
- **Deterministic**: Same inputs always produce same outputs
- **Fair**: Provides clear incentives for incremental improvement
- **Predictable**: Users can calculate expected score changes

Non-linear scoring (exponential penalties, sigmoid curves) was considered but rejected due to complexity and reduced transparency.

**References**:
- Spec FR-014: "Tool MUST handle repositories that partially meet attribute criteria"
- Spec FR-027: "Tool MUST calculate overall score based only on successfully evaluated attributes"
- Research report: All 25 attributes have quantifiable thresholds
````

## File: specs/001-agentready-scorer/.remediation/A2-tier-weight-distribution.md
````markdown
# Remediation A2: Tier-Based Weight Distribution

**Issue**: FR-031 references tier-based weighting but doesn't specify the distribution.

**Target File**: `specs/001-agentready-scorer/data-model.md`

**Action**: Add new section after entities section

---

## Content to Add

## Weight Distribution

### Default Tier-Based Weights

Per FR-031, attributes are weighted by tier priority with heavy penalties for missing essentials (especially CLAUDE.md). Tier 1 (Essential) attributes have 8x the impact of Tier 4 (Advanced) attributes.

| Tier | Description | Total Weight | Attributes | Weight Per Attribute |
|------|-------------|--------------|------------|---------------------|
| Tier 1 | Essential | 50% | 5 attributes | 10.0% each |
| Tier 2 | Critical | 30% | 10 attributes | 3.0% each |
| Tier 3 | Important | 15% | 5 attributes | 3.0% each |
| Tier 4 | Advanced | 5% | 5 attributes | 1.0% each |

**Total**: 100% across 25 attributes

**Penalty Philosophy**: Missing Tier 1 essentials (especially CLAUDE.md at 10%) creates significant score impact, incentivizing foundational improvements before advanced optimizations.

### Attribute-to-Tier Mapping

Based on research report tier assignments (lines 1535-1570):

**Tier 1 (Essential)** - 50% total weight (10% each):
1. CLAUDE.md Configuration Files (1.1) - **10.0%** âš ï¸ CRITICAL
2. README Structure (2.1) - 10.0%
3. Type Annotations (3.3) - 10.0%
4. Standard Project Layouts (4.1) - 10.0%
5. Lock Files for Reproducibility (6.1) - 10.0%

**Tier 2 (Critical)** - 30% total weight (3% each):
6. Test Coverage Requirements (5.1) - 3.0%
7. Pre-commit Hooks & CI/CD Linting (5.3) - 3.0%
8. Conventional Commit Messages (7.1) - 3.0%
9. .gitignore Completeness (7.2) - 3.0%
10. One-Command Build/Setup (8.1) - 3.0%
11. Concise Structured Documentation (1.2) - 3.0%
12. Inline Documentation (2.2) - 3.0%
13. File Size Limits (1.3) - 3.0%
14. Dependency Freshness & Security (6.2) - 3.0%
15. Separation of Concerns (4.2) - 3.0%

**Tier 3 (Important)** - 15% total weight (3% each):
16. Cyclomatic Complexity Thresholds (3.1) - 3.0%
17. Structured Logging (9.2) - 3.0%
18. OpenAPI/Swagger Specifications (10.1) - 3.0%
19. Architecture Decision Records (2.3) - 3.0%
20. Semantic File & Directory Naming (11.3) - 3.0%

**Tier 4 (Advanced)** - 5% total weight (1% each):
21. Security Scanning Automation (13.1) - 1.0%
22. Performance Benchmarks (15.1) - 1.0%
23. Code Smell Elimination (3.4) - 1.0%
24. Issue & Pull Request Templates (7.3) - 1.0%
25. Container/Virtualization Setup (8.3) - 1.0%

### Custom Weight Configuration

Users can override default weights via `.agentready-config.yaml`:

```yaml
weights:
  claude_md_file: 0.15          # Increase from 10% to 15% (org prioritizes CLAUDE.md)
  test_coverage: 0.05           # Increase from 3% to 5%
  conventional_commits: 0.01    # Decrease from 3% to 1%
  # ... other attributes use defaults, rescaled to sum to 1.0
```

**Validation** (per FR-033):
- All 25 attributes must be present (explicitly or via defaults)
- Weights must be positive numbers
- Weights must sum to 1.0 (Â±0.001 tolerance for floating point)
- Missing attributes inherit rescaled tier defaults

### Score Calculation Example

```python
overall_score = sum(
    attribute_score * weight
    for attribute_score, weight in zip(scores, weights)
    if attribute_status == 'assessed'  # Exclude skipped per FR-027
)

# Normalize if some attributes skipped
total_weight_assessed = sum(
    weight for weight, status in zip(weights, statuses)
    if status == 'assessed'
)
normalized_score = (overall_score / total_weight_assessed) * 100
```

**Example Scenario**:
- Repository missing CLAUDE.md: loses 10 points immediately
- Repository with CLAUDE.md but no tests: scores 10 (CLAUDE.md) + 0 (tests)
- Perfect CLAUDE.md + perfect tests = 10 + 3 = 13 points from just 2 attributes

### Configuration Precedence

When weights exist in multiple locations:

1. **CLI flags** (highest priority) - `--weight claude_md_file=0.15`
2. **Config file** - `.agentready-config.yaml`
3. **Tier defaults** (lowest priority) - Built-in distribution

**Rationale**: Tier-based distribution with heavy penalties for missing essentials ensures:
- Users prioritize foundational improvements (CLAUDE.md, README, types)
- Missing CLAUDE.md (10% weight) has 10x impact of missing container setup (1% weight)
- Aligns with research report's "Essential â†’ Advanced" priority guidance
- Steep gradient incentivizes completing Tier 1 before moving to advanced features
````

## File: specs/001-agentready-scorer/.remediation/README.md
````markdown
# Specification Remediation Summary

**Analysis Date**: 2025-11-21
**Analyst**: Claude Code Analysis
**Status**: Ready for Review

---

## Overview

This directory contains 5 remediation documents addressing HIGH and MEDIUM severity specification issues identified in the analysis phase. All content has been finalized based on user decisions and is ready for integration into the specification.

## Remediation Files

### A1-proportional-scoring.md (HIGH Priority)
**Issue**: Proportional scoring algorithm undefined
**Target**: `specs/001-agentready-scorer/research.md`
**Decision**: Linear proportional scoring (user approved)
**Impact**: Resolves FR-014 ambiguity, enables deterministic partial compliance scoring

### A2-tier-weight-distribution.md (HIGH Priority)
**Issue**: Tier-based weight distribution not specified
**Target**: `specs/001-agentready-scorer/data-model.md`
**Decision**: 50/30/15/5 distribution with heavy CLAUDE.md penalty (user approved)
**Impact**: Resolves FR-031 ambiguity, defines complete 25-attribute weight mapping
**Key Points**:
- CLAUDE.md gets 10% weight (critical penalty for missing)
- Tier 1: 50% (5 attributes @ 10% each)
- Actual tier assignments extracted from research report

### U1-research-update-mechanism.md (MEDIUM Priority)
**Issue**: Research report update process underspecified
**Target**: `specs/001-agentready-scorer/plan.md`
**Decision**: Explicit opt-in via `--update-research` only (user approved)
**Impact**: Resolves FR-023, enables research report updates with safety
**Key Points**:
- Bundled version is default
- Update only on explicit command
- Users can point to custom research files
- Atomic update with rollback on failure

### U2-research-validation.md (MEDIUM Priority)
**Issue**: Research report validation criteria missing
**Target**: Create new `specs/001-agentready-scorer/contracts/research-report-schema.md`
**Decision**: Warning-only validation (user approved)
**Impact**: Resolves FR-024, enables research report quality checks
**Key Points**:
- Errors block usage (missing metadata, wrong attribute count)
- Warnings allow usage (missing examples, low reference count)
- 25 attributes, 4 tiers, 20+ references required

### U3-weight-validation.md (MEDIUM Priority)
**Issue**: Weight validation rules incomplete
**Target**: `specs/001-agentready-scorer/data-model.md`
**Decision**: User recommendation approved (partial configs with rescaling)
**Impact**: Resolves FR-033, enables safe custom weight configuration
**Key Points**:
- Completeness, positivity, sum=1.0 Â±0.001 enforced
- Bounds (0.5%-20%) are warnings only
- Partial configs supported with automatic rescaling
- CLI > config file > tier defaults precedence

---

## Integration Steps

To integrate these remediations into the specification:

1. **Review Each Document**:
   ```bash
   cd specs/001-agentready-scorer/.remediation
   cat A1-proportional-scoring.md
   cat A2-tier-weight-distribution.md
   cat U1-research-update-mechanism.md
   cat U2-research-validation.md
   cat U3-weight-validation.md
   ```

2. **Apply Remediations**:
   - Copy content from A1 â†’ `research.md` (add Decision 11)
   - Copy content from A2 â†’ `data-model.md` (add Weight Distribution section)
   - Copy content from U1 â†’ `plan.md` (add to Technical Context)
   - Create new file from U2 â†’ `contracts/research-report-schema.md`
   - Copy content from U3 â†’ `data-model.md` (add Weight Validation section)

3. **Verify Integration**:
   ```bash
   # Check research.md has Decision 11
   grep "Decision 11" specs/001-agentready-scorer/research.md

   # Check data-model.md has weight sections
   grep "Weight Distribution" specs/001-agentready-scorer/data-model.md
   grep "Configuration Weight Validation" specs/001-agentready-scorer/data-model.md

   # Check plan.md has update mechanism
   grep "Research Report Update Mechanism" specs/001-agentready-scorer/plan.md

   # Check new contract file exists
   ls specs/001-agentready-scorer/contracts/research-report-schema.md
   ```

4. **Update Analysis Report**:
   - Mark A1, A2, U1, U2, U3 as RESOLVED
   - Re-run analysis to confirm no regressions
   - Update metrics (ambiguity count: 5 â†’ 0)

---

## User Decisions Summary

| Issue | Question | Decision |
|-------|----------|----------|
| A1 | Linear vs non-linear scoring? | Linear (simple, transparent) |
| A1 | Capping behavior? | Cap at 0/100, no negative scores |
| A2 | Weight distribution? | 50/30/15/5 across tiers |
| A2 | CLAUDE.md penalty? | Heavy (10%, highest single attribute) |
| A2 | Tier assignments? | Extract from actual research report |
| U1 | Update frequency? | Explicit `--update-research` only |
| U1 | Repository location? | `ambient-code/agentready` (future) |
| U1 | Version compatibility? | Tool + research versioned together |
| U2 | Validation strictness? | Errors block, warnings allow |
| U2 | Minimum references? | 20 citations (evidence threshold) |
| U3 | Partial configs? | Supported with automatic rescaling |
| U3 | Weight bounds? | 0.5%-20% warnings, not errors |
| U3 | Precedence? | CLI > config > tier defaults |

---

## Additional Items

### Backlog Created

**File**: `/Users/jeder/repos/sk/agentready/BACKLOG.md`

**Items Added**:
1. Bootstrap new GitHub repositories (P5)
2. Report schema versioning (P3)

**Note**: Bootstrap feature includes repository at `ambient-code/agentready` which does not yet exist. Do not push or implement until repository is created.

### Configuration Output

**Requirement**: JSON output needed in addition to HTML/Markdown
**Status**: Already specified in FR-005 and contracts/assessment-schema.json
**Action**: No changes needed (JSON already designed)

---

## Metrics After Remediation

**Before**:
- Ambiguity Count: 5
- Underspecification Count: 5
- High Severity: 3
- Medium Severity: 8

**After** (projected):
- Ambiguity Count: 0 (-5)
- Underspecification Count: 0 (-5)
- High Severity: 0 (-3)
- Medium Severity: 3 (terminology, documentation)

**Coverage**: 100% (unchanged)
**Constitution Compliance**: 100% (unchanged)

---

## Next Steps

**Immediate**:
1. Review remediation documents
2. Integrate into specification files
3. Re-run `/speckit.analyze` to verify resolution
4. Proceed to `/speckit.implement`

**Future**:
1. Create `ambient-code/agentready` GitHub repository
2. Implement bootstrap feature from backlog
3. Version report schemas per backlog item

**Status**: âœ… Ready for implementation
````

## File: specs/001-agentready-scorer/.remediation/U1-research-update-mechanism.md
````markdown
# Remediation U1: Research Report Update Mechanism

**Issue**: FR-023 requires update capability but doesn't specify the mechanism.

**Target File**: `specs/001-agentready-scorer/plan.md`

**Action**: Add subsection under "## Technical Context"

---

## Content to Add

### Research Report Update Mechanism

**Per FR-023**: Tool must support updating the bundled research report.

**Versioning Philosophy**: agentready releases are bundled with a specific research report version. The tool and research report are versioned together (e.g., agentready v1.2.0 includes research report v1.2.0).

**Implementation Design**:

1. **Bundled Report**:
   - Research report shipped with tool installation in `src/agentready/data/agent-ready-codebase-attributes.md`
   - Tool version and research version always match in releases
   - Users can always rely on stable, tested research content

2. **Custom Research Reports**:
   - Users can point to custom research reports:
     ```bash
     agentready --research-file /path/to/custom-research.md
     agentready --research-file https://example.com/custom-research.md
     ```
   - Enables organizations to customize criteria for internal standards
   - Custom reports must pass validation (per FR-024)

3. **Update Command** (explicit opt-in only):
   ```bash
   agentready --update-research
   ```

4. **Update Source**:
   - Primary: `https://github.com/ambient-code/agentready/raw/main/src/agentready/data/agent-ready-codebase-attributes.md`
   - Downloads latest research report from main branch
   - User explicitly opts in to update

5. **Update Process** (atomic):
   ```
   a. Download new version to temp file
   b. Validate structure (per FR-024):
      - Check for required metadata (version, date)
      - Verify 25 attributes present
      - Validate Markdown format
      - Parse successfully with research_loader.py
   c. If valid:
      - Backup current: agent-ready-codebase-attributes.md â†’ .md.backup
      - Atomic rename: temp â†’ agent-ready-codebase-attributes.md
      - Remove backup on success
      - Display: "Updated research report from vX.Y.Z to vA.B.C"
   d. If invalid:
      - Delete temp file
      - Restore from backup if needed
      - Exit with error message and validation details
      - Keep current version intact
   ```

6. **Version Tracking**:
   - Research report includes metadata header:
     ```markdown
     ---
     version: 1.2.0
     date: 2025-11-20
     ---
     ```
   - Tool displays current research version:
     ```bash
     agentready --research-version
     # Output: Research report v1.2.0 (2025-11-20)
     ```

7. **Offline Behavior**:
   - If network unavailable: "Cannot reach update server. Using bundled version v1.0.0"
   - If update fails: "Update failed (validation error). Using current version v1.0.0"
   - Never leave tool in broken state
   - Always fall back to last known-good version

8. **Rollback**:
   ```bash
   agentready --restore-bundled-research  # Restore original from package
   ```
   - Restores research report from tool installation package
   - Useful if custom/updated research causes issues

**Error Handling**:
- Network timeout (30s): Graceful failure, keep current version
- Invalid format: Detailed validation error, rollback to previous
- Permission denied: Error with instructions to check file permissions
- Partial download: Checksum validation, retry or abort

**Testing** (integration test):
- Mock HTTP server with valid/invalid research reports
- Verify atomic updates (no partial writes)
- Test rollback on validation failure
- Confirm offline graceful degradation
- Verify version detection and display

**Philosophy**: Research report updates are explicitly opt-in only via `--update-research`. Standard workflow uses stable bundled version. Users who want latest research or custom criteria can update or override as needed.
````

## File: specs/001-agentready-scorer/.remediation/U2-research-validation.md
````markdown
# Remediation U2: Research Report Validation Criteria

**Issue**: FR-024 requires validation but doesn't define the rules.

**Target File**: Create new file `specs/001-agentready-scorer/contracts/research-report-schema.md`

---

## Content to Add (New File)

# Research Report Validation Schema

**Purpose**: Define validation criteria for agent-ready-codebase-attributes.md (per FR-024)

## Required Structure

### 1. Metadata Header (REQUIRED)

```markdown
---
version: SEMVER          # e.g., "1.2.0"
date: YYYY-MM-DD         # ISO 8601 date
---
```

**Validation**:
- âœ… Version follows semantic versioning (MAJOR.MINOR.PATCH)
- âœ… Date is valid ISO 8601 format
- âš ï¸ Date is not in future (warning only)

### 2. Attribute Definitions (REQUIRED)

Must contain exactly 25 attribute sections with this structure:

```markdown
### [Number].[Subnumber] [Attribute Name]

**Definition:** [Clear definition]

**Why It Matters:** [Rationale]

**Impact on Agent Behavior:**
- [Impact point 1]
- [Impact point 2]

**Measurable Criteria:**
- [Criterion 1]
- [Criterion 2]

**Citations:**
- [Source 1]
- [Source 2]
```

**Validation**:
- âœ… Exactly 25 attributes present
- âœ… Attributes numbered sequentially (1.1, 1.2, ..., 15.1)
- âœ… Each attribute has "Definition" section (non-empty)
- âœ… Each attribute has "Measurable Criteria" section (non-empty)
- âœ… At least one citation per attribute
- âš ï¸ "Impact on Agent Behavior" section exists (warning if missing)

### 3. Tier Assignments (REQUIRED)

Research report must include tier classification section:

```markdown
## IMPLEMENTATION PRIORITIES

### Tier 1: Essential (Must-Have)
[List of attributes]

### Tier 2: Critical (Should-Have)
[List of attributes]

### Tier 3: Important (Nice-to-Have)
[List of attributes]

### Tier 4: Advanced (Optimization)
[List of attributes]
```

**Validation**:
- âœ… All 4 tiers defined
- âœ… All 25 attributes assigned to exactly one tier
- âœ… Each tier contains at least 1 attribute
- âš ï¸ Tier distribution roughly balanced (warning if one tier has >50% of attributes)

### 4. Category Coverage (REQUIRED)

Must include attributes across these categories:
1. Context Window Optimization
2. Documentation Standards
3. Code Quality Metrics
4. Repository Structure
5. Testing & CI/CD
6. Dependency Management
7. Git & Version Control
8. Build & Development Setup
9. Error Handling & Debugging
10. API & Interface Documentation
11. Modularity & Code Organization
12. CI/CD Integration
13. Security & Compliance

**Validation**:
- âœ… At least 10 of 13 categories present
- âš ï¸ Missing categories identified (warning only)

### 5. References Section (REQUIRED)

```markdown
## REFERENCES & CITATIONS

[Consolidated bibliography]
```

**Validation**:
- âœ… References section exists
- âœ… At least 20 unique citations (evidence-based design threshold)
- âš ï¸ URLs are reachable (check via HEAD request, warn only if unreachable)

## Validation Severity

**ERRORS** (block usage - report must be fixed):
- Missing metadata header
- Invalid version/date format
- Incorrect attribute count (not 25)
- Missing "Measurable Criteria" in any attribute
- Fewer than 4 tiers defined
- Attributes not assigned to tiers

**WARNINGS** (allow usage - non-critical issues):
- Missing "Impact on Agent Behavior" sections
- Fewer than 20 references
- Unreachable citation URLs
- Unbalanced tier distribution
- Missing category coverage

## Validation Implementation

```python
class ResearchReportValidator:
    def validate(self, content: str) -> ValidationResult:
        errors = []
        warnings = []

        # Parse metadata
        metadata = self._extract_metadata(content)
        if not metadata:
            errors.append("Missing metadata header with version and date")
        elif not self._is_valid_semver(metadata.get('version')):
            errors.append(f"Invalid version format: {metadata.get('version')}")

        # Count attributes
        attributes = self._extract_attributes(content)
        if len(attributes) != 25:
            errors.append(f"Expected 25 attributes, found {len(attributes)}")

        # Check measurable criteria
        for attr in attributes:
            if not attr.measurable_criteria:
                errors.append(f"Missing measurable criteria for {attr.name}")
            if not attr.impact_on_agent:
                warnings.append(f"Missing 'Impact on Agent Behavior' for {attr.name}")

        # Check tier assignments
        tiers = self._extract_tiers(content)
        if len(tiers) < 4:
            errors.append(f"Expected 4 tiers, found {len(tiers)}")

        # Check references
        refs = self._extract_references(content)
        if len(refs) < 20:
            warnings.append(f"Only {len(refs)} references (recommend 20+ for evidence-based design)")

        return ValidationResult(
            valid=len(errors) == 0,
            errors=errors,
            warnings=warnings
        )
```

## Example Validation Output

```
âœ“ Metadata header valid (v1.2.0, 2025-11-20)
âœ“ 25 attributes found
âœ“ All attributes have measurable criteria
âœ“ 4 tiers defined with all attributes assigned
âš  Attribute 1.1 missing 'Impact on Agent Behavior' section
âš  Attribute 7.3 missing 'Impact on Agent Behavior' section
âœ“ 52 references found

Validation: PASSED with 2 warnings

Warnings can be ignored - research report is usable.
```

## Usage in Tool

```python
from agentready.services.research_loader import ResearchLoader

loader = ResearchLoader()
result = loader.load_and_validate('agent-ready-codebase-attributes.md')

if not result.valid:
    print("Research report validation FAILED:")
    for error in result.errors:
        print(f"  âŒ {error}")
    sys.exit(1)

if result.warnings:
    print("Research report validation warnings:")
    for warning in result.warnings:
        print(f"  âš ï¸  {warning}")

# Proceed with valid research report
research = result.research
```
````

## File: specs/001-agentready-scorer/.remediation/U3-weight-validation.md
````markdown
# Remediation U3: Weight Validation Rules

**Issue**: FR-033 requires weight validation but doesn't define the rules.

**Target File**: `specs/001-agentready-scorer/data-model.md`

**Action**: Add subsection after the Weight Distribution section

---

## Content to Add

## Configuration Weight Validation

**Per FR-033**: Custom weights in `.agentready-config.yaml` must be validated.

### Validation Rules

#### Rule 1: Completeness
All 25 attributes must have weights (explicit or inherited):

```python
REQUIRED_ATTRIBUTES = [
    'claude_md_file', 'concise_documentation', 'file_size_limits',
    'readme_structure', 'inline_documentation', 'architecture_decisions',
    'cyclomatic_complexity', 'function_length', 'type_annotations',
    'code_smells', 'standard_layout', 'separation_concerns',
    'test_coverage', 'test_naming', 'precommit_hooks',
    'lock_files', 'dependency_freshness', 'conventional_commits',
    'gitignore_completeness', 'issue_pr_templates', 'one_command_setup',
    'dev_env_docs', 'container_setup', 'error_clarity',
    'structured_logging', 'openapi_specs', 'graphql_schemas',
    'dry_principle', 'naming_conventions', 'semantic_naming',
    'cicd_visibility', 'branch_protection', 'security_scanning',
    'secrets_management', 'performance_benchmarks'
]

def validate_completeness(config_weights, default_weights):
    missing = [
        attr for attr in REQUIRED_ATTRIBUTES
        if attr not in config_weights and attr not in default_weights
    ]
    if missing:
        raise ValidationError(f"Missing weights for: {', '.join(missing)}")
```

#### Rule 2: Positive Values
All weights must be positive numbers:

```python
def validate_positive(weights):
    invalid = {attr: w for attr, w in weights.items() if w <= 0}
    if invalid:
        raise ValidationError(f"Weights must be positive: {invalid}")
```

#### Rule 3: Sum Constraint
Weights must sum to 1.0 within floating-point tolerance:

```python
TOLERANCE = 0.001  # Allow 0.1% rounding error

def validate_sum(weights):
    total = sum(weights.values())
    if abs(total - 1.0) > TOLERANCE:
        raise ValidationError(
            f"Weights must sum to 1.0 (got {total:.4f}). "
            f"Difference: {total - 1.0:+.4f}"
        )
```

#### Rule 4: Reasonable Bounds
Individual weights should be between 0.5% and 20% (warnings only):

```python
MIN_WEIGHT = 0.005  # 0.5%
MAX_WEIGHT = 0.20   # 20%

def validate_bounds(weights):
    warnings = []
    for attr, weight in weights.items():
        if weight < MIN_WEIGHT:
            warnings.append(f"{attr}: {weight:.1%} is very low (< 0.5%)")
        if weight > MAX_WEIGHT:
            warnings.append(f"{attr}: {weight:.1%} is very high (> 20%)")
    return warnings  # Non-blocking warnings
```

### Partial Configuration Handling

Users can specify only overrides, inheriting defaults for others:

```yaml
# .agentready-config.yaml (partial configuration)
weights:
  claude_md_file: 0.15      # Override Tier 1 default (10%)
  test_coverage: 0.05       # Override Tier 2 default (3%)
  # Other 23 attributes: inherit rescaled tier defaults
```

**Rescaling Algorithm**:

```python
def merge_and_rescale(config_weights, tier_defaults):
    """
    Merge config overrides with tier defaults, then rescale to sum to 1.0.

    Args:
        config_weights: User-specified weights (partial or complete)
        tier_defaults: Default tier-based weights (complete, sums to 1.0)

    Returns:
        Final weights dictionary (complete, sums to 1.0)
    """
    # Start with tier defaults
    final_weights = tier_defaults.copy()

    # Override with config values
    final_weights.update(config_weights)

    # Rescale to sum to 1.0
    total = sum(final_weights.values())
    rescaled = {attr: w / total for attr, w in final_weights.items()}

    return rescaled

# Example:
# config = {'claude_md_file': 0.15, 'test_coverage': 0.05}
# defaults = {25 attributes summing to 1.0}
# After merge: sum = 1.07 (overrides increased total by 0.07)
# After rescale: all weights * (1.0 / 1.07) â‰ˆ 0.935x scaling factor
```

### Validation Command

```bash
agentready --validate-config [path/to/config.yaml]
```

**Output Example**:

```
Validating config: .agentready-config.yaml

âœ“ All 25 attributes have weights
âœ“ All weights are positive
âœ“ Weights sum to 1.0000 (within tolerance)
âš  claude_md_file: 15.0% is high (default: 10.0%, tier 1 average: 10.0%)
âš  test_coverage: 5.0% is high (default: 3.0%, tier 2 average: 3.0%)

Configuration: VALID with 2 warnings

Effective weights after rescaling:
  claude_md_file: 14.02% (config: 15.0%, rescaled down)
  test_coverage: 4.67% (config: 5.0%, rescaled down)
  conventional_commits: 2.80% (default: 3.0%, rescaled down)
  ... (22 more attributes)

Total: 100.00%
```

### Error Messages

**Invalid Sum**:
```
ERROR: Weights must sum to 1.0 (got 0.8500). Difference: -0.1500

Your weights total 85%. This suggests missing attributes or incorrect values.

Current distribution:
  Explicitly specified: 0.6500 (13 attributes)
  Using tier defaults: 0.2000 (12 attributes)

Suggestion: Either specify all 25 attributes explicitly, or let unspecified
attributes inherit tier defaults (which will be automatically rescaled).
```

**Negative Weight**:
```
ERROR: Weights must be positive: {'test_coverage': -0.05}

Negative weights are not allowed. Did you mean to reduce other weights instead?
```

**Missing Attributes**:
```
ERROR: Missing weights for: inline_documentation, architecture_decisions

All 25 attributes must have weights. Either:
1. Specify these attributes in your config, OR
2. Ensure they exist in default-weights.yaml

To see all required attributes:
  agentready --list-attributes
```

**Tolerance Exceeded**:
```
ERROR: Weights must sum to 1.0 (got 1.0025). Difference: +0.0025

Sum exceeds tolerance (Â±0.001). Likely due to rounding errors.

Suggestion: Reduce precision or adjust values. For example:
  claude_md_file: 0.100 (instead of 0.1003)
  test_coverage: 0.050 (instead of 0.0497)
```

### Validation Integration

- **On config load**: Automatic validation (fail fast on errors)
- **On manual validate**: Detailed report with warnings
- **On assessment run**: Validation errors block execution
- **On example generation**: `--generate-config` produces valid example with comments

### Configuration Precedence

**Precedence Order** (highest to lowest):

1. **CLI flags** - `agentready --weight claude_md_file=0.15`
2. **Config file** - `.agentready-config.yaml` in current directory
3. **Config file (alternate path)** - `agentready --config /path/to/config.yaml`
4. **Tier defaults** - Built-in `src/agentready/data/default-weights.yaml`

**Merging Rules**:
- CLI flags override config file weights
- Config file overrides tier defaults
- Unspecified attributes always inherit tier defaults
- Final weights always rescaled to sum to 1.0

**Example**:
```bash
# CLI flag takes precedence
agentready --config custom.yaml --weight claude_md_file=0.20

# Precedence:
# 1. claude_md_file = 0.20 (from CLI flag)
# 2. Other weights from custom.yaml (if present)
# 3. Remaining weights from tier defaults
# 4. All rescaled to sum to 1.0
```
````

## File: specs/001-agentready-scorer/checklists/requirements.md
````markdown
# Specification Quality Checklist: AgentReady Repository Scorer

**Purpose**: Validate specification completeness and quality before proceeding to planning
**Created**: 2025-11-20
**Feature**: [spec.md](../spec.md)

## Content Quality

- [x] No implementation details (languages, frameworks, APIs)
- [x] Focused on user value and business needs
- [x] Written for non-technical stakeholders
- [x] All mandatory sections completed

## Requirement Completeness

- [x] No [NEEDS CLARIFICATION] markers remain
- [x] Requirements are testable and unambiguous
- [x] Success criteria are measurable
- [x] Success criteria are technology-agnostic (no implementation details)
- [x] All acceptance scenarios are defined
- [x] Edge cases are identified
- [x] Scope is clearly bounded
- [x] Dependencies and assumptions identified

## Feature Readiness

- [x] All functional requirements have clear acceptance criteria
- [x] User scenarios cover primary flows
- [x] Feature meets measurable outcomes defined in Success Criteria
- [x] No implementation details leak into specification

## Validation Results

### Content Quality - PASSED

âœ“ **No implementation details**: Specification focuses on "what" not "how". Tool name and CLI interface are requirements, not implementation details.
âœ“ **User value focused**: All user stories describe value delivery (assessment, interactive reports, remediation guidance).
âœ“ **Non-technical language**: Uses domain terms (repository, assessment, attribute) accessible to stakeholders.
âœ“ **Complete sections**: User Scenarios, Requirements, Success Criteria all fully populated.

### Requirement Completeness - PASSED

âœ“ **No clarification markers**: All requirements are specific and concrete, no [NEEDS CLARIFICATION] markers present.
âœ“ **Testable requirements**: Each FR can be verified (e.g., FR-003 "evaluate all 25 attributes" is verifiable by counting evaluated attributes).
âœ“ **Measurable success criteria**: All SC include specific metrics (e.g., SC-001 "<5 minutes for <10k files", SC-002 ">95% accuracy").
âœ“ **Technology-agnostic criteria**: Success criteria describe outcomes without specifying technologies (e.g., SC-008 focuses on user action time, not specific UI framework).
âœ“ **Complete acceptance scenarios**: All user stories include Given/When/Then scenarios covering happy paths.
âœ“ **Edge cases identified**: 8 edge cases documented covering error conditions, scale, and special repository types.
âœ“ **Clear scope**: Feature bounded to assessment and reporting, not extending to automated fixes or CI/CD integration.
âœ“ **Assumptions documented**: 8 assumptions listed covering access, permissions, audience, and operational constraints.

### Feature Readiness - PASSED

âœ“ **Clear acceptance criteria**: Each FR maps to acceptance scenarios in user stories (e.g., FR-005 dual-format reports â†’ US1 acceptance scenario 5).
âœ“ **Primary flows covered**: 4 user stories progress from core value (scoring) through enhanced features (interactive HTML, remediation, version control).
âœ“ **Measurable outcomes**: 10 success criteria provide quantitative and qualitative validation points.
âœ“ **No implementation leakage**: No mention of specific frameworks, libraries, or technical architecture.

## Notes

**Validation Status**: ALL ITEMS PASSED âœ“

Specification is ready for `/speckit.plan` command. No updates required.

**Strengths**:
- Comprehensive coverage of 25 attributes from research report
- Clear priority ordering (P1-P4) enabling MVP-first approach
- Well-defined success criteria with specific thresholds
- Extensive edge case consideration

**Ready for next phase**: Planning can proceed to define technical approach, architecture, and implementation strategy.
````

## File: specs/001-agentready-scorer/contracts/assessment-schema.json
````json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "AgentReady Assessment",
  "description": "Complete repository assessment result with findings and scores",
  "type": "object",
  "required": [
    "repository",
    "timestamp",
    "overall_score",
    "certification_level",
    "attributes_assessed",
    "attributes_skipped",
    "attributes_total",
    "findings",
    "duration_seconds"
  ],
  "properties": {
    "repository": {
      "type": "object",
      "required": ["name", "path", "languages", "total_files"],
      "properties": {
        "name": {"type": "string"},
        "path": {"type": "string"},
        "url": {"type": ["string", "null"]},
        "branch": {"type": "string"},
        "commit_hash": {"type": "string", "pattern": "^[a-f0-9]{40}$"},
        "languages": {
          "type": "object",
          "additionalProperties": {"type": "integer", "minimum": 0}
        },
        "total_files": {"type": "integer", "minimum": 0},
        "total_lines": {"type": "integer", "minimum": 0}
      }
    },
    "timestamp": {
      "type": "string",
      "format": "date-time",
      "description": "ISO 8601 timestamp"
    },
    "overall_score": {
      "type": "number",
      "minimum": 0,
      "maximum": 100
    },
    "certification_level": {
      "type": "string",
      "enum": ["Platinum", "Gold", "Silver", "Bronze", "Needs Improvement"]
    },
    "attributes_assessed": {
      "type": "integer",
      "minimum": 0,
      "maximum": 25
    },
    "attributes_skipped": {
      "type": "integer",
      "minimum": 0,
      "maximum": 25
    },
    "attributes_total": {
      "type": "integer",
      "const": 25
    },
    "findings": {
      "type": "array",
      "minItems": 25,
      "maxItems": 25,
      "items": {
        "type": "object",
        "required": ["attribute", "status"],
        "properties": {
          "attribute": {
            "type": "object",
            "required": ["id", "name", "category", "tier"],
            "properties": {
              "id": {"type": "string"},
              "name": {"type": "string"},
              "category": {"type": "string"},
              "tier": {"type": "integer", "minimum": 1, "maximum": 4},
              "description": {"type": "string"},
              "criteria": {"type": "string"},
              "default_weight": {"type": "number", "minimum": 0, "maximum": 1}
            }
          },
          "status": {
            "type": "string",
            "enum": ["pass", "fail", "skipped", "error", "not_applicable"]
          },
          "score": {
            "type": ["number", "null"],
            "minimum": 0,
            "maximum": 100
          },
          "measured_value": {"type": ["string", "null"]},
          "threshold": {"type": ["string", "null"]},
          "evidence": {
            "type": "array",
            "items": {"type": "string"}
          },
          "remediation": {
            "type": ["object", "null"],
            "properties": {
              "summary": {"type": "string"},
              "steps": {
                "type": "array",
                "items": {"type": "string"}
              },
              "tools": {
                "type": "array",
                "items": {"type": "string"}
              },
              "commands": {
                "type": "array",
                "items": {"type": "string"}
              },
              "examples": {
                "type": "array",
                "items": {"type": "string"}
              },
              "citations": {
                "type": "array",
                "items": {
                  "type": "object",
                  "required": ["source", "title"],
                  "properties": {
                    "source": {"type": "string"},
                    "title": {"type": "string"},
                    "url": {"type": ["string", "null"], "format": "uri"},
                    "relevance": {"type": "string"}
                  }
                }
              }
            }
          },
          "error_message": {"type": ["string", "null"]}
        }
      }
    },
    "config": {
      "type": ["object", "null"],
      "properties": {
        "weights": {
          "type": "object",
          "additionalProperties": {"type": "number", "minimum": 0, "maximum": 1}
        },
        "excluded_attributes": {
          "type": "array",
          "items": {"type": "string"}
        },
        "output_dir": {"type": ["string", "null"]}
      }
    },
    "duration_seconds": {
      "type": "number",
      "minimum": 0
    }
  }
}
````

## File: specs/001-agentready-scorer/contracts/report-html-schema.md
````markdown
# HTML Report Structure Specification

**Purpose**: Define the structure, interactivity, and requirements for self-contained HTML reports generated by agentready

**Format**: Single HTML file with embedded CSS and JavaScript (no external dependencies)

**Filename**: `report-{timestamp}.html` (e.g., `report-2025-11-20T14-30-00.html`)

## Document Structure

### Overall HTML Skeleton

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AgentReady Assessment Report - {repository_name}</title>
    <style>
        /* ALL CSS EMBEDDED HERE (no external stylesheets) */
    </style>
</head>
<body>
    <div id="app">
        <!-- Report content here -->
    </div>

    <script>
        /* ALL JAVASCRIPT EMBEDDED HERE (no external scripts) */
        const ASSESSMENT_DATA = {JSON_DATA_HERE};
    </script>
</body>
</html>
```

## Required Sections

### 1. Header (Fixed Position)

```html
<header class="report-header">
    <div class="container">
        <h1>AgentReady Assessment Report</h1>
        <div class="repo-info">
            <span class="repo-name">{repository_name}</span>
            <span class="repo-path">{repository_path}</span>
        </div>
        <div class="score-display">
            <div class="score-circle" data-score="{overall_score}">
                <span class="score-number">{overall_score}</span>
                <span class="score-label">/100</span>
            </div>
            <div class="certification-badge {certification_class}">
                <span class="cert-emoji">{cert_emoji}</span>
                <span class="cert-name">{certification_level}</span>
            </div>
        </div>
        <div class="metadata">
            <span>ðŸ“… {timestamp}</span>
            <span>âœ… {assessed} assessed</span>
            <span>â­ï¸ {skipped} skipped</span>
            <span>â±ï¸ {duration}s</span>
        </div>
    </div>
</header>
```

**Requirements**:
- Fixed position header (stays visible on scroll)
- Circular score display with dynamic color (red <40, yellow 40-74, green 75-100)
- Certification badge with appropriate emoji
- Metadata row with assessment stats

---

### 2. Controls/Filters (Sticky Below Header)

```html
<div class="controls-bar">
    <div class="container">
        <div class="filters">
            <button class="filter-btn active" data-filter="all">
                All ({total_attributes})
            </button>
            <button class="filter-btn" data-filter="pass">
                âœ… Passing ({pass_count})
            </button>
            <button class="filter-btn" data-filter="fail">
                âŒ Failing ({fail_count})
            </button>
            <button class="filter-btn" data-filter="skipped">
                â­ï¸ Skipped ({skipped_count})
            </button>
        </div>
        <div class="sort-controls">
            <label>Sort by:</label>
            <select id="sort-selector">
                <option value="category">Category</option>
                <option value="score-asc">Score (Low to High)</option>
                <option value="score-desc">Score (High to Low)</option>
                <option value="tier">Tier (Priority)</option>
            </select>
        </div>
        <div class="search">
            <input type="text" id="search-box" placeholder="Search attributes...">
        </div>
    </div>
</div>
```

**Requirements**:
- Sticky positioning (fixed below header)
- Filter buttons toggle attribute visibility
- Active filter highlighted
- Sort dropdown changes attribute display order
- Search box filters attributes by name/description (case-insensitive)
- Count badges update dynamically

---

### 3. Summary Dashboard

```html
<section class="summary-dashboard">
    <div class="container">
        <h2>Summary</h2>
        <div class="category-grid">
            <!-- One card per category -->
            <div class="category-card" data-category="{category_id}">
                <h3 class="category-name">{category_name}</h3>
                <div class="category-score">
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {score}%"></div>
                    </div>
                    <span class="score-text">{score}%</span>
                </div>
                <div class="category-stats">
                    <span>{assessed}/{total} attributes</span>
                    <span class="status-badge {status_class}">{status_text}</span>
                </div>
            </div>
        </div>
    </div>
</section>
```

**Requirements**:
- Grid layout (responsive: 1 column mobile, 2 tablet, 3 desktop)
- Each category card shows score, progress bar, attribute count
- Click category card to scroll to detailed findings
- Status badges color-coded (green/yellow/red)

---

### 4. Detailed Findings (Collapsible Sections)

```html
<section class="detailed-findings">
    <div class="container">
        <h2>Detailed Findings</h2>

        <!-- Category Section -->
        <div class="category-section" data-category="{category_id}">
            <h3 class="category-header">
                <span class="category-number">{number}</span>
                <span class="category-title">{category_name}</span>
                <span class="category-badge">{score}%</span>
            </h3>

            <!-- Attribute Findings -->
            <div class="attribute-findings">
                <details class="finding-card" data-status="{status}" data-tier="{tier}" data-attribute-id="{id}">
                    <summary class="finding-summary">
                        <div class="summary-left">
                            <span class="status-icon">{status_emoji}</span>
                            <span class="attribute-id">{number}</span>
                            <span class="attribute-name">{name}</span>
                            <span class="tier-badge">Tier {tier}</span>
                        </div>
                        <div class="summary-right">
                            <span class="score-badge">{score}/100</span>
                            <span class="expand-icon">â–¼</span>
                        </div>
                    </summary>

                    <div class="finding-details">
                        <div class="measurement">
                            <strong>Measured:</strong> {measured_value}
                            <br>
                            <strong>Threshold:</strong> {threshold}
                        </div>

                        <div class="evidence">
                            <strong>Evidence:</strong>
                            <ul>
                                <!-- Evidence items with icons -->
                                <li><span class="evidence-icon">âœ…</span> {evidence_item}</li>
                            </ul>
                        </div>

                        <!-- Remediation (only for failing attributes) -->
                        <div class="remediation">
                            <h4>ðŸ”§ Remediation Steps</h4>
                            <p class="remediation-summary">{summary}</p>
                            <ol class="remediation-steps">
                                <li>{step}</li>
                            </ol>
                            <div class="remediation-tools">
                                <strong>Tools:</strong> {tools_list}
                            </div>
                            <div class="remediation-commands">
                                <strong>Commands:</strong>
                                <pre><code>{commands}</code></pre>
                            </div>
                            <div class="citations">
                                <strong>References:</strong>
                                <ul>
                                    <li><a href="{url}" target="_blank">{source}: {title}</a></li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
        </div>
    </div>
</section>
```

**Requirements**:
- `<details>` / `<summary>` for native collapsible behavior
- Attribute cards sortable and filterable via JavaScript
- Status icons: âœ… pass, âŒ fail, â­ï¸ skipped, âš ï¸ error, â„¹ï¸ not_applicable
- Tier badges color-coded (1=red, 2=orange, 3=yellow, 4=green)
- Remediation section only shown for failing attributes
- Code blocks with syntax highlighting (CSS-based, not JS library)
- External links open in new tab (`target="_blank"`)

---

### 5. Certification Explanation

```html
<section class="certification-info">
    <div class="container">
        <h2>Certification Level: <span class="cert-name">{certification_level}</span></h2>
        <p class="cert-description">{description}</p>

        <div class="score-ladder">
            <div class="score-tier" data-tier="platinum">
                <span class="tier-emoji">ðŸ¥‡</span>
                <strong>Platinum</strong>
                <span class="tier-range">90-100</span>
                <span class="tier-desc">Exemplary</span>
            </div>
            <div class="score-tier" data-tier="gold">
                <span class="tier-emoji">ðŸ¥‡</span>
                <strong>Gold</strong>
                <span class="tier-range">75-89</span>
                <span class="tier-desc">Highly Optimized</span>
            </div>
            <div class="score-tier active" data-tier="silver">
                <span class="tier-emoji">ðŸ¥ˆ</span>
                <strong>Silver</strong>
                <span class="tier-range">60-74</span>
                <span class="tier-desc">Well-Suited (YOUR LEVEL)</span>
            </div>
            <div class="score-tier" data-tier="bronze">
                <span class="tier-emoji">ðŸ¥‰</span>
                <strong>Bronze</strong>
                <span class="tier-range">40-59</span>
                <span class="tier-desc">Basic Compatibility</span>
            </div>
            <div class="score-tier" data-tier="needs-improvement">
                <span class="tier-emoji">â›”</span>
                <strong>Needs Improvement</strong>
                <span class="tier-range">0-39</span>
                <span class="tier-desc">Significant Work Needed</span>
            </div>
        </div>

        <div class="next-steps">
            <h3>Next Steps to {next_tier}</h3>
            <ul class="improvement-list">
                <li>
                    <span class="improvement-points">+{points}</span>
                    <span class="improvement-desc">{description}</span>
                    <a href="#{attribute_id}" class="jump-link">View Details â†’</a>
                </li>
            </ul>
        </div>
    </div>
</section>
```

**Requirements**:
- Visual score ladder with current level highlighted
- Next steps show top 3-5 improvements ranked by point potential
- Jump links scroll to specific attribute findings
- Active tier has distinct styling (border, background)

---

### 6. Footer

```html
<footer class="report-footer">
    <div class="container">
        <div class="footer-content">
            <div class="footer-section">
                <h4>Assessment Metadata</h4>
                <p><strong>Generated by:</strong> agentready v{version}</p>
                <p><strong>Research Report:</strong> v{research_version} ({research_date})</p>
                <p><strong>Duration:</strong> {minutes}m {seconds}s</p>
            </div>
            <div class="footer-section">
                <h4>Repository Snapshot</h4>
                <p><strong>Branch:</strong> {branch}</p>
                <p><strong>Commit:</strong> {commit_hash_short}</p>
                <p><strong>Languages:</strong> {languages_list}</p>
                <p><strong>Files:</strong> {total_files} | <strong>Lines:</strong> {total_lines}</p>
            </div>
            <div class="footer-section">
                <h4>Research Foundation</h4>
                <p>Based on evidence from 50+ authoritative sources including Anthropic, Microsoft, Google, ArXiv, and IEEE/ACM.</p>
                <p><a href="https://github.com/your-org/agentready" target="_blank">AgentReady Project</a></p>
            </div>
        </div>
        <div class="footer-attribution">
            <p><em>AgentReady - Optimizing codebases for AI-assisted development</em></p>
        </div>
    </div>
</footer>
```

---

## JavaScript Functionality

### Required Interactive Features

1. **Filtering**
```javascript
// Filter attribute cards by status
function filterByStatus(status) {
    const cards = document.querySelectorAll('.finding-card');
    cards.forEach(card => {
        if (status === 'all' || card.dataset.status === status) {
            card.style.display = 'block';
        } else {
            card.style.display = 'none';
        }
    });
    updateFilterCounts();
}
```

2. **Sorting**
```javascript
// Sort attributes by criteria
function sortAttributes(criteria) {
    const container = document.querySelector('.attribute-findings');
    const cards = Array.from(container.querySelectorAll('.finding-card'));

    cards.sort((a, b) => {
        if (criteria === 'score-asc') {
            return getScore(a) - getScore(b);
        } else if (criteria === 'score-desc') {
            return getScore(b) - getScore(a);
        } else if (criteria === 'tier') {
            return parseInt(a.dataset.tier) - parseInt(b.dataset.tier);
        }
        // Default: category order (no change)
        return 0;
    });

    cards.forEach(card => container.appendChild(card));
}
```

3. **Search**
```javascript
// Search attributes by text
function searchAttributes(query) {
    const lowercaseQuery = query.toLowerCase();
    const cards = document.querySelectorAll('.finding-card');

    cards.forEach(card => {
        const name = card.querySelector('.attribute-name').textContent.toLowerCase();
        const desc = card.querySelector('.finding-details').textContent.toLowerCase();

        if (name.includes(lowercaseQuery) || desc.includes(lowercaseQuery)) {
            card.style.display = 'block';
        } else {
            card.style.display = 'none';
        }
    });
}
```

4. **Smooth Scrolling**
```javascript
// Smooth scroll to category section
function scrollToCategory(categoryId) {
    const section = document.querySelector(`[data-category="${categoryId}"]`);
    section.scrollIntoView({ behavior: 'smooth', block: 'start' });
}
```

5. **Dynamic Progress Circles**
```javascript
// Animate score circle on page load
function animateScoreCircle(element, targetScore) {
    let currentScore = 0;
    const duration = 1000; // 1 second
    const increment = targetScore / (duration / 16); // 60 FPS

    const interval = setInterval(() => {
        currentScore += increment;
        if (currentScore >= targetScore) {
            currentScore = targetScore;
            clearInterval(interval);
        }
        updateCircle(element, currentScore);
    }, 16);
}
```

---

## CSS Requirements

### Responsive Breakpoints
```css
/* Mobile-first approach */
@media (min-width: 768px) { /* Tablet */ }
@media (min-width: 1024px) { /* Desktop */ }
```

### Color Scheme
```css
:root {
    /* Certification colors */
    --platinum: #E5E4E2;
    --gold: #FFD700;
    --silver: #C0C0C0;
    --bronze: #CD7F32;
    --needs-improvement: #D32F2F;

    /* Status colors */
    --pass: #4CAF50;
    --fail: #F44336;
    --warning: #FF9800;
    --info: #2196F3;
    --skipped: #9E9E9E;

    /* Tier colors */
    --tier-1: #D32F2F; /* Red - Essential */
    --tier-2: #FF6F00; /* Orange - Critical */
    --tier-3: #FBC02D; /* Yellow - Important */
    --tier-4: #388E3C; /* Green - Advanced */

    /* UI colors */
    --background: #FAFAFA;
    --surface: #FFFFFF;
    --text-primary: #212121;
    --text-secondary: #757575;
    --border: #E0E0E0;
}
```

### Typography
```css
body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
    font-size: 16px;
    line-height: 1.6;
}

code, pre {
    font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', 'Consolas', monospace;
}
```

---

## File Size Requirements

- **Maximum**: 5MB (including embedded CSS/JS)
- **Target**: <2MB for typical reports
- **CSS Minification**: Required
- **JS Minification**: Required
- **Image Embedding**: None (use Unicode emoji only)

---

## Validation Checklist

- [ ] Single HTML file with no external dependencies
- [ ] All CSS embedded in `<style>` tag
- [ ] All JavaScript embedded in `<script>` tag
- [ ] Assessment data embedded as JSON constant
- [ ] Works offline (no CDN, no external resources)
- [ ] Responsive design (mobile, tablet, desktop)
- [ ] All interactive features functional (filter, sort, search)
- [ ] Smooth scroll behavior implemented
- [ ] Collapsible sections work (native `<details>` element)
- [ ] Progress bars animated on page load
- [ ] Score circle updates dynamically
- [ ] Color-coded status indicators throughout
- [ ] All links to external sites have `target="_blank"`
- [ ] Cross-browser compatible (Chrome, Firefox, Safari, Edge)
- [ ] Accessible (keyboard navigation, ARIA labels)
- [ ] Print-friendly (CSS print stylesheet)
````

## File: specs/001-agentready-scorer/contracts/report-markdown-schema.md
````markdown
# Markdown Report Structure Specification

**Purpose**: Define the structure and formatting requirements for Markdown reports generated by agentready

**Format**: GitHub-Flavored Markdown (GFM)

**Filename**: `report-{timestamp}.md` (e.g., `report-2025-11-20T14-30-00.md`)

## Document Structure

### 1. Header Section (REQUIRED)

```markdown
# AgentReady Assessment Report

**Repository**: {repository_name}
**Path**: {repository_path}
**Date**: {iso_8601_timestamp}
**Overall Score**: {score}/100 ({certification_level})
**Attributes Assessed**: {assessed}/{total} ({skipped} skipped)
```

**Requirements**:
- Level 1 heading (`#`) for title
- Bold labels (`**Label**:`) for metadata
- ISO 8601 timestamp format
- Certification level in parentheses

---

### 2. Summary Table (REQUIRED)

```markdown
## Summary

| Category | Attributes | Score | Status |
|----------|-----------|-------|--------|
| Context Window Optimization | 2/2 | 85% | âœ… Good |
| Documentation Standards | 3/4 | 72% | âš ï¸ Needs Improvement |
| Code Quality Metrics | 4/4 | 93% | âœ… Excellent |
| Repository Structure | 2/2 | 80% | âœ… Good |
| Testing & CI/CD | 3/3 | 67% | âš ï¸ Needs Improvement |
| Dependency Management | 2/2 | 50% | âŒ Poor |
| Git & Version Control | 3/3 | 88% | âœ… Good |
| Build & Development | 2/2 | 75% | âœ… Good |
| Error Handling | 1/2 | 60% | âš ï¸ Needs Improvement |
| API Documentation | 0/2 | 0% | âŒ Not Applicable |
| Modularity | 2/3 | 70% | âš ï¸ Needs Improvement |
| CI/CD Integration | 2/2 | 90% | âœ… Excellent |
| Security | 1/2 | 40% | âŒ Poor |
| **Overall** | **23/25** | **72%** | **ðŸ¥ˆ Silver** |
```

**Requirements**:
- Level 2 heading (`##`) for "Summary"
- Table with 4 columns: Category, Attributes, Score, Status
- Attributes column shows assessed/total format
- Score column shows percentage
- Status uses emoji indicators:
  - âœ… Good (70-89%)
  - âš ï¸ Needs Improvement (40-69%)
  - âŒ Poor (0-39%)
  - âœ… Excellent (90-100%)
  - âŒ Not Applicable (all attributes skipped/N/A)
- Overall row in bold
- Certification emoji: ðŸ¥‡ Platinum, ðŸ¥‡ Gold, ðŸ¥ˆ Silver, ðŸ¥‰ Bronze, â›” Needs Improvement

---

### 3. Detailed Findings (REQUIRED)

For each category, include subsection with all attributes:

```markdown
## Detailed Findings

### 1. Context Window Optimization

#### 1.1 CLAUDE.md Configuration Files âœ…

**Tier**: 1 (Essential)
**Score**: 100/100
**Status**: Pass

**Measured**: File exists (487 lines)
**Threshold**: <1000 lines with required sections

**Evidence**:
- âœ… File exists: `CLAUDE.md`
- âœ… File size: 487 lines (under 1000 line limit)
- âœ… Contains tech stack section
- âœ… Contains standard commands section

<details>
<summary>ðŸ”§ Good Practices Found</summary>

- Concise format optimized for Claude context window
- Clear repository structure documentation
- Explicit security/compliance notes included
</details>

---

#### 1.2 Concise Structured Documentation âš ï¸

**Tier**: 1 (Essential)
**Score**: 60/100
**Status**: Needs Improvement

**Measured**: README 1247 lines
**Threshold**: <500 lines

**Evidence**:
- âš ï¸ README exists but exceeds recommended size: `README.md` (1247 lines)
- âœ… Uses standard Markdown headings
- âš ï¸ Missing table of contents for >100 line document

<details>
<summary>ðŸ”§ Remediation Steps</summary>

**Summary**: Split README into smaller, focused documents

**Steps**:
1. Identify sections that could be separate documents (API docs, architecture, deployment)
2. Create `docs/` directory for detailed documentation
3. Keep README focused on quickstart and overview
4. Add links to detailed docs from README
5. Add table of contents to README

**Tools Needed**:
- None (manual refactoring)

**Commands**:
```bash
mkdir -p docs/
# Move detailed sections to docs/api.md, docs/architecture.md, etc.
```

**References**:
- [Real Python: Documentation Best Practices](https://realpython.com/documenting-python-code/)
- [GitHub README Guide](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-readmes)

</details>

---
```

**Requirements for Each Attribute**:
- Level 4 heading (`####`) with attribute number, name, and status emoji
- Bold labels for Tier, Score, Status, Measured, Threshold
- Tier number with description (e.g., "1 (Essential)")
- Status: "Pass", "Fail", "Needs Improvement", "Skipped", "Not Applicable"
- Evidence as bulleted list with emoji indicators
- Remediation in `<details>` block (collapsible)
- Remediation includes: Summary, Steps (ordered list), Tools, Commands (code block), References (links)
- Horizontal rule (`---`) between attributes

---

### 4. Certification Level Details (REQUIRED)

```markdown
## Certification Level: Silver (72/100)

Your repository has achieved **Silver** certification. This indicates a solid foundation for agent-ready development, with room for targeted improvements.

**Score Breakdown**:
- **Platinum** (90-100): Exemplary agent-ready codebase
- **Gold** (75-89): Highly optimized for AI-assisted development
- **Silver** (60-74): Well-suited for agent development (YOUR LEVEL)
- **Bronze** (40-59): Basic agent compatibility
- **Needs Improvement** (0-39): Significant optimization needed

**Next Steps to Gold** (need +3 points):
1. Fix README size (1.2 Concise Structured Documentation) - potential +10 points
2. Add pre-commit hooks (5.3 Pre-commit Hooks & CI/CD) - potential +8 points
3. Improve dependency freshness (6.2 Dependency Freshness) - potential +5 points
```

**Requirements**:
- Level 2 heading with certification name and score
- Paragraph explaining certification meaning
- Score breakdown showing all certification levels
- "YOUR LEVEL" marker on current level
- "Next Steps" section with actionable improvements (3-5 items)
- Each improvement shows attribute reference and potential point gain

---

### 5. Footer (REQUIRED)

```markdown
---

## Assessment Metadata

**Generated by**: agentready v1.0.0
**Research Report Version**: 1.0.0 (2025-11-20)
**Assessment Duration**: 2 minutes 7 seconds
**Research Citations**: 50+ authoritative sources (Anthropic, Microsoft, Google, ArXiv, IEEE/ACM)

**Repository Snapshot**:
- Branch: main
- Commit: abc123def456789
- Languages: Python (42 files), Markdown (12 files)
- Total Files: 54
- Total Lines: 8,432

---

*This report generated by AgentReady - optimizing codebases for AI-assisted development.*
*Based on evidence from 50+ authoritative sources including Anthropic, Microsoft, Google, ArXiv, and IEEE/ACM.*
```

**Requirements**:
- Horizontal rule before metadata section
- Level 2 heading for "Assessment Metadata"
- Bold labels for all metadata fields
- Repository snapshot with key statistics
- Footer in italics with attribution
- Research report version tracking

---

## Formatting Standards

### Emoji Usage
- âœ… Pass / Good / Yes
- âŒ Fail / Poor / No
- âš ï¸ Warning / Needs Improvement
- â„¹ï¸ Info / Note
- ðŸ”§ Remediation / Fix
- ðŸ¥‡ Platinum
- ðŸ¥‡ Gold
- ðŸ¥ˆ Silver
- ðŸ¥‰ Bronze
- â›” Needs Improvement

### Code Blocks
- Use triple backticks with language tag: ` ```bash `, ` ```python `, etc.
- One command per line in bash blocks
- Include comments for complex commands

### Links
- External links: `[Text](URL)`
- Internal anchors: `[Text](#section-id)` (for table of contents)
- All citation URLs must be absolute, not relative

### Tables
- Use pipe-separated format with header separator (`|---|---|`)
- Align columns for readability in source
- Keep tables simple (max 5 columns)

---

## File Size Limits

- **Maximum**: 1MB (markdown text)
- **Target**: <500KB for typical reports
- **Line Length**: No hard limit, but aim for <120 characters for readability

---

## Validation Checklist

- [ ] Header section includes all required fields
- [ ] Summary table has all categories and overall row
- [ ] Detailed findings include all 25 attributes (or clearly marked as skipped)
- [ ] Each attribute has tier, score, status, evidence
- [ ] Failing attributes have remediation steps
- [ ] Certification level section explains current level and next steps
- [ ] Footer includes assessment metadata and repository snapshot
- [ ] All emoji used correctly and consistently
- [ ] All code blocks have language tags
- [ ] All links are absolute URLs (no relative paths)
- [ ] Document renders correctly on GitHub/GitLab/Bitbucket
- [ ] No HTML tags except `<details>` and `<summary>`
````

## File: specs/001-agentready-scorer/contracts/research-report-schema.md
````markdown
# Research Report Validation Schema

**Purpose**: Define validation criteria for agent-ready-codebase-attributes.md (per FR-024)

## Required Structure

### 1. Metadata Header (REQUIRED)

```markdown
---
version: SEMVER          # e.g., "1.2.0"
date: YYYY-MM-DD         # ISO 8601 date
---
```

**Validation**:
- âœ… Version follows semantic versioning (MAJOR.MINOR.PATCH)
- âœ… Date is valid ISO 8601 format
- âš ï¸ Date is not in future (warning only)

### 2. Attribute Definitions (REQUIRED)

Must contain exactly 25 attribute sections with this structure:

```markdown
### [Number].[Subnumber] [Attribute Name]

**Definition:** [Clear definition]

**Why It Matters:** [Rationale]

**Impact on Agent Behavior:**
- [Impact point 1]
- [Impact point 2]

**Measurable Criteria:**
- [Criterion 1]
- [Criterion 2]

**Citations:**
- [Source 1]
- [Source 2]
```

**Validation**:
- âœ… Exactly 25 attributes present
- âœ… Attributes numbered sequentially (1.1, 1.2, ..., 15.1)
- âœ… Each attribute has "Definition" section (non-empty)
- âœ… Each attribute has "Measurable Criteria" section (non-empty)
- âœ… At least one citation per attribute
- âš ï¸ "Impact on Agent Behavior" section exists (warning if missing)

### 3. Tier Assignments (REQUIRED)

Research report must include tier classification section:

```markdown
## IMPLEMENTATION PRIORITIES

### Tier 1: Essential (Must-Have)
[List of attributes]

### Tier 2: Critical (Should-Have)
[List of attributes]

### Tier 3: Important (Nice-to-Have)
[List of attributes]

### Tier 4: Advanced (Optimization)
[List of attributes]
```

**Validation**:
- âœ… All 4 tiers defined
- âœ… All 25 attributes assigned to exactly one tier
- âœ… Each tier contains at least 1 attribute
- âš ï¸ Tier distribution roughly balanced (warning if one tier has >50% of attributes)

### 4. Category Coverage (REQUIRED)

Must include attributes across these categories:
1. Context Window Optimization
2. Documentation Standards
3. Code Quality Metrics
4. Repository Structure
5. Testing & CI/CD
6. Dependency Management
7. Git & Version Control
8. Build & Development Setup
9. Error Handling & Debugging
10. API & Interface Documentation
11. Modularity & Code Organization
12. CI/CD Integration
13. Security & Compliance

**Validation**:
- âœ… At least 10 of 13 categories present
- âš ï¸ Missing categories identified (warning only)

### 5. References Section (REQUIRED)

```markdown
## REFERENCES & CITATIONS

[Consolidated bibliography]
```

**Validation**:
- âœ… References section exists
- âœ… At least 20 unique citations (evidence-based design threshold)
- âš ï¸ URLs are reachable (check via HEAD request, warn only if unreachable)

## Validation Severity

**ERRORS** (block usage - report must be fixed):
- Missing metadata header
- Invalid version/date format
- Incorrect attribute count (not 25)
- Missing "Measurable Criteria" in any attribute
- Fewer than 4 tiers defined
- Attributes not assigned to tiers

**WARNINGS** (allow usage - non-critical issues):
- Missing "Impact on Agent Behavior" sections
- Fewer than 20 references
- Unreachable citation URLs
- Unbalanced tier distribution
- Missing category coverage

## Validation Implementation

```python
class ResearchReportValidator:
    def validate(self, content: str) -> ValidationResult:
        errors = []
        warnings = []

        # Parse metadata
        metadata = self._extract_metadata(content)
        if not metadata:
            errors.append("Missing metadata header with version and date")
        elif not self._is_valid_semver(metadata.get('version')):
            errors.append(f"Invalid version format: {metadata.get('version')}")

        # Count attributes
        attributes = self._extract_attributes(content)
        if len(attributes) != 25:
            errors.append(f"Expected 25 attributes, found {len(attributes)}")

        # Check measurable criteria
        for attr in attributes:
            if not attr.measurable_criteria:
                errors.append(f"Missing measurable criteria for {attr.name}")
            if not attr.impact_on_agent:
                warnings.append(f"Missing 'Impact on Agent Behavior' for {attr.name}")

        # Check tier assignments
        tiers = self._extract_tiers(content)
        if len(tiers) < 4:
            errors.append(f"Expected 4 tiers, found {len(tiers)}")

        # Check references
        refs = self._extract_references(content)
        if len(refs) < 20:
            warnings.append(f"Only {len(refs)} references (recommend 20+ for evidence-based design)")

        return ValidationResult(
            valid=len(errors) == 0,
            errors=errors,
            warnings=warnings
        )
```

## Example Validation Output

```
âœ“ Metadata header valid (v1.2.0, 2025-11-20)
âœ“ 25 attributes found
âœ“ All attributes have measurable criteria
âœ“ 4 tiers defined with all attributes assigned
âš  Attribute 1.1 missing 'Impact on Agent Behavior' section
âš  Attribute 7.3 missing 'Impact on Agent Behavior' section
âœ“ 52 references found

Validation: PASSED with 2 warnings

Warnings can be ignored - research report is usable.
```

## Usage in Tool

```python
from agentready.services.research_loader import ResearchLoader

loader = ResearchLoader()
result = loader.load_and_validate('agent-ready-codebase-attributes.md')

if not result.valid:
    print("Research report validation FAILED:")
    for error in result.errors:
        print(f"  âŒ {error}")
    sys.exit(1)

if result.warnings:
    print("Research report validation warnings:")
    for warning in result.warnings:
        print(f"  âš ï¸  {warning}")

# Proceed with valid research report
research = result.research
```
````

## File: specs/001-agentready-scorer/data-model.md
````markdown
# Data Model: AgentReady Repository Scorer

**Phase**: 1 - Data Modeling
**Date**: 2025-11-20
**Purpose**: Define entities, relationships, and validation rules for the assessment system

## Overview

The data model consists of 7 primary entities organized in a hierarchical assessment workflow:
`Repository` â†’ `Assessment` â†’ `Finding[]` â†’ `Remediation`

All entities use Python dataclasses with type hints for clarity and validation.

## Entity Definitions

### 1. Repository

Represents the target git repository being assessed.

**Fields**:
- `path: pathlib.Path` - Absolute path to repository root
- `name: str` - Repository name (derived from path)
- `url: str | None` - Remote origin URL if available
- `branch: str` - Current branch name
- `commit_hash: str` - Current HEAD commit SHA
- `languages: dict[str, int]` - Detected languages with file counts (e.g., `{"Python": 42, "JavaScript": 18}`)
- `total_files: int` - Total files in repository (respecting .gitignore)
- `total_lines: int` - Total lines of code

**Validation Rules**:
- `path` must exist and contain `.git` directory
- `name` derived from `path.name`
- `languages` keys must be valid language names
- `total_files` and `total_lines` must be non-negative

**Relationships**:
- One Repository â†’ One Assessment

**Example**:
```python
Repository(
    path=Path("/home/user/myproject"),
    name="myproject",
    url="https://github.com/user/myproject.git",
    branch="main",
    commit_hash="abc123def456",
    languages={"Python": 42, "Markdown": 12},
    total_files=54,
    total_lines=8432
)
```

---

### 2. Attribute

Defines one of the 25 agent-ready quality attributes from the research report.

**Fields**:
- `id: str` - Unique identifier (e.g., "claude_md_file", "test_coverage")
- `name: str` - Human-readable name (e.g., "CLAUDE.md Configuration Files")
- `category: str` - Research report section (e.g., "Context Window Optimization")
- `tier: int` - Priority tier 1-4 (1=Essential, 4=Advanced)
- `description: str` - What this attribute measures
- `criteria: str` - Measurable criteria for passing
- `default_weight: float` - Default weight in scoring (0.0-1.0)

**Validation Rules**:
- `id` must be unique, lowercase, snake_case
- `tier` must be 1, 2, 3, or 4
- `default_weight` must be in range [0.0, 1.0]
- All default weights across 25 attributes must sum to 1.0 Â± 0.01

**Relationships**:
- One Attribute â†’ Many Findings (across different assessments)

**Example**:
```python
Attribute(
    id="claude_md_file",
    name="CLAUDE.md Configuration Files",
    category="Context Window Optimization",
    tier=1,
    description="Markdown file at repository root for Claude Code context",
    criteria="File exists, <1000 lines, contains required sections",
    default_weight=0.08
)
```

---

### 3. Assessment

Represents a complete evaluation of a repository at a specific point in time.

**Fields**:
- `repository: Repository` - The repository assessed
- `timestamp: datetime` - When assessment was performed
- `overall_score: float` - Weighted average score 0-100
- `certification_level: str` - Platinum/Gold/Silver/Bronze based on score
- `attributes_assessed: int` - Number of successfully evaluated attributes
- `attributes_skipped: int` - Number of skipped attributes (tool missing, errors)
- `attributes_total: int` - Total attributes (should be 25)
- `findings: list[Finding]` - Individual attribute results
- `config: Config | None` - Custom configuration used (if any)
- `duration_seconds: float` - Time taken for assessment

**Validation Rules**:
- `overall_score` must be in range [0.0, 100.0]
- `certification_level` must be one of: "Platinum", "Gold", "Silver", "Bronze", "Needs Improvement"
- `attributes_assessed + attributes_skipped` must equal `attributes_total`
- `findings` length must equal `attributes_total`

**Certification Thresholds**:
- Platinum: 90-100
- Gold: 75-89
- Silver: 60-74
- Bronze: 40-59
- Needs Improvement: 0-39

**Relationships**:
- One Assessment â†’ One Repository
- One Assessment â†’ Many Findings (exactly 25)

**Example**:
```python
Assessment(
    repository=repo,
    timestamp=datetime(2025, 11, 20, 14, 30, 0),
    overall_score=72.5,
    certification_level="Silver",
    attributes_assessed=23,
    attributes_skipped=2,
    attributes_total=25,
    findings=[...],
    config=None,
    duration_seconds=127.3
)
```

---

### 4. Finding

Result of assessing a single attribute against a repository.

**Fields**:
- `attribute: Attribute` - The attribute being assessed
- `status: str` - "pass", "fail", "skipped", "error", "not_applicable"
- `score: float | None` - Score 0-100, or None if skipped/error
- `measured_value: str | None` - Actual measurement (e.g., "847 lines", "63% coverage")
- `threshold: str | None` - Expected threshold (e.g., "<300 lines", ">80% coverage")
- `evidence: list[str]` - Specific files/metrics supporting the finding
- `remediation: Remediation | None` - How to fix if failing
- `error_message: str | None` - Error details if status="error"

**Validation Rules**:
- `status` must be one of the defined values
- `score` required if status="pass" or status="fail", otherwise None
- `score` must be in range [0.0, 100.0] if present
- `error_message` required if status="error"

**Status Meanings**:
- `pass`: Attribute meets criteria (score typically 70-100)
- `fail`: Attribute doesn't meet criteria (score typically 0-69)
- `skipped`: Attribute couldn't be assessed (missing tool, permission error)
- `error`: Unexpected error during assessment
- `not_applicable`: Attribute doesn't apply to this repository (e.g., language-specific check)

**Relationships**:
- Many Findings â†’ One Assessment
- Many Findings â†’ One Attribute

**Example**:
```python
Finding(
    attribute=Attribute(...),
    status="fail",
    score=40.0,
    measured_value="847 lines",
    threshold="<300 lines",
    evidence=["src/large_module.py: 847 lines"],
    remediation=Remediation(...),
    error_message=None
)
```

---

### 5. Remediation

Actionable guidance for fixing a failing attribute.

**Fields**:
- `summary: str` - One-line summary of what to do
- `steps: list[str]` - Ordered steps to remediate
- `tools: list[str]` - Tools/packages needed (e.g., "black", "pytest-cov")
- `commands: list[str]` - Example commands to run
- `examples: list[str]` - Code/config examples
- `citations: list[Citation]` - Links to documentation/research

**Validation Rules**:
- `summary` must be non-empty
- `steps` should have at least one step
- Each command should be a single line

**Relationships**:
- One Remediation â†’ One Finding
- Many Remediation â†’ Many Citations

**Example**:
```python
Remediation(
    summary="Split large_module.py into smaller modules",
    steps=[
        "Identify logical groupings within large_module.py",
        "Extract each grouping into separate module",
        "Update imports in dependent files",
        "Verify tests still pass"
    ],
    tools=["radon"],
    commands=["radon cc src/large_module.py -a"],
    examples=["# Before: 847 lines\n# After: 4 modules of ~200 lines each"],
    citations=[...]
)
```

---

### 6. Citation

Reference to authoritative source from research report.

**Fields**:
- `source: str` - Source name (e.g., "Anthropic Engineering Blog")
- `title: str` - Article/paper title
- `url: str | None` - Link to source
- `relevance: str` - Why this citation supports the attribute

**Validation Rules**:
- `source` must be non-empty
- `url` must be valid URL if present

**Relationships**:
- Many Citations â†’ One Remediation

**Example**:
```python
Citation(
    source="Microsoft Learn",
    title="Code metrics - Cyclomatic complexity",
    url="https://learn.microsoft.com/code-metrics",
    relevance="Defines cyclomatic complexity thresholds"
)
```

---

### 7. Config

User configuration for customizing assessment behavior.

**Fields**:
- `weights: dict[str, float]` - Custom attribute weights (attribute_id â†’ weight)
- `excluded_attributes: list[str]` - Attributes to skip
- `language_overrides: dict[str, list[str]]` - Force language detection
- `output_dir: pathlib.Path | None` - Custom output directory

**Validation Rules**:
- `weights` keys must be valid attribute IDs
- `weights` values must be in range [0.0, 1.0]
- `weights` values must sum to 1.0 Â± 0.01
- `excluded_attributes` must contain valid attribute IDs

**Relationships**:
- One Config â†’ One Assessment (optional)

**Example**:
```python
Config(
    weights={
        "claude_md_file": 0.10,  # Increase importance
        "test_coverage": 0.10,
        # ... other attributes
    },
    excluded_attributes=["performance_benchmarks"],
    language_overrides={"Python": ["*.pyx"]},
    output_dir=Path("/custom/reports")
)
```

---

## Weight Distribution

### Default Tier-Based Weights

Per FR-031, attributes are weighted by tier priority with heavy penalties for missing essentials (especially CLAUDE.md). Tier 1 (Essential) attributes have 10x the impact of Tier 4 (Advanced) attributes.

| Tier | Description | Total Weight | Attributes | Weight Per Attribute |
|------|-------------|--------------|------------|---------------------|
| Tier 1 | Essential | 50% | 5 attributes | 10.0% each |
| Tier 2 | Critical | 30% | 10 attributes | 3.0% each |
| Tier 3 | Important | 15% | 5 attributes | 3.0% each |
| Tier 4 | Advanced | 5% | 5 attributes | 1.0% each |

**Total**: 100% across 25 attributes

**Penalty Philosophy**: Missing Tier 1 essentials (especially CLAUDE.md at 10%) creates significant score impact, incentivizing foundational improvements before advanced optimizations.

### Attribute-to-Tier Mapping

Based on research report tier assignments (lines 1535-1570):

**Tier 1 (Essential)** - 50% total weight (10% each):
1. CLAUDE.md Configuration Files (1.1) - **10.0%** âš ï¸ CRITICAL
2. README Structure (2.1) - 10.0%
3. Type Annotations (3.3) - 10.0%
4. Standard Project Layouts (4.1) - 10.0%
5. Lock Files for Reproducibility (6.1) - 10.0%

**Tier 2 (Critical)** - 30% total weight (3% each):
6. Test Coverage Requirements (5.1) - 3.0%
7. Pre-commit Hooks & CI/CD Linting (5.3) - 3.0%
8. Conventional Commit Messages (7.1) - 3.0%
9. .gitignore Completeness (7.2) - 3.0%
10. One-Command Build/Setup (8.1) - 3.0%
11. Concise Structured Documentation (1.2) - 3.0%
12. Inline Documentation (2.2) - 3.0%
13. File Size Limits (1.3) - 3.0%
14. Dependency Freshness & Security (6.2) - 3.0%
15. Separation of Concerns (4.2) - 3.0%

**Tier 3 (Important)** - 15% total weight (3% each):
16. Cyclomatic Complexity Thresholds (3.1) - 3.0%
17. Structured Logging (9.2) - 3.0%
18. OpenAPI/Swagger Specifications (10.1) - 3.0%
19. Architecture Decision Records (2.3) - 3.0%
20. Semantic File & Directory Naming (11.3) - 3.0%

**Tier 4 (Advanced)** - 5% total weight (1% each):
21. Security Scanning Automation (13.1) - 1.0%
22. Performance Benchmarks (15.1) - 1.0%
23. Code Smell Elimination (3.4) - 1.0%
24. Issue & Pull Request Templates (7.3) - 1.0%
25. Container/Virtualization Setup (8.3) - 1.0%

### Custom Weight Configuration

Users can override default weights via `.agentready-config.yaml`:

```yaml
weights:
  claude_md_file: 0.15          # Increase from 10% to 15% (org prioritizes CLAUDE.md)
  test_coverage: 0.05           # Increase from 3% to 5%
  conventional_commits: 0.01    # Decrease from 3% to 1%
  # ... other attributes use defaults, rescaled to sum to 1.0
```

**Validation** (per FR-033):
- All 25 attributes must be present (explicitly or via defaults)
- Weights must be positive numbers
- Weights must sum to 1.0 (Â±0.001 tolerance for floating point)
- Missing attributes inherit rescaled tier defaults

### Score Calculation Example

```python
overall_score = sum(
    attribute_score * weight
    for attribute_score, weight in zip(scores, weights)
    if attribute_status == 'assessed'  # Exclude skipped per FR-027
)

# Normalize if some attributes skipped
total_weight_assessed = sum(
    weight for weight, status in zip(weights, statuses)
    if status == 'assessed'
)
normalized_score = (overall_score / total_weight_assessed) * 100
```

**Example Scenario**:
- Repository missing CLAUDE.md: loses 10 points immediately
- Repository with CLAUDE.md but no tests: scores 10 (CLAUDE.md) + 0 (tests)
- Perfect CLAUDE.md + perfect tests = 10 + 3 = 13 points from just 2 attributes

### Configuration Precedence

When weights exist in multiple locations:

1. **CLI flags** (highest priority) - `--weight claude_md_file=0.15`
2. **Config file** - `.agentready-config.yaml`
3. **Tier defaults** (lowest priority) - Built-in distribution

**Rationale**: Tier-based distribution with heavy penalties for missing essentials ensures:
- Users prioritize foundational improvements (CLAUDE.md, README, types)
- Missing CLAUDE.md (10% weight) has 10x impact of missing container setup (1% weight)
- Aligns with research report's "Essential â†’ Advanced" priority guidance
- Steep gradient incentivizes completing Tier 1 before moving to advanced features

---

## Configuration Weight Validation

**Per FR-033**: Custom weights in `.agentready-config.yaml` must be validated.

### Validation Rules

#### Rule 1: Completeness
All 25 attributes must have weights (explicit or inherited):

```python
REQUIRED_ATTRIBUTES = [
    'claude_md_file', 'concise_documentation', 'file_size_limits',
    'readme_structure', 'inline_documentation', 'architecture_decisions',
    'cyclomatic_complexity', 'function_length', 'type_annotations',
    'code_smells', 'standard_layout', 'separation_concerns',
    'test_coverage', 'test_naming', 'precommit_hooks',
    'lock_files', 'dependency_freshness', 'conventional_commits',
    'gitignore_completeness', 'issue_pr_templates', 'one_command_setup',
    'dev_env_docs', 'container_setup', 'error_clarity',
    'structured_logging', 'openapi_specs', 'graphql_schemas',
    'dry_principle', 'naming_conventions', 'semantic_naming',
    'cicd_visibility', 'branch_protection', 'security_scanning',
    'secrets_management', 'performance_benchmarks'
]

def validate_completeness(config_weights, default_weights):
    missing = [
        attr for attr in REQUIRED_ATTRIBUTES
        if attr not in config_weights and attr not in default_weights
    ]
    if missing:
        raise ValidationError(f"Missing weights for: {', '.join(missing)}")
```

#### Rule 2: Positive Values
All weights must be positive numbers:

```python
def validate_positive(weights):
    invalid = {attr: w for attr, w in weights.items() if w <= 0}
    if invalid:
        raise ValidationError(f"Weights must be positive: {invalid}")
```

#### Rule 3: Sum Constraint
Weights must sum to 1.0 within floating-point tolerance:

```python
TOLERANCE = 0.001  # Allow 0.1% rounding error

def validate_sum(weights):
    total = sum(weights.values())
    if abs(total - 1.0) > TOLERANCE:
        raise ValidationError(
            f"Weights must sum to 1.0 (got {total:.4f}). "
            f"Difference: {total - 1.0:+.4f}"
        )
```

#### Rule 4: Reasonable Bounds
Individual weights should be between 0.5% and 20% (warnings only):

```python
MIN_WEIGHT = 0.005  # 0.5%
MAX_WEIGHT = 0.20   # 20%

def validate_bounds(weights):
    warnings = []
    for attr, weight in weights.items():
        if weight < MIN_WEIGHT:
            warnings.append(f"{attr}: {weight:.1%} is very low (< 0.5%)")
        if weight > MAX_WEIGHT:
            warnings.append(f"{attr}: {weight:.1%} is very high (> 20%)")
    return warnings  # Non-blocking warnings
```

### Partial Configuration Handling

Users can specify only overrides, inheriting defaults for others:

```yaml
# .agentready-config.yaml (partial configuration)
weights:
  claude_md_file: 0.15      # Override Tier 1 default (10%)
  test_coverage: 0.05       # Override Tier 2 default (3%)
  # Other 23 attributes: inherit rescaled tier defaults
```

**Rescaling Algorithm**:

```python
def merge_and_rescale(config_weights, tier_defaults):
    """
    Merge config overrides with tier defaults, then rescale to sum to 1.0.

    Args:
        config_weights: User-specified weights (partial or complete)
        tier_defaults: Default tier-based weights (complete, sums to 1.0)

    Returns:
        Final weights dictionary (complete, sums to 1.0)
    """
    # Start with tier defaults
    final_weights = tier_defaults.copy()

    # Override with config values
    final_weights.update(config_weights)

    # Rescale to sum to 1.0
    total = sum(final_weights.values())
    rescaled = {attr: w / total for attr, w in final_weights.items()}

    return rescaled

# Example:
# config = {'claude_md_file': 0.15, 'test_coverage': 0.05}
# defaults = {25 attributes summing to 1.0}
# After merge: sum = 1.07 (overrides increased total by 0.07)
# After rescale: all weights * (1.0 / 1.07) â‰ˆ 0.935x scaling factor
```

### Validation Command

```bash
agentready --validate-config [path/to/config.yaml]
```

**Output Example**:

```
Validating config: .agentready-config.yaml

âœ“ All 25 attributes have weights
âœ“ All weights are positive
âœ“ Weights sum to 1.0000 (within tolerance)
âš  claude_md_file: 15.0% is high (default: 10.0%, tier 1 average: 10.0%)
âš  test_coverage: 5.0% is high (default: 3.0%, tier 2 average: 3.0%)

Configuration: VALID with 2 warnings

Effective weights after rescaling:
  claude_md_file: 14.02% (config: 15.0%, rescaled down)
  test_coverage: 4.67% (config: 5.0%, rescaled down)
  conventional_commits: 2.80% (default: 3.0%, rescaled down)
  ... (22 more attributes)

Total: 100.00%
```

### Error Messages

**Invalid Sum**:
```
ERROR: Weights must sum to 1.0 (got 0.8500). Difference: -0.1500

Your weights total 85%. This suggests missing attributes or incorrect values.

Current distribution:
  Explicitly specified: 0.6500 (13 attributes)
  Using tier defaults: 0.2000 (12 attributes)

Suggestion: Either specify all 25 attributes explicitly, or let unspecified
attributes inherit tier defaults (which will be automatically rescaled).
```

**Negative Weight**:
```
ERROR: Weights must be positive: {'test_coverage': -0.05}

Negative weights are not allowed. Did you mean to reduce other weights instead?
```

**Missing Attributes**:
```
ERROR: Missing weights for: inline_documentation, architecture_decisions

All 25 attributes must have weights. Either:
1. Specify these attributes in your config, OR
2. Ensure they exist in default-weights.yaml

To see all required attributes:
  agentready --list-attributes
```

**Tolerance Exceeded**:
```
ERROR: Weights must sum to 1.0 (got 1.0025). Difference: +0.0025

Sum exceeds tolerance (Â±0.001). Likely due to rounding errors.

Suggestion: Reduce precision or adjust values. For example:
  claude_md_file: 0.100 (instead of 0.1003)
  test_coverage: 0.050 (instead of 0.0497)
```

### Validation Integration

- **On config load**: Automatic validation (fail fast on errors)
- **On manual validate**: Detailed report with warnings
- **On assessment run**: Validation errors block execution
- **On example generation**: `--generate-config` produces valid example with comments

### Configuration Precedence

**Precedence Order** (highest to lowest):

1. **CLI flags** - `agentready --weight claude_md_file=0.15`
2. **Config file** - `.agentready-config.yaml` in current directory
3. **Config file (alternate path)** - `agentready --config /path/to/config.yaml`
4. **Tier defaults** - Built-in `src/agentready/data/default-weights.yaml`

**Merging Rules**:
- CLI flags override config file weights
- Config file overrides tier defaults
- Unspecified attributes always inherit tier defaults
- Final weights always rescaled to sum to 1.0

**Example**:
```bash
# CLI flag takes precedence
agentready --config custom.yaml --weight claude_md_file=0.20

# Precedence:
# 1. claude_md_file = 0.20 (from CLI flag)
# 2. Other weights from custom.yaml (if present)
# 3. Remaining weights from tier defaults
# 4. All rescaled to sum to 1.0
```

---

## State Transitions

### Assessment Workflow

```
Repository â†’ Validate â†’ Detect Languages â†’ Load Config â†’ Create Assessment

For each Attribute:
    Create Assessor â†’ Check Applicability

    if applicable:
        Execute Assessment â†’ Create Finding
    else:
        Create Finding (status=not_applicable)

Calculate Overall Score â†’ Determine Certification Level

Generate Reports â†’ Save to Disk
```

### Finding Status Flow

```
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚   Start     â”‚
                           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                            â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  is_applicable?â”‚          â”‚   Execute      â”‚
            â”‚      No         â”‚          â”‚   Assessment   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                            â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ not_applicable  â”‚      â”‚            â”‚            â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
                                â”‚ pass   â”‚  â”‚   fail    â”‚ â”‚ error  â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Serialization

All entities must support JSON serialization for:
- Report generation (HTML/Markdown use JSON as intermediate format)
- Automation integration (tools consuming assessment data)
- Caching/persistence (save assessment results)

**Serialization Strategy**:
- Use `dataclasses.asdict()` with custom JSON encoder
- Convert `datetime` to ISO 8601 strings
- Convert `Path` to strings
- Convert enums to string values
- Preserve nested structures

**Example JSON Output**:
```json
{
  "repository": {
    "name": "myproject",
    "path": "/home/user/myproject",
    "languages": {"Python": 42},
    "total_files": 54
  },
  "timestamp": "2025-11-20T14:30:00",
  "overall_score": 72.5,
  "certification_level": "Silver",
  "findings": [
    {
      "attribute": {"id": "claude_md_file", "name": "CLAUDE.md"},
      "status": "pass",
      "score": 100.0,
      "evidence": ["CLAUDE.md exists (487 lines)"]
    }
  ]
}
```

---

## Summary

Data model defines 7 entities with clear responsibilities:
- `Repository`: What is being assessed
- `Attribute`: What is being measured
- `Assessment`: Container for complete evaluation
- `Finding`: Result of one attribute check
- `Remediation`: How to fix failures
- `Citation`: Supporting research
- `Config`: User customization

All validation rules enforced at model layer (fail fast). State transitions are linear (no complex workflows). JSON serialization supports all output formats and automation integrations.
````

## File: specs/001-agentready-scorer/plan.md
````markdown
# Implementation Plan: AgentReady Repository Scorer

**Branch**: `001-agentready-scorer` | **Date**: 2025-11-20 | **Spec**: [spec.md](spec.md)
**Input**: Feature specification from `/specs/001-agentready-scorer/spec.md`

**Note**: This template is filled in by the `/speckit.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.

## Summary

Build a command-line tool called "agentready" that assesses git repositories against 25 evidence-based attributes for AI-assisted development readiness. The tool analyzes repository quality across categories like documentation, code quality, testing, structure, CI/CD, and security, then generates dual-format reports (interactive HTML + version-control-friendly Markdown) with specific findings, evidence, remediation guidance, and an overall agent-readiness score with certification level (Platinum/Gold/Silver/Bronze).

Technical approach: Library-first architecture with standalone assessment modules for each attribute category, CLI wrapper orchestrating the scan workflow, bundled research data with update capability, and self-contained HTML report generation with embedded interactivity.

## Technical Context

**Language/Version**: Python 3.11+ (only N and N-1 versions supported per constitution)
**Primary Dependencies**:
- Click (CLI framework)
- Jinja2 (HTML template rendering)
- PyYAML (configuration file parsing)
- gitpython (repository introspection)
- Language-specific analyzers: radon (Python complexity), lizard (multi-language complexity)

**Storage**: File-based (no database required)
- Bundled research report (agent-ready-codebase-attributes.md)
- Optional user configuration file (.agentready-config.yaml)
- Generated reports (.agentready/ directory)

**Testing**: pytest with pytest-cov for coverage tracking
- Unit tests for individual attribute assessors
- Integration tests for full scan workflows
- Contract tests for report schemas

**Target Platform**: Cross-platform CLI (Linux, macOS, Windows via Python)
**Project Type**: Single project (CLI tool with library modules)
**Performance Goals**:
- Complete assessment in <5 minutes for repositories with <10k files
- Deterministic scoring (consistent results across runs)
- Minimal memory footprint (<500MB for typical repositories)

**Constraints**:
- Offline-capable (internet only for research report updates)
- Self-contained HTML reports (no external CDN dependencies)
- Graceful degradation (partial results if some checks fail)
- No modification of scanned repository (read-only analysis)

**Scale/Scope**:
- 25 attribute assessors (one per research report attribute)
- Support 5+ programming languages (Python, JavaScript, TypeScript, Go, Java minimum)
- Handle repositories up to 100k files (with performance degradation warning)

### Research Report Update Mechanism

**Per FR-023**: Tool must support updating the bundled research report.

**Versioning Philosophy**: agentready releases are bundled with a specific research report version. The tool and research report are versioned together (e.g., agentready v1.2.0 includes research report v1.2.0).

**Implementation Design**:

1. **Bundled Report**:
   - Research report shipped with tool installation in `src/agentready/data/agent-ready-codebase-attributes.md`
   - Tool version and research version always match in releases
   - Users can always rely on stable, tested research content

2. **Custom Research Reports**:
   - Users can point to custom research reports:
     ```bash
     agentready --research-file /path/to/custom-research.md
     agentready --research-file https://example.com/custom-research.md
     ```
   - Enables organizations to customize criteria for internal standards
   - Custom reports must pass validation (per FR-024)

3. **Update Command** (explicit opt-in only):
   ```bash
   agentready --update-research
   ```

4. **Update Source**:
   - Primary: `https://github.com/ambient-code/agentready/raw/main/src/agentready/data/agent-ready-codebase-attributes.md`
   - Downloads latest research report from main branch
   - User explicitly opts in to update

5. **Update Process** (atomic):
   ```
   a. Download new version to temp file
   b. Validate structure (per FR-024):
      - Check for required metadata (version, date)
      - Verify 25 attributes present
      - Validate Markdown format
      - Parse successfully with research_loader.py
   c. If valid:
      - Backup current: agent-ready-codebase-attributes.md â†’ .md.backup
      - Atomic rename: temp â†’ agent-ready-codebase-attributes.md
      - Remove backup on success
      - Display: "Updated research report from vX.Y.Z to vA.B.C"
   d. If invalid:
      - Delete temp file
      - Restore from backup if needed
      - Exit with error message and validation details
      - Keep current version intact
   ```

6. **Version Tracking**:
   - Research report includes metadata header:
     ```markdown
     ---
     version: 1.2.0
     date: 2025-11-20
     ---
     ```
   - Tool displays current research version:
     ```bash
     agentready --research-version
     # Output: Research report v1.2.0 (2025-11-20)
     ```

7. **Offline Behavior**:
   - If network unavailable: "Cannot reach update server. Using bundled version v1.0.0"
   - If update fails: "Update failed (validation error). Using current version v1.0.0"
   - Never leave tool in broken state
   - Always fall back to last known-good version

8. **Rollback**:
   ```bash
   agentready --restore-bundled-research  # Restore original from package
   ```
   - Restores research report from tool installation package
   - Useful if custom/updated research causes issues

**Error Handling**:
- Network timeout (30s): Graceful failure, keep current version
- Invalid format: Detailed validation error, rollback to previous
- Permission denied: Error with instructions to check file permissions
- Partial download: Checksum validation, retry or abort

**Testing** (integration test):
- Mock HTTP server with valid/invalid research reports
- Verify atomic updates (no partial writes)
- Test rollback on validation failure
- Confirm offline graceful degradation
- Verify version detection and display

**Philosophy**: Research report updates are explicitly opt-in only via `--update-research`. Standard workflow uses stable bundled version. Users who want latest research or custom criteria can update or override as needed.

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### Principle I: Evidence-Based Design
âœ… **PASS** - All 25 attributes derived from agent-ready-codebase-attributes.md research report with 50+ authoritative citations. Every assessment criterion grounded in documented findings from Anthropic, Microsoft, Google, ArXiv, IEEE/ACM sources.

### Principle II: Measurable Quality
âœ… **PASS** - Each attribute has:
- Clear criteria (e.g., "CLAUDE.md exists and <1000 lines", "test coverage >80%")
- Automated tooling specified (radon, pytest-cov, gitpython, lizard)
- Quantifiable thresholds from research report
- Good/bad examples in research report

### Principle III: Tool-First Mindset
âœ… **PASS** - Architecture follows library-first approach:
- Standalone assessor modules (importable, independently testable)
- CLI wrapper as thin orchestration layer
- Text-based I/O (repository path in â†’ JSON/Markdown/HTML out)
- Each assessor usable without full agentready toolchain

### Principle IV: Test-Driven Development
âœ… **PASS** - TDD workflow planned:
- Phase 2 will generate test tasks BEFORE implementation tasks
- Contract tests for report schemas (validate structure)
- Integration tests for scan workflows
- Unit tests for each attribute assessor
- Test coverage >80% enforced

### Principle V: Structured Output
âœ… **PASS** - Dual-format output design:
- HTML for human consumption (interactive, visual)
- Markdown for version control/automation (parseable, diffable)
- Internal JSON representation (structured data model)
- Structured logging with progress indicators
- Error messages with context and guidance (FR-030)

### Principle VI: Incremental Delivery
âœ… **PASS** - User stories prioritized for MVP-first:
- P1 (MVP): Core scoring engine â†’ Delivers immediate value
- P2: Interactive HTML features â†’ Enhanced UX
- P3: Remediation guidance â†’ Actionable next steps
- P4: Version control integration â†’ Team collaboration
- Each story independently testable and deployable

### Principle VII: Documentation as Code
âœ… **PASS** - Documentation strategy:
- README with <5 minute quickstart (generated in Phase 1)
- CLAUDE.md with project context (to be created)
- Inline docstrings explaining "why" (assessment rationale)
- Examples in quickstart.md
- Research report citations preserved in all outputs

**GATE STATUS**: âœ… ALL PRINCIPLES PASS - Proceed to Phase 0 research

## Project Structure

### Documentation (this feature)

```text
specs/001-agentready-scorer/
â”œâ”€â”€ spec.md              # Feature specification
â”œâ”€â”€ plan.md              # This file (/speckit.plan command output)
â”œâ”€â”€ research.md          # Phase 0 output (/speckit.plan command)
â”œâ”€â”€ data-model.md        # Phase 1 output (/speckit.plan command)
â”œâ”€â”€ quickstart.md        # Phase 1 output (/speckit.plan command)
â”œâ”€â”€ contracts/           # Phase 1 output (/speckit.plan command)
â”‚   â”œâ”€â”€ assessment-schema.json    # Assessment data model JSON schema
â”‚   â”œâ”€â”€ report-markdown-schema.md # Markdown report structure
â”‚   â””â”€â”€ report-html-schema.md     # HTML report structure/API
â””â”€â”€ checklists/
    â””â”€â”€ requirements.md  # Validation checklist
```

### Source Code (repository root)

```text
src/
â”œâ”€â”€ agentready/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ main.py           # Click-based CLI entry point
â”‚   â”œâ”€â”€ assessors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py           # Base assessor interface
â”‚   â”‚   â”œâ”€â”€ documentation.py  # Attributes 1.1-2.3 (CLAUDE.md, README, docstrings, ADRs)
â”‚   â”‚   â”œâ”€â”€ code_quality.py   # Attributes 3.1-3.4 (complexity, length, types, smells)
â”‚   â”‚   â”œâ”€â”€ structure.py      # Attributes 4.1-4.2 (layout, separation of concerns)
â”‚   â”‚   â”œâ”€â”€ testing.py        # Attributes 5.1-5.3 (coverage, naming, pre-commit)
â”‚   â”‚   â”œâ”€â”€ dependencies.py   # Attributes 6.1-6.2 (lock files, freshness/security)
â”‚   â”‚   â”œâ”€â”€ vcs.py            # Attributes 7.1-7.3 (commits, gitignore, templates)
â”‚   â”‚   â”œâ”€â”€ build.py          # Attributes 8.1-8.3 (one-command setup, docs, containers)
â”‚   â”‚   â”œâ”€â”€ errors.py         # Attributes 9.1-9.2 (error clarity, structured logging)
â”‚   â”‚   â”œâ”€â”€ api_docs.py       # Attributes 10.1-10.2 (OpenAPI, GraphQL)
â”‚   â”‚   â”œâ”€â”€ modularity.py     # Attributes 11.1-11.3 (DRY, naming, semantic files)
â”‚   â”‚   â”œâ”€â”€ cicd.py           # Attributes 12.1-12.2 (pipeline visibility, branch protection)
â”‚   â”‚   â”œâ”€â”€ security.py       # Attributes 13.1-13.2 (scanning, secrets)
â”‚   â”‚   â””â”€â”€ performance.py    # Attribute 15.1 (benchmarks)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ repository.py     # Repository entity
â”‚   â”‚   â”œâ”€â”€ attribute.py      # Attribute definition
â”‚   â”‚   â”œâ”€â”€ assessment.py     # Assessment entity
â”‚   â”‚   â”œâ”€â”€ finding.py        # Finding entity
â”‚   â”‚   â””â”€â”€ config.py         # Configuration model
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ scanner.py        # Orchestrates assessment workflow
â”‚   â”‚   â”œâ”€â”€ scorer.py         # Calculates weighted scores
â”‚   â”‚   â”œâ”€â”€ language_detector.py  # Detects repository languages
â”‚   â”‚   â””â”€â”€ research_loader.py    # Loads bundled/updated research report
â”‚   â”œâ”€â”€ reporters/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py           # Base reporter interface
â”‚   â”‚   â”œâ”€â”€ markdown.py       # Markdown report generator
â”‚   â”‚   â””â”€â”€ html.py           # HTML report generator (with embedded JS/CSS)
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ report.html.j2    # Jinja2 template for HTML report
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ agent-ready-codebase-attributes.md  # Bundled research report
â”‚       â””â”€â”€ default-weights.yaml                 # Default tier-based weights
â”‚
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_assessors.py     # Individual assessor tests
â”‚   â”œâ”€â”€ test_models.py        # Data model tests
â”‚   â”œâ”€â”€ test_services.py      # Service layer tests
â”‚   â””â”€â”€ test_reporters.py     # Report generator tests
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_scan_workflow.py      # End-to-end scan tests
â”‚   â”œâ”€â”€ test_report_generation.py  # Dual-format report tests
â”‚   â””â”€â”€ test_cli.py                # CLI interface tests
â””â”€â”€ contract/
    â”œâ”€â”€ test_assessment_schema.py  # JSON schema validation
    â”œâ”€â”€ test_markdown_schema.py    # Markdown structure validation
    â””â”€â”€ test_html_schema.py        # HTML structure validation

pyproject.toml                # Project metadata, dependencies, build config
README.md                     # Project overview and quickstart
CLAUDE.md                     # Claude Code context file
.agentready-config.example.yaml  # Example configuration file
```

**Structure Decision**: Single project (Option 1) selected. This is a CLI tool with library modules, not a web/mobile application. The structure follows Python src-layout convention with clear separation between CLI (thin wrapper), core logic (assessors, services, models), and output (reporters). Tests mirror source structure for easy navigation.

## Complexity Tracking

> **No violations** - All constitution principles pass without requiring justification.
````

## File: specs/001-agentready-scorer/quickstart.md
````markdown
# QuickStart: AgentReady Repository Scorer

**Goal**: Get from repository clone to running your first assessment in <5 minutes

**Prerequisites**: Python 3.11+ installed

---

## Installation

### Option 1: Install from PyPI (Recommended)

```bash
# Create and activate virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install agentready
pip install agentready

# Verify installation
agentready --version
```

### Option 2: Install from Source

```bash
# Clone repository
git clone https://github.com/your-org/agentready.git
cd agentready

# Install with development dependencies
pip install -e ".[dev]"

# Verify installation
agentready --version
```

---

## Basic Usage

### Run Assessment on Current Directory

```bash
# Navigate to your repository
cd /path/to/your/repository

# Run assessment (uses current directory)
agentready

# Reports saved to .agentready/ directory
```

### Run Assessment on Specific Repository

```bash
# Assess a different repository
agentready /path/to/other/repository

# Or use relative paths
agentready ../my-project
```

### View Generated Reports

```bash
# Open HTML report in browser
open .agentready/report-latest.html     # macOS
xdg-open .agentready/report-latest.html # Linux
start .agentready/report-latest.html    # Windows

# View Markdown report
cat .agentready/report-latest.md

# Or open in your favorite markdown viewer
code .agentready/report-latest.md  # VS Code
```

---

## Common Options

### Verbose Output

See detailed progress during assessment:

```bash
agentready --verbose
```

Output:
```
ðŸ” Analyzing repository: /home/user/myproject
âœ“ Validated git repository
âœ“ Detected languages: Python, Markdown
âœ“ Loaded research report (v1.0.0)

ðŸ“Š Assessing attributes:
  [1/25] âœ… CLAUDE.md Configuration Files... 100/100
  [2/25] âš ï¸  Concise Structured Documentation... 60/100
  [3/25] âœ… README Structure... 85/100
  ...
  [25/25] âŒ Performance Benchmarks... Skipped (no benchmarks found)

ðŸ“„ Generating reports...
  âœ“ HTML report: .agentready/report-2025-11-20T14-30-00.html
  âœ“ Markdown report: .agentready/report-2025-11-20T14-30-00.md

âœ¨ Assessment complete!
   Score: 72/100 (Silver)
   Duration: 2m 7s
```

### Custom Output Directory

Save reports to a specific location:

```bash
agentready --output-dir /custom/path
```

Reports will be saved to `/custom/path/` instead of `.agentready/`

### Update Research Report

Download latest version of the research report:

```bash
agentready --update-research
```

This updates the bundled research report with the latest findings and recommendations.

---

## Configuration File

### Create Custom Configuration

```bash
# Generate example configuration
agentready --generate-config

# This creates .agentready-config.yaml
```

### Example Configuration

```yaml
# .agentready-config.yaml

# Custom attribute weights (must sum to 1.0)
weights:
  claude_md_file: 0.10          # Increase importance (default: 0.08)
  test_coverage: 0.10           # Increase importance (default: 0.05)
  conventional_commits: 0.05    # Keep default
  # ... other attributes use defaults

# Exclude specific attributes from assessment
excluded_attributes:
  - performance_benchmarks  # Skip if not relevant to your project

# Override language detection
language_overrides:
  Python:
    - "*.pyx"    # Include Cython files as Python
  JavaScript:
    - "*.jsx"
    - "*.tsx"    # Include TypeScript/React files

# Custom output directory (overrides --output-dir flag)
output_dir: reports/agentready
```

### Use Configuration

```bash
# agentready automatically loads .agentready-config.yaml from current directory
agentready

# Or specify a different config file
agentready --config /path/to/config.yaml
```

---

## Understanding Your Results

### Certification Levels

| Level | Score | Meaning |
|-------|-------|---------|
| ðŸ¥‡ Platinum | 90-100 | Exemplary agent-ready codebase |
| ðŸ¥‡ Gold | 75-89 | Highly optimized for AI agents |
| ðŸ¥ˆ Silver | 60-74 | Well-suited for AI development |
| ðŸ¥‰ Bronze | 40-59 | Basic agent compatibility |
| â›” Needs Improvement | 0-39 | Significant work needed |

### Prioritizing Improvements

The HTML report's "Next Steps" section shows top improvements ranked by:
1. **Point Potential**: How many points you could gain
2. **Tier Priority**: Tier 1 (Essential) improvements listed first
3. **Ease of Fix**: Quick wins highlighted

**Example Priority**:
```
Next Steps to Gold (need +3 points):
1. Fix README size (+10 potential) - Tier 1 - Easy
2. Add pre-commit hooks (+8 potential) - Tier 2 - Medium
3. Improve dependency freshness (+5 potential) - Tier 2 - Easy
```

Focus on Tier 1 and Tier 2 improvements first for maximum impact.

---

## Tracking Progress Over Time

### Compare Reports

```bash
# Run assessment
agentready

# Make improvements to your repository
# (e.g., add CLAUDE.md, improve test coverage, add pre-commit hooks)

# Run assessment again
agentready

# Compare Markdown reports (they're git-friendly!)
git diff .agentready/report-2025-11-20T14-30-00.md .agentready/report-2025-11-20T16-45-00.md
```

### Commit Reports to Version Control

```bash
# Add reports to git for historical tracking
git add .agentready/report-*.md
git commit -m "docs: add agent-ready assessment (Silver, 72/100)"

# Include in pull requests to show improvements
```

---

## Troubleshooting

### "Not a git repository" Error

```bash
Error: /path/to/directory is not a git repository (no .git directory found)
```

**Solution**: Ensure you're in a git repository root:
```bash
git init  # If needed
git status  # Verify
```

### Missing Tool Warnings

```bash
â­ï¸ Skipped: Cyclomatic Complexity (missing tool: radon)
```

**Solution**: Install missing language-specific tools:
```bash
# Python complexity analysis
pip install radon

# Multi-language complexity
pip install lizard

# Re-run assessment
agentready
```

### Slow Assessment (>5 minutes)

```bash
âš ï¸ Assessment took 8m 32s (expected <5m for <10k files)
```

**Causes**:
- Very large repository (>10k files)
- Slow disk I/O
- Many languages to analyze

**Solutions**:
- Use `--exclude` to skip vendor/generated directories
- Run on SSD for faster file scanning
- Consider assessing specific subdirectories

### Permission Errors

```bash
â­ï¸ Skipped: File Size Limits (permission denied: /protected/directory)
```

**Solution**: Ensure read access to all repository files or accept partial results (agentready continues with available data).

---

## Next Steps

1. **Review HTML Report**: Open `.agentready/report-latest.html` in browser
2. **Check Top Priorities**: Look at "Next Steps" section
3. **Make Improvements**: Start with Tier 1 (Essential) attributes
4. **Re-assess**: Run `agentready` again to see score improvement
5. **Track Progress**: Commit reports to git for historical comparison

---

## Examples

### Assess Multiple Repositories

```bash
#!/bin/bash
# assess-all.sh - Assess all repositories in a directory

for repo in ~/projects/*/; do
    echo "Assessing $repo..."
    agentready "$repo" --output-dir "$repo/.agentready"
done
```

### CI/CD Integration

```yaml
# .github/workflows/agentready.yml
name: Agent-Ready Assessment

on:
  pull_request:
    branches: [ main ]

jobs:
  assess:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install agentready
        run: pip install agentready

      - name: Run assessment
        run: agentready --output-dir reports/

      - name: Upload report
        uses: actions/upload-artifact@v3
        with:
          name: agentready-report
          path: reports/report-latest.html

      - name: Comment score on PR
        run: |
          SCORE=$(cat reports/report-latest.md | grep "Overall Score" | awk '{print $3}')
          echo "Agent-Ready Score: $SCORE" >> $GITHUB_STEP_SUMMARY
```

### Custom Weighting for Organization

```yaml
# .agentready-config.yaml
# Enterprise configuration emphasizing security and testing

weights:
  # Security-focused (30% total)
  security_scanning: 0.10
  secrets_management: 0.10
  dependency_freshness: 0.10

  # Testing-focused (25% total)
  test_coverage: 0.10
  test_naming: 0.08
  pre_commit_hooks: 0.07

  # Documentation (20% total)
  claude_md_file: 0.08
  readme_structure: 0.07
  inline_documentation: 0.05

  # Code quality (15% total)
  type_annotations: 0.06
  cyclomatic_complexity: 0.05
  code_smells: 0.04

  # Remaining attributes split remaining 10%
  # (not listed = use calculated defaults from remaining weight)
```

---

## Getting Help

```bash
# Show all available commands and options
agentready --help

# Show version information
agentready --version

# Show research report version
agentready --research-version

# Generate example configuration
agentready --generate-config

# Validate existing configuration
agentready --validate-config
```

---

## Quick Reference

| Command | Purpose |
|---------|---------|
| `agentready` | Assess current directory |
| `agentready /path` | Assess specific repository |
| `agentready --verbose` | Show detailed progress |
| `agentready --output-dir DIR` | Custom output location |
| `agentready --config FILE` | Use custom configuration |
| `agentready --update-research` | Update research report |
| `agentready --generate-config` | Create example config |
| `agentready --help` | Show all options |

---

**Time from install to first report**: ~2 minutes
**Typical assessment duration**: 2-5 minutes
**Report formats**: HTML (interactive) + Markdown (version control)
**Default output**: `.agentready/` directory

ðŸŽ¯ **Goal Achieved**: You now have a comprehensive assessment of your repository's AI-readiness!
````

## File: specs/001-agentready-scorer/research.md
````markdown
# Research: AgentReady Repository Scorer

**Phase**: 0 - Technical Research and Best Practices
**Date**: 2025-11-20
**Purpose**: Document technical decisions, rationale, and best practices for implementation

## 1. CLI Framework Selection

### Decision: Click

**Rationale**:
- Declarative command definition with decorators (clean, readable)
- Automatic help generation and input validation
- Native support for subcommands, options, and arguments
- Excellent error handling with user-friendly messages
- Wide adoption in Python CLI tools (pytest, pip, AWS CLI v2)

**Alternatives Considered**:
- `argparse` (stdlib): Too verbose, manual help text, basic validation
- `typer`: Type-hint based, but adds FastAPI dependency (unnecessary weight)
- `docopt`: String-based interface description (less type-safe)

**Best Practices**:
- Use `click.Path(exists=True, file_okay=False)` for repository path validation
- Implement `--verbose` with `click.option('--verbose', is_flag=True)`
- Use `click.echo()` for stdout (handles encoding across platforms)
- Use `click.secho()` for colored output (certification levels, progress)
- Implement progress bars with `click.progressbar()`

## 2. HTML Report Generation

### Decision: Jinja2 + Embedded CSS/JavaScript

**Rationale**:
- Jinja2 is industry-standard Python templating (Django, Flask)
- Template-based approach separates presentation from logic
- Easy to maintain and update report layout
- Embedding CSS/JS ensures offline functionality (FR-003)
- Single-file output simplifies distribution

**Alternatives Considered**:
- Direct HTML string building: Unmaintainable, error-prone
- WeasyPrint (HTMLâ†’PDF): Not required, HTML itself is deliverable
- React/Vue SPA: Requires build step, complex for static reports

**Best Practices**:
- Inline all CSS in `<style>` tag (avoid external stylesheets)
- Inline all JavaScript in `<script>` tag (no CDN dependencies)
- Use vanilla JavaScript (no jQuery/framework dependencies)
- Minify embedded CSS/JS for smaller file size
- Use CSS Grid/Flexbox for responsive layout (modern browser support)
- Implement collapsible sections with `<details>` and `<summary>` tags (native HTML5)
- Add `data-*` attributes for JavaScript hooks (clean separation)

**Interactive Features Implementation**:
- Filtering: JavaScript array `.filter()` on attribute findings
- Sorting: JavaScript array `.sort()` with custom comparators
- Collapsible sections: CSS `:target` pseudo-class + JavaScript event listeners
- Progress bars: CSS `width` percentage on div elements
- Color coding: CSS classes based on score thresholds

## 3. Language Detection Strategy

### Decision: File Extension Analysis + gitignore Awareness

**Rationale**:
- File extensions are reliable primary indicator (.py, .js, .ts, .go, .java)
- gitignore exclusion prevents counting generated/vendor code
- Fast scanning (no file content parsing required for detection)
- Supports polyglot repositories (multiple languages)

**Alternatives Considered**:
- GitHub Linguist: Heavyweight dependency, overkill for detection
- File content analysis (shebangs, syntax): Slow, unnecessary
- Package manager detection only: Misses standalone scripts

**Best Practices**:
- Scan repository once, cache language map
- Respect .gitignore patterns (use gitpython's `git ls-files`)
- Map extensions to languages: `.py`â†’Python, `.js/.jsx`â†’JavaScript, `.ts/.tsx`â†’TypeScript
- Handle ambiguous extensions (`.h` could be C or C++)
- Set minimum file threshold (e.g., 5+ files to count as "using language")

## 4. Attribute Weighting System

### Decision: Tier-Based Default + YAML Configuration Override

**Rationale**:
- Research report defines 4 tiers (Essential â†’ Advanced)
- Default weights encode tier priorities (Tier 1 highest impact)
- YAML configuration allows organizational customization
- Weights must sum to 1.0 (percentages) for interpretable scores

**Default Weight Distribution**:
```
Tier 1 (Essential) - 5 attributes:    40% total (8% each)
Tier 2 (Critical) - 6 attributes:     30% total (5% each)
Tier 3 (Important) - 7 attributes:    20% total (~2.86% each)
Tier 4 (Advanced) - 7 attributes:     10% total (~1.43% each)
```

**Configuration File Format** (.agentready-config.yaml):
```yaml
weights:
  claude_md_file: 0.08
  conventional_commits: 0.05
  type_annotations: 0.08
  # ... all 25 attributes

# Optional: Override tier defaults
tier_multipliers:
  tier_1: 2.0  # Double importance
  tier_2: 1.5
  tier_3: 1.0
  tier_4: 0.5
```

**Best Practices**:
- Validate weights sum to 1.0 Â± 0.01 (floating point tolerance)
- Provide clear error messages for invalid configurations
- Document weight rationale in example config file
- Allow partial overrides (missing attributes use defaults)

## 5. Assessment Architecture Pattern

### Decision: Strategy Pattern with Base Assessor Interface

**Rationale**:
- Each attribute is an independent assessment strategy
- Common interface ensures consistency across assessors
- Easy to add new attributes or modify existing ones
- Enables parallel assessment execution (future optimization)
- Supports graceful degradation (skip failed assessors)

**Base Assessor Interface**:
```python
class BaseAssessor(ABC):
    @property
    @abstractmethod
    def attribute_id(self) -> str:
        """Unique attribute identifier (e.g., 'claude_md_file')"""

    @property
    @abstractmethod
    def tier(self) -> int:
        """Tier 1-4 from research report"""

    @abstractmethod
    def assess(self, repository: Repository) -> Finding:
        """Execute assessment, return Finding with score, evidence, remediation"""

    def is_applicable(self, repository: Repository) -> bool:
        """Check if attribute applies to this repository (e.g., language-specific)"""
        return True
```

**Best Practices**:
- Keep assessors stateless (no instance variables)
- Return structured Finding objects (never raw dicts)
- Include specific evidence (file paths, line numbers, measurements)
- Generate remediation guidance from research report content
- Handle errors gracefully (return Finding with error status, not exceptions)
- Use type hints for all method signatures

## 6. Repository Analysis Tools

### Decision: gitpython + Language-Specific Analyzers

**Primary Tools**:
- **gitpython**: Repository metadata, commit history, .gitignore parsing
- **radon**: Python cyclomatic complexity, maintainability index
- **lizard**: Multi-language complexity analysis (C, C++, Java, JavaScript, etc.)
- **stdlib pathlib**: File system traversal and pattern matching

**Rationale**:
- gitpython provides Git-native operations (respects .gitignore)
- radon is Python-specific, highly accurate for Python codebases
- lizard supports multiple languages with consistent interface
- Avoid shell calls to external tools (portability, security)

**Best Practices**:
- Cache file system scans (don't traverse repository multiple times)
- Use gitpython's `git.Repo.iter_commits()` for commit analysis
- Lazy-load analysis tools (import only when language detected)
- Set reasonable limits (max files to analyze, timeout per check)
- Provide progress callbacks for long-running analyses

## 7. Error Handling and Graceful Degradation

### Decision: Try-Assess-Skip Pattern

**Rationale**:
- Partial results more valuable than complete failure
- Users can fix blockers (install missing tools) and re-run
- Clear error reporting enables self-service troubleshooting
- Maintains progress indication (X/25 attributes assessed)

**Error Handling Strategy**:
```python
for assessor in assessors:
    try:
        if not assessor.is_applicable(repo):
            findings.append(Finding.not_applicable(assessor.attribute_id))
            continue

        finding = assessor.assess(repo)
        findings.append(finding)

    except MissingToolError as e:
        findings.append(Finding.skipped(
            assessor.attribute_id,
            reason=f"Missing tool: {e.tool_name}",
            remediation=f"Install with: {e.install_command}"
        ))

    except PermissionError as e:
        findings.append(Finding.skipped(
            assessor.attribute_id,
            reason=f"Permission denied: {e.filename}"
        ))

    except Exception as e:
        findings.append(Finding.error(
            assessor.attribute_id,
            reason=f"Unexpected error: {str(e)}"
        ))
        logger.exception(f"Assessment failed for {assessor.attribute_id}")
```

**Best Practices**:
- Define custom exception types for known failure modes
- Include remediation guidance in error findings
- Log full stack traces for unexpected errors (debugging)
- Count skipped/error attributes separately from assessed
- Adjust score calculation (exclude skipped from denominator per FR-027)

## 8. Report File Naming and Versioning

### Decision: Timestamp-Based Filenames with Latest Symlink

**Rationale**:
- Timestamps enable tracking score improvements over time
- Unique filenames prevent accidental overwrites
- Symlink to latest makes it easy to find most recent report
- ISO 8601 format ensures sortable filenames

**File Naming Convention**:
```
.agentready/
â”œâ”€â”€ report-2025-11-20T14-30-00.html
â”œâ”€â”€ report-2025-11-20T14-30-00.md
â”œâ”€â”€ report-latest.html -> report-2025-11-20T14-30-00.html
â””â”€â”€ report-latest.md -> report-2025-11-20T14-30-00.md
```

**Best Practices**:
- Use ISO 8601 with colons replaced by hyphens (cross-platform safe)
- Create symlinks atomically (write to temp, rename)
- Offer `--output-name` flag for custom naming
- Clean up old reports (keep last N runs, configurable)
- Store assessment metadata in JSON sidecar file (for automation)

## 9. Markdown Report Structure

### Decision: GitHub-Flavored Markdown with Tables

**Rationale**:
- GFM is ubiquitous (GitHub, GitLab, Bitbucket, VS Code)
- Tables display structured data clearly
- Collapsible sections with `<details>` work in GFM
- Easy to diff in version control

**Report Structure**:
```markdown
# AgentReady Assessment Report

**Repository**: owner/repo
**Date**: 2025-11-20 14:30:00
**Score**: 72/100 (Silver)
**Attributes Assessed**: 23/25 (2 skipped)

## Summary

| Category | Score | Status |
|----------|-------|--------|
| Documentation | 85% | âœ… Pass |
| Code Quality | 62% | âš ï¸ Needs Improvement |
| ... | ... | ... |

## Detailed Findings

### 1. Context Window Optimization

#### 1.1 CLAUDE.md Configuration Files âœ…

**Score**: 100/100
**Evidence**:
- File exists: `CLAUDE.md` (487 lines)
- Contains tech stack section
- Contains standard commands

<details>
<summary>Good Practices Found</summary>

- Concise format (<1000 lines)
- Clear repository structure
</details>

#### 1.2 Concise Structured Documentation âš ï¸

**Score**: 60/100
**Evidence**:
- README exists (1247 lines) - exceeds recommended 500 lines

**Remediation**:
- Split README into multiple docs
- Move detailed API docs to wiki
```

**Best Practices**:
- Use emoji sparingly (âœ…âŒâš ï¸ for status only)
- Include clickable table of contents for long reports
- Preserve all research report citations
- Use code blocks with language tags for remediation commands
- Keep tables simple (3-5 columns maximum)

## 10. Testing Strategy

### Decision: Pytest with Fixtures + Test Repositories

**Rationale**:
- Pytest is Python's de-facto testing framework
- Fixtures enable reusable test repositories
- Test repositories provide known-good/known-bad data
- Contract tests validate report schemas

**Test Repository Approach**:
Create fixture repositories in `tests/fixtures/repositories/`:
- `minimal-python/`: Bare Python project (most attributes fail)
- `gold-standard-python/`: Exemplary Python project (high score)
- `polyglot/`: Multi-language repository
- `edge-cases/`: Unusual structures (monorepo, submodules, etc.)

**Best Practices**:
- Use `@pytest.fixture(scope="session")` for repository setup (expensive)
- Store expected scores/findings in JSON files
- Test each assessor in isolation (unit tests)
- Test full workflow with real git repositories (integration tests)
- Use `pytest-cov` to enforce >80% coverage
- Mock external tools in unit tests, use real tools in integration tests

## 11. Proportional Scoring Algorithm

### Decision: Linear Proportional Scoring

**Context**: Many attributes have measurable thresholds that can be partially met (e.g., 65% test coverage when 80% is required). Linear proportional scoring provides deterministic, understandable results (per FR-014).

**Rationale**: Linear proportional scoring is:
- **Simple**: Easy to understand and explain to users
- **Deterministic**: Same inputs always produce same outputs
- **Fair**: Provides clear incentives for incremental improvement
- **Predictable**: Users can calculate expected score changes

**Algorithm**:
```python
def calculate_proportional_score(measured_value, threshold, attribute_type):
    """
    Calculate proportional score for partial compliance.

    Args:
        measured_value: The measured value (numeric or parseable)
        threshold: The target threshold
        attribute_type: 'higher_is_better' or 'lower_is_better'

    Returns:
        Score from 0-100
    """
    if attribute_type == 'higher_is_better':
        # Example: test coverage (want higher values)
        if measured_value >= threshold:
            return 100
        elif measured_value <= 0:
            return 0
        else:
            return (measured_value / threshold) * 100

    elif attribute_type == 'lower_is_better':
        # Example: file length (want lower values)
        if measured_value <= threshold:
            return 100
        elif threshold == 0:
            return 0  # Avoid division by zero
        else:
            # Degrade linearly, cap at 0
            return max(0, 100 - ((measured_value - threshold) / threshold) * 100)
```

**Edge Cases**:
- Division by zero: Return 0 score
- Negative values: Clamp to 0
- Values exceeding 2x threshold (lower_is_better): Cap at 0
- Values exceeding 2x threshold (higher_is_better): Cap at 100

**Examples**:
- Test coverage: 65% measured, 80% threshold â†’ 65/80 * 100 = 81.25 score
- File length: 450 lines measured, 300 threshold â†’ 100 - ((450-300)/300)*100 = 50 score
- Cyclomatic complexity: 5 measured, 10 threshold â†’ 100 (meets threshold)

**Alternatives Considered**:
- Exponential penalties: Too harsh for minor violations
- Sigmoid curves: Complex to explain, non-obvious behavior
- Step functions: Too coarse-grained (all-or-nothing)

**References**:
- Spec FR-014: "Tool MUST handle repositories that partially meet attribute criteria"
- Spec FR-027: "Tool MUST calculate overall score based only on successfully evaluated attributes"

## Summary

All technical decisions grounded in pragmatic Python ecosystem best practices. No research gaps or unresolved questions. Implementation can proceed directly to Phase 1 (data modeling and contracts).
````

## File: specs/001-agentready-scorer/spec.md
````markdown
# Feature Specification: AgentReady Repository Scorer

**Feature Branch**: `001-agentready-scorer`
**Created**: 2025-11-20
**Status**: Draft
**Input**: User description: "I want to build a tool that scores git repos based on the findings of this report: agent-ready-codebase-attributes.md. The goal is to prepare a git repo for introduction of agentic development patterns. The tools goal is to provide repo owners with a report and brief remediation plan for each item that requires remediation. the tool must be called agentready. use the content of the report to generate remediation plans etc. i want the report to be a single html file that is interactive. i also want a markdown version of the report to be generated. always generate both formats when the tool is run"

## Clarifications

### Session 2025-11-20

- Q: Where should the agentready tool save the generated HTML and Markdown reports? â†’ A: `.agentready/` directory in repository root by default, with user-specified output directory via CLI flag as option
- Q: How does the agentready tool locate the agent-ready-codebase-attributes.md research report file? â†’ A: Research report bundled with tool installation, with support for downloading latest version from URL/repository
- Q: What should happen if the tool cannot complete assessment for some attributes (e.g., missing analysis tools, permission errors)? â†’ A: Complete available attributes, clearly flag failed/skipped attributes in report with reasons
- Q: What should the tool display to stdout/stderr during execution (before reports are generated)? â†’ A: Summary progress by default, detailed real-time logging with verbose flag
- Q: How should attribute weights be determined for calculating the overall agent-readiness score? â†’ A: Fixed weights based on research report tier system by default, with user-configurable weights via configuration file

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Score Repository Against Agent-Ready Criteria (Priority: P1)

A repository owner wants to assess how well their codebase is optimized for AI-assisted development. They run the agentready tool on their repository and receive a comprehensive score with specific findings across all 25 agent-ready attributes documented in the research report.

**Why this priority**: This is the core value proposition - without scoring and assessment, there is no actionable feedback. This alone provides immediate value by identifying gaps.

**Independent Test**: Can be fully tested by running the tool against any git repository and verifying it produces a score and identifies attribute compliance levels. Delivers a complete assessment without needing remediation features.

**Acceptance Scenarios**:

1. **Given** a git repository with various quality attributes, **When** user runs agentready tool on repository, **Then** tool analyzes all 25 attributes from the research report and produces an overall agent-readiness score
2. **Given** a repository missing CLAUDE.md file, **When** agentready scans the repository, **Then** tool correctly identifies this attribute as non-compliant and includes it in the assessment
3. **Given** a repository with 85% test coverage, **When** agentready evaluates test coverage attribute, **Then** tool correctly identifies this as meeting the >80% threshold and scores it positively
4. **Given** a repository with no lock files, **When** agentready checks dependency management, **Then** tool flags missing lock files and includes specific examples of what's missing
5. **Given** analysis is complete, **When** tool generates reports, **Then** both HTML and Markdown versions are created with identical content

---

### User Story 2 - View Interactive HTML Report (Priority: P2)

After running the assessment, the repository owner views an interactive HTML report that allows them to explore findings by category, filter by compliance level, and drill into specific attribute details with expandable sections.

**Why this priority**: Enhances usability and makes large amounts of assessment data digestible. Builds on P1 by improving how results are consumed.

**Independent Test**: Can be tested by generating and opening the HTML report in a browser, verifying interactive elements (collapsible sections, filters, sorting) work without requiring the scoring engine.

**Acceptance Scenarios**:

1. **Given** agentready has completed assessment, **When** user opens the HTML report in browser, **Then** report displays overall score prominently with visual indicators (color coding, progress bars)
2. **Given** HTML report is displayed, **When** user clicks on an attribute category (e.g., "Documentation Standards"), **Then** section expands to show detailed findings for all attributes in that category
3. **Given** report shows multiple failing attributes, **When** user applies "show only failures" filter, **Then** report hides passing attributes and displays only those requiring remediation
4. **Given** an attribute is marked as failing, **When** user views that attribute in report, **Then** specific evidence is shown (e.g., "File size: src/large_module.py is 847 lines, exceeds 300 line limit")

---

### User Story 3 - Receive Remediation Guidance (Priority: P3)

For each attribute that fails compliance checks, the repository owner receives specific, actionable remediation steps based on the research report's recommendations, tooling suggestions, and examples.

**Why this priority**: Transforms assessment into action. Builds on P1 (scoring) and P2 (viewing) by providing the "what to do next" guidance.

**Independent Test**: Can be tested by reviewing remediation plans for known failing attributes and verifying they include specific tools, commands, examples, and links to documentation.

**Acceptance Scenarios**:

1. **Given** repository lacks CLAUDE.md file, **When** user views remediation for this attribute, **Then** guidance includes specific template, required sections, and example content
2. **Given** repository has cyclomatic complexity >25 in multiple functions, **When** user views remediation plan, **Then** plan lists specific files/functions exceeding threshold, suggests refactoring tools (radon for Python), and provides complexity reduction strategies
3. **Given** repository missing pre-commit hooks, **When** user views remediation, **Then** plan includes installation commands, sample configuration for the detected language, and links to pre-commit framework documentation
4. **Given** multiple attributes fail, **When** user views overall remediation plan, **Then** plan prioritizes fixes by impact tier (Tier 1 Essential â†’ Tier 4 Advanced) as defined in research report

---

### User Story 4 - Read Markdown Report for Version Control (Priority: P4)

Repository owners want to track agent-readiness improvements over time by committing Markdown reports to version control, allowing them to compare scores across commits and share findings with team members in pull requests.

**Why this priority**: Enables continuous improvement tracking and team collaboration. Enhances existing features but not critical for initial value delivery.

**Independent Test**: Can be tested by generating Markdown report, committing it to git, and verifying it renders properly on platforms like GitHub with all tables, formatting, and links intact.

**Acceptance Scenarios**:

1. **Given** agentready completes assessment, **When** Markdown report is generated, **Then** report uses standard Markdown formatting (tables, headers, links) that renders correctly on GitHub
2. **Given** Markdown report from previous run exists, **When** user compares it with new report, **Then** differences in scores and attribute compliance are easily identifiable
3. **Given** report contains external citations, **When** user views Markdown report, **Then** all citations from research report are properly linked and attributions preserved

---

### Edge Cases

- What happens when repository is not a git repository (no .git directory)? Tool aborts with clear error message before assessment begins (validated per FR-017)
- How does tool handle extremely large repositories (>100k files)? Tool completes assessment but may exceed 5-minute target (SC-001 applies to <10k files only)
- What if repository uses multiple programming languages with different standards? Tool evaluates all detected languages and applies language-specific criteria per FR-013
- How does tool behave when file access is denied (permission errors)? Tool skips inaccessible files, continues assessment, flags permission issues in report per FR-026
- What if repository has no code files (documentation-only repos)? Tool completes assessment with many attributes marked N/A or failing, still produces valid report
- How does tool handle repositories with submodules or monorepo structures? Tool evaluates repository as-is, treating submodules as part of file structure
- What if HTML/Markdown report generation fails mid-process? Both reports fail atomically per FR-019, no partial report files left
- How does tool handle repositories with non-standard layouts that don't match any common pattern? Tool attempts best-effort pattern matching, may score lower on structure-related attributes

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: Tool MUST be named "agentready" and executable from command line
- **FR-002**: Tool MUST accept a git repository path as input (current directory if not specified)
- **FR-003**: Tool MUST evaluate all 25 attributes defined in agent-ready-codebase-attributes.md research report
- **FR-004**: Tool MUST calculate an overall agent-readiness score (0-100) using weighted average of attribute scores (weights from FR-031/FR-032)
- **FR-005**: Tool MUST generate both HTML and Markdown format reports in a single execution
- **FR-006**: Tool MUST include specific evidence for each attribute evaluation (file names, line counts, measurements)
- **FR-007**: Tool MUST provide remediation guidance for each failing attribute, derived from research report recommendations
- **FR-008**: HTML report MUST be a single self-contained file (embedded CSS/JavaScript, no external dependencies)
- **FR-009**: HTML report MUST include interactive features (collapsible sections, filtering, sorting)
- **FR-010**: Markdown report MUST use standard GitHub-flavored Markdown for maximum compatibility
- **FR-011**: Tool MUST preserve all citations and attributions from the research report in generated reports
- **FR-012**: Tool MUST categorize attributes by tier (Tier 1 Essential through Tier 4 Advanced) as defined in research report
- **FR-013**: Tool MUST detect programming language(s) used in repository and apply language-specific criteria
- **FR-014**: Tool MUST handle repositories that partially meet attribute criteria (e.g., 50% test coverage) with proportional scoring
- **FR-015**: Remediation plans MUST include specific tooling recommendations, commands, and configuration examples from research report
- **FR-016**: Reports MUST include certification level (Platinum/Gold/Silver/Bronze) based on score ranges defined in research report
- **FR-017**: Tool MUST validate git repository before starting assessment (presence of .git directory)
- **FR-018**: Tool MUST provide progress indication showing completed attributes count during analysis (superseded by FR-028/FR-029 for specifics)
- **FR-019**: Both report formats MUST be generated atomically (both succeed or both fail)
- **FR-020**: Tool MUST save reports to `.agentready/` directory in repository root by default, creating directory if it doesn't exist
- **FR-021**: Tool MUST support `--output-dir` CLI flag allowing users to specify custom output directory for reports
- **FR-022**: Tool MUST bundle agent-ready-codebase-attributes.md research report as internal resource accessible without external dependencies
- **FR-023**: Tool MUST support updating bundled research report by downloading latest version from authoritative URL/repository
- **FR-024**: Tool MUST validate research report integrity (format, required sections) before using it for assessment
- **FR-025**: Tool MUST complete assessment for all evaluable attributes even if some attributes fail, rather than aborting entire scan
- **FR-026**: Tool MUST clearly indicate skipped or failed attributes in reports with specific failure reasons (e.g., "missing tool: radon", "permission denied: /protected/file")
- **FR-027**: Tool MUST calculate overall score based only on successfully evaluated attributes, excluding failed/skipped attributes from denominator
- **FR-028**: Tool MUST display summary progress to stdout by default (e.g., "Analyzing... [12/25 attributes complete]")
- **FR-029**: Tool MUST support `--verbose` CLI flag that enables detailed real-time logging of each file analyzed and check performed
- **FR-030**: Tool MUST output errors to stderr with clear error messages and context
- **FR-031**: Tool MUST use tier-based weighting system from research report by default (Tier 1 Essential weighted higher than Tier 4 Advanced)
- **FR-032**: Tool MUST support optional configuration file allowing users to customize attribute weights for organizational priorities
- **FR-033**: Tool MUST validate configuration file weights (all attributes present, weights are positive numbers, sum to meaningful total)

### Key Entities

- **Repository**: The target git repository being assessed, characterized by root path, detected languages, file structure, and git metadata
- **Attribute**: One of 25 agent-ready quality attributes from research report, including name, category, tier, weight, measurable criteria, and assessment method
- **Assessment**: Complete evaluation of repository, including overall score, certification level, per-attribute results, timestamp, and repository metadata
- **Finding**: Individual attribute evaluation result, including pass/fail status, measured value, threshold, evidence (specific files/metrics), and remediation guidance
- **Report**: Generated output document (HTML or Markdown), containing assessment results, formatted for specific use case (interactive viewing vs. version control)
- **Remediation Plan**: Actionable guidance for fixing failing attribute, including tools, commands, examples, and citations from research report
- **Citation**: Reference to authoritative source from research report, including author, title, publication, URL, and relevance to specific attribute

### Assumptions

- Repository owners have command-line access to their repositories
- Standard file system permissions allow reading repository files
- Repository follows conventional version control practices (not bare repository)
- Target audience has basic understanding of software quality metrics
- Internet connectivity optional for tool execution (required only for downloading updated research report)
- Reports will be viewed in modern browsers (for HTML) or platforms supporting GitHub-flavored Markdown
- Repository size is reasonable for local file system traversal (not petabyte-scale)
- Programming language detection can be performed via file extensions and common patterns

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: Users can run complete assessment and generate both reports in under 5 minutes for repositories with <10k files
- **SC-002**: Tool correctly identifies compliance for all 25 attributes with >95% accuracy when validated against known test repositories
- **SC-003**: Generated HTML reports are fully functional without internet connectivity (no external CDN dependencies)
- **SC-004**: Markdown reports render correctly on GitHub, GitLab, and Bitbucket platforms without formatting issues
- **SC-005**: 90% of remediation plans include specific, executable commands or tool configurations that users can apply immediately
- **SC-006**: Overall agent-readiness scores align within 5 points when validated against manual evaluation by domain expert
- **SC-007**: Tool successfully handles repositories in at least 5 major programming languages (Python, JavaScript, TypeScript, Go, Java)
- **SC-008**: Users can identify top 3 priority improvements within 2 minutes of viewing report (via filtering/sorting features)
- **SC-009**: 100% of citations and attributions from research report are preserved in generated reports
- **SC-010**: Tool produces consistent scores when run multiple times on same repository (deterministic results)
````

## File: src/agentready/assessors/base.py
````python
"""Base assessor interface for attribute evaluation."""

from abc import ABC, abstractmethod

from ..models.finding import Finding
from ..models.repository import Repository


class BaseAssessor(ABC):
    """Abstract base class for all attribute assessors.

    Each assessor evaluates one or more related attributes and returns
    structured findings with evidence, scores, and remediation guidance.

    Assessors follow the strategy pattern and are stateless for easy testing
    and parallel execution.
    """

    @property
    @abstractmethod
    def attribute_id(self) -> str:
        """Unique attribute identifier (e.g., 'claude_md_file').

        Must be lowercase snake_case matching the attribute ID in the
        research report and default-weights.yaml.
        """
        pass

    @property
    @abstractmethod
    def tier(self) -> int:
        """Tier 1-4 from research report (1=Essential, 4=Advanced)."""
        pass

    @abstractmethod
    def assess(self, repository: Repository) -> Finding:
        """Execute assessment and return Finding with score, evidence, remediation.

        Args:
            repository: Repository entity with path, languages, metadata

        Returns:
            Finding with status (pass/fail/skipped/error/not_applicable),
            score (0-100 if applicable), evidence, and remediation

        Raises:
            This method should NOT raise exceptions. Handle errors gracefully
            and return Finding.error() or Finding.skipped() instead.
        """
        pass

    def is_applicable(self, repository: Repository) -> bool:
        """Check if attribute applies to this repository.

        Default implementation returns True (attribute applies to all repos).
        Override for language-specific or conditional attributes.

        Args:
            repository: Repository entity with detected languages

        Returns:
            True if attribute should be assessed, False to skip

        Examples:
            - Python-specific check: return "Python" in repository.languages
            - API check: return any openapi/swagger files exist
        """
        return True

    def calculate_proportional_score(
        self,
        measured_value: float,
        threshold: float,
        higher_is_better: bool = True,
    ) -> float:
        """Calculate proportional score for partial compliance.

        Uses linear interpolation to score values between 0 and threshold.

        Args:
            measured_value: The measured value (e.g., 65 for coverage %)
            threshold: The target threshold (e.g., 80 for coverage %)
            higher_is_better: True for metrics like coverage, False for complexity

        Returns:
            Score from 0-100

        Examples:
            - Test coverage: 65% measured, 80% threshold, higher_is_better=True
              â†’ 65/80 * 100 = 81.25 score
            - File length: 450 lines measured, 300 threshold, higher_is_better=False
              â†’ 100 - ((450-300)/300)*100 = 50 score
        """
        if higher_is_better:
            # Want higher values (e.g., test coverage, type annotation %)
            if measured_value >= threshold:
                return 100.0
            elif measured_value <= 0:
                return 0.0
            else:
                return min(100.0, (measured_value / threshold) * 100.0)
        else:
            # Want lower values (e.g., complexity, file length)
            if measured_value <= threshold:
                return 100.0
            elif threshold == 0:
                return 0.0  # Avoid division by zero
            else:
                # Degrade linearly, cap at 0
                penalty = ((measured_value - threshold) / threshold) * 100.0
                return max(0.0, 100.0 - penalty)
````

## File: src/agentready/assessors/code_quality.py
````python
"""Code quality assessors for complexity, file length, type annotations, and code smells."""

import subprocess
from pathlib import Path

from ..models.attribute import Attribute
from ..models.finding import Citation, Finding, Remediation
from ..models.repository import Repository
from ..services.scanner import MissingToolError
from .base import BaseAssessor


class TypeAnnotationsAssessor(BaseAssessor):
    """Assesses type annotation coverage in code.

    Tier 1 Essential (10% weight) - Type hints are critical for AI understanding.
    """

    @property
    def attribute_id(self) -> str:
        return "type_annotations"

    @property
    def tier(self) -> int:
        return 1  # Essential

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="Type Annotations",
            category="Code Quality",
            tier=self.tier,
            description="Type hints in function signatures",
            criteria=">80% of functions have type annotations",
            default_weight=0.10,
        )

    def is_applicable(self, repository: Repository) -> bool:
        """Only applicable to statically-typed or type-hinted languages."""
        applicable_languages = {
            "Python",
            "TypeScript",
            "Java",
            "C#",
            "Kotlin",
            "Go",
            "Rust",
        }
        return bool(set(repository.languages.keys()) & applicable_languages)

    def assess(self, repository: Repository) -> Finding:
        """Check type annotation coverage.

        For Python: Use mypy or similar
        For TypeScript: Check tsconfig.json strict mode
        For others: Heuristic checks
        """
        if "Python" in repository.languages:
            return self._assess_python_types(repository)
        elif "TypeScript" in repository.languages:
            return self._assess_typescript_types(repository)
        else:
            # For other languages, use heuristic
            return Finding.not_applicable(
                self.attribute,
                reason=f"Type annotation check not implemented for {list(repository.languages.keys())}",
            )

    def _assess_python_types(self, repository: Repository) -> Finding:
        """Assess Python type annotations using file inspection."""
        # Simple heuristic: count functions with/without type hints
        try:
            result = subprocess.run(
                ["git", "ls-files", "*.py"],
                cwd=repository.path,
                capture_output=True,
                text=True,
                timeout=30,
                check=True,
            )
            python_files = [f for f in result.stdout.strip().split("\n") if f]
        except (subprocess.SubprocessError, FileNotFoundError):
            python_files = [
                str(f.relative_to(repository.path))
                for f in repository.path.rglob("*.py")
            ]

        total_functions = 0
        typed_functions = 0

        for file_path in python_files:
            full_path = repository.path / file_path
            try:
                with open(full_path, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if line.startswith("def ") and "(" in line:
                            total_functions += 1
                            # Check for type hints (-> in signature)
                            if "->" in line or ":" in line.split("(")[1]:
                                typed_functions += 1
            except (OSError, UnicodeDecodeError):
                continue

        if total_functions == 0:
            return Finding.not_applicable(
                self.attribute, reason="No Python functions found"
            )

        coverage_percent = (typed_functions / total_functions) * 100
        score = self.calculate_proportional_score(
            measured_value=coverage_percent,
            threshold=80.0,
            higher_is_better=True,
        )

        status = "pass" if score >= 75 else "fail"

        return Finding(
            attribute=self.attribute,
            status=status,
            score=score,
            measured_value=f"{coverage_percent:.1f}%",
            threshold="â‰¥80%",
            evidence=[
                f"Typed functions: {typed_functions}/{total_functions}",
                f"Coverage: {coverage_percent:.1f}%",
            ],
            remediation=self._create_remediation() if status == "fail" else None,
            error_message=None,
        )

    def _assess_typescript_types(self, repository: Repository) -> Finding:
        """Assess TypeScript type configuration."""
        tsconfig_path = repository.path / "tsconfig.json"

        if not tsconfig_path.exists():
            return Finding(
                attribute=self.attribute,
                status="fail",
                score=0.0,
                measured_value="missing tsconfig.json",
                threshold="strict mode enabled",
                evidence=["tsconfig.json not found"],
                remediation=self._create_remediation(),
                error_message=None,
            )

        try:
            import json

            with open(tsconfig_path, "r") as f:
                tsconfig = json.load(f)

            strict = tsconfig.get("compilerOptions", {}).get("strict", False)

            if strict:
                return Finding(
                    attribute=self.attribute,
                    status="pass",
                    score=100.0,
                    measured_value="strict mode enabled",
                    threshold="strict mode enabled",
                    evidence=["tsconfig.json has strict: true"],
                    remediation=None,
                    error_message=None,
                )
            else:
                return Finding(
                    attribute=self.attribute,
                    status="fail",
                    score=50.0,
                    measured_value="strict mode disabled",
                    threshold="strict mode enabled",
                    evidence=["tsconfig.json missing strict: true"],
                    remediation=self._create_remediation(),
                    error_message=None,
                )

        except (OSError, json.JSONDecodeError) as e:
            return Finding.error(
                self.attribute, reason=f"Could not parse tsconfig.json: {str(e)}"
            )

    def _create_remediation(self) -> Remediation:
        """Create remediation guidance for type annotations."""
        return Remediation(
            summary="Add type annotations to function signatures",
            steps=[
                "For Python: Add type hints to function parameters and return types",
                "For TypeScript: Enable strict mode in tsconfig.json",
                "Use mypy or pyright for Python type checking",
                "Use tsc --strict for TypeScript",
                "Add type annotations gradually to existing code",
            ],
            tools=["mypy", "pyright", "typescript"],
            commands=[
                "# Python",
                "pip install mypy",
                "mypy --strict src/",
                "",
                "# TypeScript",
                "npm install --save-dev typescript",
                'echo \'{"compilerOptions": {"strict": true}}\' > tsconfig.json',
            ],
            examples=[
                """# Python - Before
def calculate(x, y):
    return x + y

# Python - After
def calculate(x: float, y: float) -> float:
    return x + y
""",
                """// TypeScript - tsconfig.json
{
  "compilerOptions": {
    "strict": true,
    "noImplicitAny": true,
    "strictNullChecks": true
  }
}
""",
            ],
            citations=[
                Citation(
                    source="Python.org",
                    title="Type Hints",
                    url="https://docs.python.org/3/library/typing.html",
                    relevance="Official Python type hints documentation",
                ),
                Citation(
                    source="TypeScript",
                    title="TypeScript Handbook",
                    url="https://www.typescriptlang.org/docs/handbook/2/everyday-types.html",
                    relevance="TypeScript type system guide",
                ),
            ],
        )


class CyclomaticComplexityAssessor(BaseAssessor):
    """Assesses cyclomatic complexity using radon."""

    @property
    def attribute_id(self) -> str:
        return "cyclomatic_complexity"

    @property
    def tier(self) -> int:
        return 3  # Important

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="Cyclomatic Complexity Thresholds",
            category="Code Quality",
            tier=self.tier,
            description="Cyclomatic complexity thresholds enforced",
            criteria="Average complexity <10, no functions >15",
            default_weight=0.03,
        )

    def is_applicable(self, repository: Repository) -> bool:
        """Applicable to languages supported by radon or lizard."""
        supported = {"Python", "JavaScript", "TypeScript", "C", "C++", "Java"}
        return bool(set(repository.languages.keys()) & supported)

    def assess(self, repository: Repository) -> Finding:
        """Check cyclomatic complexity using radon or lizard."""
        if "Python" in repository.languages:
            return self._assess_python_complexity(repository)
        else:
            # Use lizard for other languages
            return self._assess_with_lizard(repository)

    def _assess_python_complexity(self, repository: Repository) -> Finding:
        """Assess Python complexity using radon."""
        try:
            # Check if radon is available
            result = subprocess.run(
                ["radon", "cc", str(repository.path), "-s", "-a"],
                capture_output=True,
                text=True,
                timeout=60,
            )

            if result.returncode != 0:
                raise MissingToolError("radon", install_command="pip install radon")

            # Parse radon output for average complexity
            # Output format: "Average complexity: A (5.2)"
            output = result.stdout

            if "Average complexity:" in output:
                # Extract average value
                avg_line = [
                    l for l in output.split("\n") if "Average complexity:" in l
                ][0]
                avg_value = float(avg_line.split("(")[1].split(")")[0])

                score = self.calculate_proportional_score(
                    measured_value=avg_value,
                    threshold=10.0,
                    higher_is_better=False,
                )

                status = "pass" if score >= 75 else "fail"

                return Finding(
                    attribute=self.attribute,
                    status=status,
                    score=score,
                    measured_value=f"{avg_value:.1f}",
                    threshold="<10.0",
                    evidence=[f"Average cyclomatic complexity: {avg_value:.1f}"],
                    remediation=(
                        self._create_remediation() if status == "fail" else None
                    ),
                    error_message=None,
                )
            else:
                return Finding.not_applicable(
                    self.attribute, reason="No Python code to analyze"
                )

        except MissingToolError as e:
            raise  # Re-raise to be caught by Scanner
        except Exception as e:
            return Finding.error(
                self.attribute, reason=f"Complexity analysis failed: {str(e)}"
            )

    def _assess_with_lizard(self, repository: Repository) -> Finding:
        """Assess complexity using lizard (multi-language)."""
        try:
            result = subprocess.run(
                ["lizard", str(repository.path)],
                capture_output=True,
                text=True,
                timeout=60,
            )

            if result.returncode != 0:
                raise MissingToolError("lizard", install_command="pip install lizard")

            # Parse lizard output
            # This is simplified - production code should parse properly
            return Finding.not_applicable(
                self.attribute, reason="Lizard analysis not fully implemented"
            )

        except MissingToolError as e:
            raise
        except Exception as e:
            return Finding.error(
                self.attribute, reason=f"Complexity analysis failed: {str(e)}"
            )

    def _create_remediation(self) -> Remediation:
        """Create remediation guidance for high complexity."""
        return Remediation(
            summary="Reduce cyclomatic complexity by refactoring complex functions",
            steps=[
                "Identify functions with complexity >15",
                "Break down complex functions into smaller functions",
                "Extract conditional logic into separate functions",
                "Use early returns to reduce nesting",
                "Consider using strategy pattern for complex conditionals",
            ],
            tools=["radon", "lizard"],
            commands=[
                "# Install radon",
                "pip install radon",
                "",
                "# Check complexity",
                "radon cc src/ -s -a",
                "",
                "# Find high complexity functions",
                "radon cc src/ -n C",
            ],
            examples=[],
            citations=[
                Citation(
                    source="Microsoft",
                    title="Code Metrics - Cyclomatic Complexity",
                    url="https://learn.microsoft.com/en-us/visualstudio/code-quality/code-metrics-cyclomatic-complexity",
                    relevance="Explanation of cyclomatic complexity and thresholds",
                )
            ],
        )
````

## File: src/agentready/assessors/documentation.py
````python
"""Documentation assessor for CLAUDE.md, README, docstrings, and ADRs."""

from pathlib import Path

from ..models.attribute import Attribute
from ..models.finding import Citation, Finding, Remediation
from ..models.repository import Repository
from .base import BaseAssessor


class CLAUDEmdAssessor(BaseAssessor):
    """Assesses presence and quality of CLAUDE.md configuration file.

    CLAUDE.md is the MOST IMPORTANT attribute (10% weight - Tier 1 Essential).
    Missing this file has 10x the impact of missing advanced features.
    """

    @property
    def attribute_id(self) -> str:
        return "claude_md_file"

    @property
    def tier(self) -> int:
        return 1  # Essential

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="CLAUDE.md Configuration Files",
            category="Context Window Optimization",
            tier=self.tier,
            description="Project-specific configuration for Claude Code",
            criteria="CLAUDE.md file exists in repository root",
            default_weight=0.10,
        )

    def assess(self, repository: Repository) -> Finding:
        """Check for CLAUDE.md file in repository root.

        Pass criteria: CLAUDE.md exists
        Scoring: Binary (100 if exists, 0 if not)
        """
        claude_md_path = repository.path / "CLAUDE.md"

        if claude_md_path.exists():
            # Check file size (should have content)
            try:
                size = claude_md_path.stat().st_size
                if size < 50:
                    # File exists but is too small
                    return Finding(
                        attribute=self.attribute,
                        status="fail",
                        score=25.0,
                        measured_value=f"{size} bytes",
                        threshold=">50 bytes",
                        evidence=[f"CLAUDE.md exists but is minimal ({size} bytes)"],
                        remediation=self._create_remediation(),
                        error_message=None,
                    )

                return Finding(
                    attribute=self.attribute,
                    status="pass",
                    score=100.0,
                    measured_value="present",
                    threshold="present",
                    evidence=[f"CLAUDE.md found at {claude_md_path}"],
                    remediation=None,
                    error_message=None,
                )

            except OSError:
                return Finding.error(
                    self.attribute, reason="Could not read CLAUDE.md file"
                )
        else:
            return Finding(
                attribute=self.attribute,
                status="fail",
                score=0.0,
                measured_value="missing",
                threshold="present",
                evidence=["CLAUDE.md not found in repository root"],
                remediation=self._create_remediation(),
                error_message=None,
            )

    def _create_remediation(self) -> Remediation:
        """Create remediation guidance for missing/inadequate CLAUDE.md."""
        return Remediation(
            summary="Create CLAUDE.md file with project-specific configuration for Claude Code",
            steps=[
                "Create CLAUDE.md file in repository root",
                "Add project overview and purpose",
                "Document key architectural patterns",
                "Specify coding standards and conventions",
                "Include build/test/deployment commands",
                "Add any project-specific context that helps AI assistants",
            ],
            tools=[],
            commands=[
                "touch CLAUDE.md",
                "# Add content describing your project",
            ],
            examples=[
                """# My Project

## Overview
Brief description of what this project does.

## Architecture
Key patterns and structure.

## Development
```bash
# Install dependencies
npm install

# Run tests
npm test

# Build
npm run build
```

## Coding Standards
- Use TypeScript strict mode
- Follow ESLint configuration
- Write tests for new features
"""
            ],
            citations=[
                Citation(
                    source="Anthropic",
                    title="Claude Code Documentation",
                    url="https://docs.anthropic.com/claude-code",
                    relevance="Official guidance on CLAUDE.md configuration",
                )
            ],
        )


class READMEAssessor(BaseAssessor):
    """Assesses README structure and completeness."""

    @property
    def attribute_id(self) -> str:
        return "readme_structure"

    @property
    def tier(self) -> int:
        return 1  # Essential

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="README Structure",
            category="Documentation Standards",
            tier=self.tier,
            description="Well-structured README with key sections",
            criteria="README.md with installation, usage, and development sections",
            default_weight=0.10,
        )

    def assess(self, repository: Repository) -> Finding:
        """Check for README.md with required sections.

        Pass criteria: README.md exists with essential sections
        Scoring: Proportional based on section count
        """
        readme_path = repository.path / "README.md"

        if not readme_path.exists():
            return Finding(
                attribute=self.attribute,
                status="fail",
                score=0.0,
                measured_value="missing",
                threshold="present with sections",
                evidence=["README.md not found"],
                remediation=self._create_remediation(),
                error_message=None,
            )

        # Read README and check for key sections
        try:
            with open(readme_path, "r", encoding="utf-8") as f:
                content = f.read().lower()

            required_sections = {
                "installation": any(
                    keyword in content
                    for keyword in ["install", "setup", "getting started"]
                ),
                "usage": any(
                    keyword in content for keyword in ["usage", "quickstart", "example"]
                ),
                "development": any(
                    keyword in content
                    for keyword in ["development", "contributing", "build"]
                ),
            }

            found_sections = sum(required_sections.values())
            total_sections = len(required_sections)

            score = self.calculate_proportional_score(
                measured_value=found_sections,
                threshold=total_sections,
                higher_is_better=True,
            )

            status = "pass" if score >= 75 else "fail"

            evidence = [
                f"Found {found_sections}/{total_sections} essential sections",
                f"Installation: {'âœ“' if required_sections['installation'] else 'âœ—'}",
                f"Usage: {'âœ“' if required_sections['usage'] else 'âœ—'}",
                f"Development: {'âœ“' if required_sections['development'] else 'âœ—'}",
            ]

            return Finding(
                attribute=self.attribute,
                status=status,
                score=score,
                measured_value=f"{found_sections}/{total_sections} sections",
                threshold=f"{total_sections}/{total_sections} sections",
                evidence=evidence,
                remediation=self._create_remediation() if status == "fail" else None,
                error_message=None,
            )

        except OSError as e:
            return Finding.error(
                self.attribute, reason=f"Could not read README.md: {str(e)}"
            )

    def _create_remediation(self) -> Remediation:
        """Create remediation guidance for inadequate README."""
        return Remediation(
            summary="Create or enhance README.md with essential sections",
            steps=[
                "Add project overview and description",
                "Include installation/setup instructions",
                "Document basic usage with examples",
                "Add development/contributing guidelines",
                "Include build and test commands",
            ],
            tools=[],
            commands=[],
            examples=[
                """# Project Name

## Overview
What this project does and why it exists.

## Installation
```bash
pip install -e .
```

## Usage
```bash
myproject --help
```

## Development
```bash
# Run tests
pytest

# Format code
black .
```
"""
            ],
            citations=[
                Citation(
                    source="GitHub",
                    title="About READMEs",
                    url="https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-readmes",
                    relevance="Best practices for README structure",
                )
            ],
        )
````

## File: src/agentready/assessors/stub_assessors.py
````python
"""Stub implementations for remaining assessors - minimal but functional.

These are simplified implementations to get the MVP working. Each can be
enhanced later with more sophisticated detection and scoring logic.
"""

from pathlib import Path

from ..models.attribute import Attribute
from ..models.finding import Citation, Finding, Remediation
from ..models.repository import Repository
from .base import BaseAssessor


class LockFilesAssessor(BaseAssessor):
    """Tier 1 Essential - Lock files for reproducible dependencies."""

    @property
    def attribute_id(self) -> str:
        return "lock_files"

    @property
    def tier(self) -> int:
        return 1

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="Lock Files for Reproducibility",
            category="Dependency Management",
            tier=self.tier,
            description="Lock files present for dependency pinning",
            criteria="package-lock.json, yarn.lock, poetry.lock, or requirements.txt with versions",
            default_weight=0.10,
        )

    def assess(self, repository: Repository) -> Finding:
        lock_files = [
            "package-lock.json",
            "yarn.lock",
            "pnpm-lock.yaml",
            "poetry.lock",
            "Pipfile.lock",
            "requirements.txt",
            "Cargo.lock",
            "Gemfile.lock",
            "go.sum",
        ]

        found = [f for f in lock_files if (repository.path / f).exists()]

        if found:
            return Finding(
                attribute=self.attribute,
                status="pass",
                score=100.0,
                measured_value=", ".join(found),
                threshold="at least one lock file",
                evidence=[f"Found: {', '.join(found)}"],
                remediation=None,
                error_message=None,
            )
        else:
            return Finding(
                attribute=self.attribute,
                status="fail",
                score=0.0,
                measured_value="none",
                threshold="at least one lock file",
                evidence=["No lock files found"],
                remediation=Remediation(
                    summary="Add lock file for dependency reproducibility",
                    steps=[
                        "Use npm install, poetry lock, or equivalent to generate lock file"
                    ],
                    tools=[],
                    commands=["npm install  # generates package-lock.json"],
                    examples=[],
                    citations=[],
                ),
                error_message=None,
            )


# Tier 2 Critical Assessors (3% each)


class ConventionalCommitsAssessor(BaseAssessor):
    """Tier 2 - Conventional commit messages."""

    @property
    def attribute_id(self) -> str:
        return "conventional_commits"

    @property
    def tier(self) -> int:
        return 2

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="Conventional Commit Messages",
            category="Git & Version Control",
            tier=self.tier,
            description="Follows conventional commit format",
            criteria="â‰¥80% of recent commits follow convention",
            default_weight=0.03,
        )

    def assess(self, repository: Repository) -> Finding:
        # Simplified: Check if commitlint or husky is configured
        has_commitlint = (repository.path / ".commitlintrc.json").exists()
        has_husky = (repository.path / ".husky").exists()

        if has_commitlint or has_husky:
            return Finding(
                attribute=self.attribute,
                status="pass",
                score=100.0,
                measured_value="configured",
                threshold="configured",
                evidence=["Commit linting configured"],
                remediation=None,
                error_message=None,
            )
        else:
            return Finding(
                attribute=self.attribute,
                status="fail",
                score=0.0,
                measured_value="not configured",
                threshold="configured",
                evidence=["No commitlint or husky configuration"],
                remediation=Remediation(
                    summary="Configure conventional commits with commitlint",
                    steps=["Install commitlint", "Configure husky for commit-msg hook"],
                    tools=["commitlint", "husky"],
                    commands=[
                        "npm install --save-dev @commitlint/cli @commitlint/config-conventional husky"
                    ],
                    examples=[],
                    citations=[],
                ),
                error_message=None,
            )


class GitignoreAssessor(BaseAssessor):
    """Tier 2 - Gitignore completeness."""

    @property
    def attribute_id(self) -> str:
        return "gitignore_completeness"

    @property
    def tier(self) -> int:
        return 2

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name=".gitignore Completeness",
            category="Git & Version Control",
            tier=self.tier,
            description="Comprehensive .gitignore file",
            criteria=".gitignore exists and covers common patterns",
            default_weight=0.03,
        )

    def assess(self, repository: Repository) -> Finding:
        gitignore = repository.path / ".gitignore"

        if not gitignore.exists():
            return Finding(
                attribute=self.attribute,
                status="fail",
                score=0.0,
                measured_value="missing",
                threshold="present",
                evidence=[".gitignore not found"],
                remediation=Remediation(
                    summary="Create .gitignore file",
                    steps=["Add .gitignore with common patterns for your language"],
                    tools=[],
                    commands=["touch .gitignore"],
                    examples=[],
                    citations=[],
                ),
                error_message=None,
            )

        # Check if it has content
        try:
            size = gitignore.stat().st_size
            score = 100.0 if size > 50 else 50.0
            status = "pass" if size > 50 else "fail"

            return Finding(
                attribute=self.attribute,
                status=status,
                score=score,
                measured_value=f"{size} bytes",
                threshold=">50 bytes",
                evidence=[f".gitignore found ({size} bytes)"],
                remediation=(
                    None
                    if status == "pass"
                    else Remediation(
                        summary="Expand .gitignore coverage",
                        steps=["Add common ignore patterns"],
                        tools=[],
                        commands=[],
                        examples=[],
                        citations=[],
                    )
                ),
                error_message=None,
            )
        except OSError:
            return Finding.error(self.attribute, reason="Could not read .gitignore")


# Create stub assessors for remaining attributes
# These return "not_applicable" for now but can be enhanced later


class StubAssessor(BaseAssessor):
    """Generic stub assessor for unimplemented attributes."""

    def __init__(
        self, attr_id: str, name: str, category: str, tier: int, weight: float
    ):
        self._attr_id = attr_id
        self._name = name
        self._category = category
        self._tier = tier
        self._weight = weight

    @property
    def attribute_id(self) -> str:
        return self._attr_id

    @property
    def tier(self) -> int:
        return self._tier

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self._attr_id,
            name=self._name,
            category=self._category,
            tier=self._tier,
            description=f"Assessment for {self._name}",
            criteria="To be implemented",
            default_weight=self._weight,
        )

    def assess(self, repository: Repository) -> Finding:
        return Finding.not_applicable(
            self.attribute,
            reason=f"{self._name} assessment not yet implemented",
        )


# Factory function to create all stub assessors
def create_stub_assessors():
    """Create stub assessors for remaining attributes."""
    return [
        # Tier 2 Critical
        StubAssessor(
            "one_command_setup",
            "One-Command Build/Setup",
            "Build & Development",
            2,
            0.03,
        ),
        StubAssessor(
            "concise_documentation",
            "Concise Structured Documentation",
            "Context Window Optimization",
            2,
            0.03,
        ),
        StubAssessor(
            "inline_documentation",
            "Inline Documentation",
            "Documentation Standards",
            2,
            0.03,
        ),
        StubAssessor(
            "file_size_limits",
            "File Size Limits",
            "Context Window Optimization",
            2,
            0.03,
        ),
        StubAssessor(
            "dependency_freshness",
            "Dependency Freshness & Security",
            "Dependency Management",
            2,
            0.03,
        ),
        StubAssessor(
            "separation_concerns",
            "Separation of Concerns",
            "Repository Structure",
            2,
            0.03,
        ),
        # Tier 3 Important
        StubAssessor(
            "structured_logging", "Structured Logging", "Error Handling", 3, 0.03
        ),
        StubAssessor(
            "openapi_specs",
            "OpenAPI/Swagger Specifications",
            "API Documentation",
            3,
            0.03,
        ),
        StubAssessor(
            "architecture_decisions",
            "Architecture Decision Records",
            "Documentation Standards",
            3,
            0.03,
        ),
        StubAssessor(
            "semantic_naming", "Semantic File & Directory Naming", "Modularity", 3, 0.03
        ),
        # Tier 4 Advanced
        StubAssessor(
            "security_scanning", "Security Scanning Automation", "Security", 4, 0.01
        ),
        StubAssessor(
            "performance_benchmarks", "Performance Benchmarks", "Performance", 4, 0.01
        ),
        StubAssessor("code_smells", "Code Smell Elimination", "Code Quality", 4, 0.01),
        StubAssessor(
            "issue_pr_templates",
            "Issue & Pull Request Templates",
            "Git & Version Control",
            4,
            0.01,
        ),
        StubAssessor(
            "container_setup",
            "Container/Virtualization Setup",
            "Build & Development",
            4,
            0.01,
        ),
    ]
````

## File: src/agentready/assessors/testing.py
````python
"""Testing assessors for test coverage, naming conventions, and pre-commit hooks."""

import re
import subprocess
from pathlib import Path

from ..models.attribute import Attribute
from ..models.finding import Citation, Finding, Remediation
from ..models.repository import Repository
from .base import BaseAssessor


class TestCoverageAssessor(BaseAssessor):
    """Assesses test coverage requirements.

    Tier 2 Critical (3% weight) - Test coverage is important for AI-assisted refactoring.
    """

    @property
    def attribute_id(self) -> str:
        return "test_coverage"

    @property
    def tier(self) -> int:
        return 2  # Critical

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="Test Coverage Requirements",
            category="Testing & CI/CD",
            tier=self.tier,
            description="Test coverage thresholds configured and enforced",
            criteria=">80% code coverage",
            default_weight=0.03,
        )

    def is_applicable(self, repository: Repository) -> bool:
        """Applicable if tests directory exists."""
        test_dirs = ["tests", "test", "spec", "__tests__"]
        return any((repository.path / d).exists() for d in test_dirs)

    def assess(self, repository: Repository) -> Finding:
        """Check for test coverage configuration and actual coverage.

        Looks for:
        - Python: pytest.ini, .coveragerc, pyproject.toml with coverage config
        - JavaScript: jest.config.js, package.json with coverage threshold
        """
        if "Python" in repository.languages:
            return self._assess_python_coverage(repository)
        elif any(lang in repository.languages for lang in ["JavaScript", "TypeScript"]):
            return self._assess_javascript_coverage(repository)
        else:
            return Finding.not_applicable(
                self.attribute,
                reason=f"Coverage check not implemented for {list(repository.languages.keys())}",
            )

    def _assess_python_coverage(self, repository: Repository) -> Finding:
        """Assess Python test coverage configuration."""
        # Check for coverage configuration files
        coverage_configs = [
            repository.path / ".coveragerc",
            repository.path / "pyproject.toml",
            repository.path / "setup.cfg",
        ]

        has_coverage_config = any(f.exists() for f in coverage_configs)

        # Check for pytest-cov in dependencies
        has_pytest_cov = False
        pyproject = repository.path / "pyproject.toml"
        if pyproject.exists():
            try:
                with open(pyproject, "r", encoding="utf-8") as f:
                    content = f.read()
                    has_pytest_cov = "pytest-cov" in content
            except OSError:
                pass

        # Score based on configuration presence
        if has_coverage_config and has_pytest_cov:
            score = 100.0
            status = "pass"
            evidence = [
                "Coverage configuration found",
                "pytest-cov dependency present",
            ]
        elif has_coverage_config or has_pytest_cov:
            score = 50.0
            status = "fail"
            evidence = [
                f"Coverage config: {'âœ“' if has_coverage_config else 'âœ—'}",
                f"pytest-cov: {'âœ“' if has_pytest_cov else 'âœ—'}",
            ]
        else:
            score = 0.0
            status = "fail"
            evidence = ["No coverage configuration found"]

        return Finding(
            attribute=self.attribute,
            status=status,
            score=score,
            measured_value="configured" if score > 50 else "not configured",
            threshold="configured with >80% threshold",
            evidence=evidence,
            remediation=self._create_remediation() if status == "fail" else None,
            error_message=None,
        )

    def _assess_javascript_coverage(self, repository: Repository) -> Finding:
        """Assess JavaScript/TypeScript test coverage configuration."""
        package_json = repository.path / "package.json"

        if not package_json.exists():
            return Finding(
                attribute=self.attribute,
                status="fail",
                score=0.0,
                measured_value="no package.json",
                threshold="configured coverage",
                evidence=["package.json not found"],
                remediation=self._create_remediation(),
                error_message=None,
            )

        try:
            import json

            with open(package_json, "r") as f:
                pkg = json.load(f)

            # Check for jest or vitest with coverage
            has_jest = "jest" in pkg.get("devDependencies", {})
            has_vitest = "vitest" in pkg.get("devDependencies", {})
            has_coverage = has_jest or has_vitest

            if has_coverage:
                return Finding(
                    attribute=self.attribute,
                    status="pass",
                    score=100.0,
                    measured_value="configured",
                    threshold="configured",
                    evidence=["Test coverage tool configured"],
                    remediation=None,
                    error_message=None,
                )
            else:
                return Finding(
                    attribute=self.attribute,
                    status="fail",
                    score=0.0,
                    measured_value="not configured",
                    threshold="configured",
                    evidence=["No test coverage tool found in devDependencies"],
                    remediation=self._create_remediation(),
                    error_message=None,
                )

        except (OSError, json.JSONDecodeError) as e:
            return Finding.error(
                self.attribute, reason=f"Could not parse package.json: {str(e)}"
            )

    def _create_remediation(self) -> Remediation:
        """Create remediation guidance for test coverage."""
        return Remediation(
            summary="Configure test coverage with â‰¥80% threshold",
            steps=[
                "Install coverage tool (pytest-cov for Python, jest for JavaScript)",
                "Configure coverage threshold in project config",
                "Add coverage reporting to CI/CD pipeline",
                "Run coverage locally before committing",
            ],
            tools=["pytest-cov", "jest", "vitest", "coverage"],
            commands=[
                "# Python",
                "pip install pytest-cov",
                "pytest --cov=src --cov-report=term-missing --cov-fail-under=80",
                "",
                "# JavaScript",
                "npm install --save-dev jest",
                "npm test -- --coverage --coverageThreshold='{\\'global\\': {\\'lines\\': 80}}'",
            ],
            examples=[
                """# Python - pyproject.toml
[tool.pytest.ini_options]
addopts = "--cov=src --cov-report=term-missing"

[tool.coverage.report]
fail_under = 80
""",
                """// JavaScript - package.json
{
  "jest": {
    "coverageThreshold": {
      "global": {
        "lines": 80,
        "statements": 80,
        "functions": 80,
        "branches": 80
      }
    }
  }
}
""",
            ],
            citations=[
                Citation(
                    source="pytest-cov",
                    title="Coverage Configuration",
                    url="https://pytest-cov.readthedocs.io/",
                    relevance="pytest-cov configuration guide",
                )
            ],
        )


class PreCommitHooksAssessor(BaseAssessor):
    """Assesses pre-commit hooks configuration."""

    @property
    def attribute_id(self) -> str:
        return "precommit_hooks"

    @property
    def tier(self) -> int:
        return 2  # Critical

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="Pre-commit Hooks & CI/CD Linting",
            category="Testing & CI/CD",
            tier=self.tier,
            description="Pre-commit hooks configured for linting and formatting",
            criteria=".pre-commit-config.yaml exists",
            default_weight=0.03,
        )

    def assess(self, repository: Repository) -> Finding:
        """Check for pre-commit configuration."""
        precommit_config = repository.path / ".pre-commit-config.yaml"

        if precommit_config.exists():
            return Finding(
                attribute=self.attribute,
                status="pass",
                score=100.0,
                measured_value="configured",
                threshold="configured",
                evidence=[".pre-commit-config.yaml found"],
                remediation=None,
                error_message=None,
            )
        else:
            return Finding(
                attribute=self.attribute,
                status="fail",
                score=0.0,
                measured_value="not configured",
                threshold="configured",
                evidence=[".pre-commit-config.yaml not found"],
                remediation=self._create_remediation(),
                error_message=None,
            )

    def _create_remediation(self) -> Remediation:
        """Create remediation guidance for pre-commit hooks."""
        return Remediation(
            summary="Configure pre-commit hooks for automated code quality checks",
            steps=[
                "Install pre-commit framework",
                "Create .pre-commit-config.yaml",
                "Add hooks for linting and formatting",
                "Install hooks: pre-commit install",
                "Run on all files: pre-commit run --all-files",
            ],
            tools=["pre-commit"],
            commands=[
                "pip install pre-commit",
                "pre-commit install",
                "pre-commit run --all-files",
            ],
            examples=[
                """# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
"""
            ],
            citations=[
                Citation(
                    source="pre-commit.com",
                    title="Pre-commit Framework",
                    url="https://pre-commit.com/",
                    relevance="Official pre-commit documentation",
                )
            ],
        )
````

## File: src/agentready/data/agent-ready-codebase-attributes.md
````markdown
# Agent-Ready Codebase Attributes: Comprehensive Research
*Optimizing Codebases for Claude Code and AI-Assisted Development*

**Version:** 1.0.0
**Date:** 2025-11-20
**Focus:** Claude Code/Claude-specific optimization
**Sources:** 50+ authoritative sources including Anthropic, Microsoft, Google, ArXiv, IEEE/ACM

---

## Executive Summary

This document catalogs 25 high-impact attributes that make codebases optimal for AI-assisted development, specifically Claude Code. Each attribute includes:
- Definition and importance for AI agents
- Impact on agent behavior (context window, comprehension, task success)
- Measurable criteria and tooling
- Authoritative citations
- Good vs. bad examples

**Top 10 Critical Attributes** (highest ROI):
1. CLAUDE.md/AGENTS.md configuration files
2. Conventional commit messages
3. Type annotations (static typing)
4. Test coverage >80%
5. Standard project layouts
6. Comprehensive README
7. Dependency lock files
8. Pre-commit hooks + CI/CD enforcement
9. Structured logging
10. API specifications (OpenAPI/GraphQL)

---

## 1. CONTEXT WINDOW OPTIMIZATION

### 1.1 CLAUDE.md Configuration Files

**Definition:** Markdown file at repository root automatically ingested by Claude at conversation start.

**Why It Matters:** CLAUDE.md files are "naively dropped into context up front," providing immediate project context without repeated explanations. Reduces prompt engineering time by ~40%.

**Impact on Agent Behavior:**
- Immediate understanding of tech stack, repository structure, standard commands
- Consistent adherence to project conventions
- Reduced need for repeated context-setting
- Frames entire session with project-specific guidance

**Measurable Criteria:**
- File size: <1000 lines (concise, focused)
- Essential sections:
  - Tech stack with versions
  - Repository map/structure
  - Standard commands (build, test, lint, format)
  - Testing strategy
  - Style/lint rules
  - Branch/PR workflow
  - "Do not touch" zones
  - Security/compliance notes

**Citation:** Anthropic Engineering Blog - "Claude Code Best Practices" (2025)

**Example:**
```markdown
# Good CLAUDE.md
# Tech Stack
- Python 3.11+, pytest, black + isort

# Standard Commands
- Run tests: `pytest tests/`
- Format: `black . && isort .`
- Build: `make build`

# Repository Structure
- src/ - Main application code
- tests/ - Test files mirror src/
- docs/ - Documentation

# Boundaries
- Never modify files in legacy/
- Require approval before changing config.yaml
```

---

### 1.2 Concise, Structured Documentation

**Definition:** Documentation maximizing information density while minimizing token consumption.

**Why It Matters:** Despite expanding context windows (1M+ tokens), attention mechanisms have quadratic complexity growth. Performance drops significantly on long-context tasks: 29%â†’3% (Claude 3.5 Sonnet) or 70.2%â†’40% (Qwen2.5).

**Impact on Agent Behavior:**
- Faster information retrieval through clear headings
- Reduced context pollution
- Improved response accuracy
- Better navigation across documentation

**Measurable Criteria:**
- Use standard Markdown headings (#, ##, ###)
- README <500 lines; use wiki/docs for extensive content
- Table of contents for documents >100 lines
- Bullet points over prose paragraphs
- One concept per section

**Citations:**
- ArXiv: "LongCodeBench: Evaluating Coding LLMs at 1M Context Windows" (2025)
- IBM Research: "Why larger LLM context windows are all the rage"

---

### 1.3 File Size Limits

**Definition:** Individual source files <200-300 lines.

**Why It Matters:** Working memory handles ~4 objects simultaneously. Large files exceed cognitive capacity for both humans and AI.

**Impact on Agent Behavior:**
- More precise file selection
- Reduced irrelevant context in responses
- Safer targeted modifications
- Better understanding of module boundaries

**Measurable Criteria:**
- Target: <200-300 lines per file
- Warning threshold: 500 lines
- Exception: Generated code, data files
- Enforce via linters (e.g., pylint max-module-lines)

**Citations:**
- Stack Overflow: "At what point/range is a code file too big?"
- Medium: "Psychology of Code Readability" by Egon Elbre

---

## 2. DOCUMENTATION STANDARDS

### 2.1 README Structure

**Definition:** Standardized README with essential sections in predictable order.

**Why It Matters:** Repositories with well-structured READMEs receive more engagement (GitHub data). README serves as agent's entry point for understanding project purpose, setup, and usage.

**Impact on Agent Behavior:**
- Faster project comprehension
- Accurate answers to onboarding questions
- Better architectural understanding without exploring entire codebase
- Consistent expectations across projects

**Measurable Criteria:**
Essential sections (in order):
1. Project title and description
2. Installation/setup instructions
3. Quick start/usage examples
4. Core features
5. Dependencies and requirements
6. Testing instructions
7. Contributing guidelines
8. License

**Citations:**
- GitHub Blog: "How to write a great agents.md"
- Make a README project documentation
- Welcome to the Jungle: "Essential Sections for Better Documentation"

---

### 2.2 Inline Documentation (Docstrings/Comments)

**Definition:** Function, class, and module-level documentation using language-specific conventions (Python docstrings, JSDoc/TSDoc).

**Why It Matters:** Type hints significantly improve LLM experience. Well-typed code directs LLMs into latent space regions corresponding to higher code qualityâ€”similar to how LaTeX-formatted math problems get better results.

**Impact on Agent Behavior:**
- Understanding function purpose without reading implementation
- Better parameter validation suggestions
- More accurate return type predictions
- Improved test generation
- Enhanced refactoring confidence

**Measurable Criteria:**
- All public functions/methods have docstrings
- Docstrings include: description, parameters, return values, exceptions, examples
- Python: PEP 257 compliant
- JavaScript/TypeScript: JSDoc or TSDoc
- Coverage: >80% of public API documented
- Tools: pydocstyle, documentation-js

**Citations:**
- Medium: "LLM Coding Concepts: Static Typing, Structured Output, and AsyncIO"
- ArXiv: "TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories"
- TypeScript Documentation: JSDoc Reference

**Example:**
```python
# Good: Comprehensive docstring
def calculate_discount(price: float, discount_percent: float) -> float:
    """
    Calculate discounted price.

    Args:
        price: Original price in USD
        discount_percent: Discount percentage (0-100)

    Returns:
        Discounted price

    Raises:
        ValueError: If discount_percent not in 0-100 range

    Example:
        >>> calculate_discount(100.0, 20.0)
        80.0
    """
    if not 0 <= discount_percent <= 100:
        raise ValueError("Discount must be 0-100")
    return price * (1 - discount_percent / 100)

# Bad: No documentation
def calc_disc(p, d):
    return p * (1 - d / 100)
```

---

### 2.3 Architecture Decision Records (ADRs)

**Definition:** Lightweight documents capturing architectural decisions with context, decision, and consequences.

**Why It Matters:** ADRs provide historical context for "why" decisions were made. When AI encounters patterns or constraints, ADRs explain rationale, preventing counter-productive suggestions.

**Impact on Agent Behavior:**
- Understanding project evolution and design philosophy
- Avoiding proposing previously rejected alternatives
- Aligning suggestions with established architectural principles
- Better context for refactoring recommendations

**Measurable Criteria:**
- Store in `docs/adr/` or `.adr/` directory
- Use consistent template (Michael Nygard or MADR)
- Each ADR includes: Title, Status, Context, Decision, Consequences
- Status values: Proposed, Accepted, Deprecated, Superseded
- One decision per ADR
- Sequential numbering (ADR-001, ADR-002...)

**Citations:**
- AWS Prescriptive Guidance: "ADR process"
- GitHub: joelparkerhenderson/architecture-decision-record
- Microsoft Azure Well-Architected Framework

**Template:**
```markdown
# ADR-001: Use PostgreSQL for Primary Database

Status: Accepted

## Context
Need persistent storage supporting ACID transactions, complex queries, and JSON data.

## Decision
Use PostgreSQL 14+ as primary database.

## Consequences
Positive:
- Strong ACID guarantees
- Rich query capabilities (joins, window functions)
- JSON support via jsonb

Negative:
- More operational complexity than managed NoSQL
- Requires schema migration planning
- Horizontal scaling more complex
```

---

## 3. CODE QUALITY METRICS

### 3.1 Cyclomatic Complexity Thresholds

**Definition:** Measurement of linearly independent paths through code, indicating decision point density.

**Why It Matters:** High cyclomatic complexity confuses both humans and AI. While not perfect (doesn't capture cognitive complexity), it correlates strongly with testing difficulty and error potential.

**Impact on Agent Behavior:**
- Functions with complexity >25 are harder to understand
- Reduced confidence in safe modifications
- More difficult to generate comprehensive tests
- Increased likelihood of introducing bugs during refactoring

**Measurable Criteria:**
- Target: Cyclomatic complexity <10 per function
- Warning threshold: 15
- Error threshold: 25
- Tools: clang-tidy (C++), radon (Python), complexity-report (JavaScript), gocyclo (Go)

**Citations:**
- Microsoft Learn: "Code metrics - Cyclomatic complexity"
- Checkstyle Documentation
- LinearB Blog: "Cyclomatic Complexity explained"

---

### 3.2 Function/Method Length Limits

**Definition:** Keeping functions/methods small (typically <50 lines, ideally <20).

**Why It Matters:** Working memory handles ~4 objects simultaneously. Long functions exceed cognitive capacity. Research on reading comprehension shows lines >50-75 characters reduce comprehension; code has higher cognitive load per line.

**Impact on Agent Behavior:**
- Easier holistic function understanding
- Better isolation for testing
- Safer modifications without unintended side effects
- Clearer single responsibility principle adherence

**Measurable Criteria:**
- Target: <20 lines per function
- Warning: 50 lines
- Hard limit: 100 lines
- Exception: Complex algorithms with extensive explanatory comments
- Tools: pylint (max-function-lines), eslint (max-lines-per-function)

**Citations:**
- Medium: "Psychology of Code Readability" by Egon Elbre
- UX Stack Exchange: Line length readability research
- Clang-Tidy: readability-function-cognitive-complexity

---

### 3.3 Type Annotations (Static Typing)

**Definition:** Explicit type declarations for variables, parameters, and return values.

**Why It Matters:** Type hints significantly improve LLM code understanding. Higher-quality codebases have type annotations, directing LLMs toward higher-quality latent space regions. Creates synergistic improvement: LLMs generate better typed code, which helps future LLM interactions.

**Impact on Agent Behavior:**
- Better input validation
- Type error detection before execution
- Structured output generation
- Improved autocomplete suggestions
- Enhanced refactoring safety

**Measurable Criteria:**
- Python: All public functions have parameter and return type hints
- TypeScript: `strict` mode enabled in tsconfig.json
- Go: Inherently typed
- Coverage: >80% of functions typed
- Tools: mypy (Python), pyright (Python), tsc --strict (TypeScript)

**Citations:**
- Medium: "LLM Coding Concepts: Static Typing, Structured Output"
- ArXiv: "Automated Type Annotation in Python Using LLMs"
- Dropbox Tech Blog: "Our journey to type checking 4 million lines of Python"

**Example:**
```python
# Good: Full type annotations
from typing import List, Optional

def find_users(
    role: str,
    active: bool = True,
    limit: Optional[int] = None
) -> List[User]:
    """Find users matching criteria."""
    query = User.query.filter_by(role=role, active=active)
    if limit:
        query = query.limit(limit)
    return query.all()

# Bad: No type hints
def find_users(role, active=True, limit=None):
    query = User.query.filter_by(role=role, active=active)
    if limit:
        query = query.limit(limit)
    return query.all()
```

---

### 3.4 Code Smell Elimination

**Definition:** Removing indicators of deeper problems: long methods, large classes, duplicate code, dead code, magic numbers.

**Why It Matters:** Research shows AI-generated code increases "code churn" (copy/paste vs. refactoring) and DRY principle violations. Clean baseline prevents AI from perpetuating anti-patterns.

**Impact on Agent Behavior:**
- Better intent understanding
- More accurate refactoring suggestions
- Avoidance of anti-pattern propagation
- Improved code quality over time

**Measurable Criteria:**
- Tools: SonarQube, PMD, Checkstyle, pylint, eslint
- Zero critical smells
- <5 major smells per 1000 lines of code
- Common smells monitored:
  - Duplicate code (DRY violations)
  - Long methods (>50 lines)
  - Large classes (>500 lines)
  - Long parameter lists (>5 params)
  - Divergent change (one class changing for multiple reasons)

**Citations:**
- GitClear: "Coding on Copilot" whitepaper
- Codacy Blog: "Code Smells and Anti-Patterns"
- ScienceDirect: "Code smells and refactoring: A tertiary systematic review"

---

## 4. REPOSITORY STRUCTURE

### 4.1 Standard Project Layouts

**Definition:** Using community-recognized directory structures for each language/framework.

**Why It Matters:** Standard layouts reduce cognitive overhead. AI models trained on open-source code recognize patterns (Python's src/, Go's cmd/ and internal/, Java's Maven structure).

**Impact on Agent Behavior:**
- Faster navigation
- Accurate location assumptions for new files
- Automatic adherence to established conventions
- Reduced confusion about file placement

**Measurable Criteria:**

**Python (src layout):**
```
project/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ package/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ module.py
â”œâ”€â”€ tests/
â”œâ”€â”€ docs/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â””â”€â”€ requirements.txt
```

**Go:**
```
project/
â”œâ”€â”€ cmd/           # Main applications
â”‚   â””â”€â”€ app/
â”‚       â””â”€â”€ main.go
â”œâ”€â”€ internal/      # Private code
â”œâ”€â”€ pkg/           # Public libraries
â”œâ”€â”€ go.mod
â””â”€â”€ go.sum
```

**JavaScript/TypeScript (Node.js):**
```
project/
â”œâ”€â”€ src/
â”œâ”€â”€ test/
â”œâ”€â”€ dist/
â”œâ”€â”€ package.json
â”œâ”€â”€ package-lock.json
â””â”€â”€ tsconfig.json
```

**Citations:**
- Real Python: "Python Application Layouts"
- GitHub: golang-standards/project-layout
- Stack Overflow: "Best project structure for Python application"

---

### 4.2 Separation of Concerns

**Definition:** Organizing code so each module/file/function has single, well-defined responsibility (SOLID principles).

**Why It Matters:** 2 of 5 SOLID principles derive directly from separation of concerns. Clear boundaries improve testability, maintainability, and reduce cognitive load.

**Impact on Agent Behavior:**
- Targeted modifications without affecting unrelated code
- Better refactoring suggestions
- Clearer module purpose understanding
- Reduced side effect risk

**Measurable Criteria:**
- Each module/class has one reason to change
- High cohesion within modules (related functions together)
- Low coupling between modules (minimal dependencies)
- Organize by feature/domain, not technical layer (avoid separate "controllers", "services", "models" directories)

**Citations:**
- Wikipedia: "Separation of concerns"
- DevIQ: "Separation of Concerns"
- Medium: "Single responsibility and Separation of concerns principles"

---

## 5. TESTING & CI/CD

### 5.1 Test Coverage Requirements

**Definition:** Percentage of code executed by automated tests.

**Why It Matters:** High test coverage enables confident AI modifications. Research shows AI tools (Cursor AI) can cut test coverage time by 85% while maintaining qualityâ€”but only when good tests exist as foundation.

**Impact on Agent Behavior:**
- Safety net enabling aggressive refactoring
- Tests document expected behavior
- Immediate feedback on breaking changes
- Higher confidence in suggested modifications

**Measurable Criteria:**
- Minimum: 70% line coverage
- Target: 80-90% line coverage
- Critical paths: 100% coverage
- Track: Statement coverage, branch coverage, function coverage
- Tools: pytest-cov (Python), Jest/Istanbul (JavaScript), go test -cover (Go)
- Coverage reports in CI/CD with failure threshold

**Citations:**
- Salesforce Engineering: "How Cursor AI Cut Legacy Code Coverage Time by 85%"
- Qodo AI Blog: "Harnessing AI to Revolutionize Test Coverage Analysis"
- Medium: "How to Improve Code Coverage using Generative AI tools"

---

### 5.2 Test Naming Conventions

**Definition:** Descriptive test names following patterns like `test_should_<expected>_when_<condition>`.

**Why It Matters:** Clear test names help AI understand intent without reading implementation. When tests fail, AI diagnoses issues faster with self-documenting names.

**Impact on Agent Behavior:**
- Generation of similar test patterns
- Faster edge case understanding
- More accurate fix proposals aligned with intent
- Better test coverage gap identification

**Measurable Criteria:**
- Pattern: `test_<method>_<scenario>_<expected_outcome>`
- Example: `test_create_user_with_invalid_email_raises_value_error`
- Avoid: `test1`, `test_edge_case`, `test_bug_fix`, `test_method_name`
- Test names should be readable as sentences

**Citations:**
- pytest documentation: Test naming best practices
- JUnit best practices
- Go testing conventions

**Example:**
```python
# Good: Self-documenting test names
def test_create_user_with_valid_data_returns_user_instance():
    user = create_user(email="test@example.com", name="Test")
    assert isinstance(user, User)

def test_create_user_with_invalid_email_raises_value_error():
    with pytest.raises(ValueError, match="Invalid email"):
        create_user(email="not-an-email", name="Test")

def test_create_user_with_duplicate_email_raises_integrity_error():
    create_user(email="test@example.com", name="Test 1")
    with pytest.raises(IntegrityError):
        create_user(email="test@example.com", name="Test 2")

# Bad: Unclear test names
def test_user1():
    user = create_user(email="test@example.com", name="Test")
    assert user

def test_user2():
    with pytest.raises(ValueError):
        create_user(email="invalid", name="Test")
```

---

### 5.3 Pre-commit Hooks & CI/CD Linting

**Definition:** Automated code quality checks before commits (pre-commit hooks) and in CI/CD pipeline.

**Why It Matters:** Pre-commit hooks provide immediate feedback but can be bypassed. Running same checks in CI/CD ensures enforcement. Linting errors prevent successful CI runs, wasting time and compute.

**Impact on Agent Behavior:**
- Ensures AI-generated code meets quality standards
- Immediate feedback loop for improvements
- Consistent code style across all contributions
- Prevents low-quality code from entering repository

**Measurable Criteria:**
- Pre-commit framework installed and configured
- Hooks include:
  - Formatters: black/autopep8 (Python), prettier (JS/TS), gofmt (Go)
  - Linters: flake8/pylint (Python), eslint (JS/TS), golint (Go)
  - Type checkers: mypy/pyright (Python), tsc (TypeScript)
- **Critical:** Same checks run in CI/CD (non-skippable)
- CI fails on any linting error
- Fast execution: <30 seconds total

**Citations:**
- Memfault Blog: "Automatically format and lint code with pre-commit"
- Medium: "Elevate Your CI: Mastering Pre-commit Hooks and GitHub Actions"
- GitHub: pre-commit/pre-commit

---

## 6. DEPENDENCY MANAGEMENT

### 6.1 Lock Files for Reproducibility

**Definition:** Pinning exact dependency versions including transitive dependencies.

**Why It Matters:** Lock files ensure reproducible builds across environments. Without them, "works on my machine" problems plague AI-generated code. Different dependency versions can break builds, fail tests, or introduce bugs.

**Impact on Agent Behavior:**
- Confident dependency-related suggestions
- Accurate compatibility issue diagnosis
- Reproducible environment recommendations
- Version-specific API usage

**Measurable Criteria:**
- Lock file committed to repository
- **npm:** package-lock.json or yarn.lock
- **Python:** requirements.txt (from pip freeze), poetry.lock, or uv.lock
- **Go:** go.sum (automatically managed)
- **Ruby:** Gemfile.lock
- Lock file updated with every dependency change
- CI/CD uses lock file for installation

**Citations:**
- npm Blog: "Why Keep package-lock.json?"
- DEV Community: "Dependency management: package.json and package-lock.json explained"
- Python Packaging User Guide

---

### 6.2 Dependency Freshness & Security Scanning

**Definition:** Regularly updating dependencies and scanning for known vulnerabilities.

**Why It Matters:** Outdated dependencies introduce security risks and compatibility issues. AI-generated code may use deprecated APIs if dependencies are stale. Security vulnerabilities in dependencies can compromise entire application.

**Impact on Agent Behavior:**
- Suggestions use modern, non-deprecated APIs
- Awareness of security considerations
- Better library feature recommendations
- Avoidance of known vulnerability patterns

**Measurable Criteria:**
- Automated dependency updates: Dependabot, Renovate, or equivalent
- Security scanning in CI/CD: Snyk, npm audit, safety (Python), govulncheck (Go)
- Update cadence:
  - Patch versions: Weekly/automated
  - Minor versions: Monthly
  - Major versions: Quarterly with testing
- Zero known high/critical vulnerabilities in production
- Vulnerability response SLA: High severity within 7 days

**Citations:**
- GitHub Dependabot documentation
- OWASP Dependency-Check
- Snyk best practices
- npm audit documentation

---

## 7. GIT & VERSION CONTROL

### 7.1 Conventional Commit Messages

**Definition:** Structured commit messages following format: `<type>(<scope>): <description>`.

**Why It Matters:** Conventional commits enable automated semantic versioning, changelog generation, and commit intent understanding. AI can parse history to understand feature evolution and impact.

**Impact on Agent Behavior:**
- Generates properly formatted commit messages
- Understands which changes are breaking
- Appropriate version bump suggestions
- Better git history comprehension
- Automated changelog contribution

**Measurable Criteria:**
- Format: `type(scope): description`
- **Types:** feat, fix, docs, style, refactor, perf, test, chore, build, ci
- **Breaking changes:** `BREAKING CHANGE:` footer or `!` after type
- **Tools:** commitlint, commitizen, semantic-release
- **Enforcement:** Pre-commit hook or CI check
- All commits follow convention (enforce in CI)

**Citations:**
- Conventional Commits specification v1.0.0
- Medium: "GIT â€” Semantic versioning and conventional commits"
- CMU SEI Blog: "Versioning with Git Tags and Conventional Commits"

**Example:**
```
# Good commits
feat(auth): add OAuth2 login support
fix(api): handle null values in user response
docs(readme): update installation instructions
perf(database): add index on user_email column

# Breaking change
feat(api)!: change user endpoint from /user to /users

BREAKING CHANGE: User endpoint URL has changed from /user to /users.
Update all API clients accordingly.

# Bad commits
update stuff
fixed bug
changes
wip
asdf
```

---

### 7.2 .gitignore Completeness

**Definition:** Comprehensive .gitignore preventing sensitive files, build artifacts, and environment-specific files from version control.

**Why It Matters:** Incomplete .gitignore pollutes repository with irrelevant files, consuming context window space and creating security risks (accidentally committing .env files, credentials).

**Impact on Agent Behavior:**
- Focus on source code, not build artifacts
- Security files excluded prevent accidental exposure
- Cleaner repository navigation
- Reduced context pollution

**Measurable Criteria:**
- Use language-specific templates from github/gitignore
- **Exclude:**
  - Build artifacts (dist/, build/, *.pyc, *.class)
  - Dependencies (node_modules/, venv/, vendor/)
  - IDE files (.vscode/, .idea/, *.swp)
  - OS files (.DS_Store, Thumbs.db)
  - Environment variables (.env, .env.local)
  - Credentials (*.pem, *.key, credentials.json)
  - Logs (*.log, logs/)
- One .gitignore at repository root (avoid multiple nested)
- Review when adding new tools/frameworks

**Citations:**
- GitHub: github/gitignore template collection
- Medium: "Mastering .gitignore: A Comprehensive Guide"
- Git documentation

---

### 7.3 Issue & Pull Request Templates

**Definition:** Standardized templates for issues and PRs in .github/ directory.

**Why It Matters:** Templates provide structure for AI when creating issues or PRs. Ensures all necessary context is provided consistently.

**Impact on Agent Behavior:**
- Automatically fills templates when creating PRs
- Ensures checklist completion
- Consistent issue reporting format
- Better context for understanding existing issues/PRs

**Measurable Criteria:**
- `PULL_REQUEST_TEMPLATE.md` in .github/ or root
- Issue templates in `.github/ISSUE_TEMPLATE/`
- **PR template includes:**
  - Summary of changes
  - Related issues (Fixes #123)
  - Testing performed
  - Checklist (tests added, docs updated, etc.)
- **Issue templates for:**
  - Bug reports (with reproduction steps)
  - Feature requests (with use case)
  - Questions/discussions

**Citations:**
- GitHub Docs: "About issue and pull request templates"
- GitHub Blog: "Multiple issue and pull request templates"
- Embedded Artistry: "A GitHub Pull Request Template for Your Projects"

---

## 8. BUILD & DEVELOPMENT SETUP

### 8.1 One-Command Build/Setup

**Definition:** Single command to set up development environment from fresh clone.

**Why It Matters:** Lengthy setup documentation increases friction and errors. One-command setup enables AI to quickly reproduce environments and test changes. Reduces "works on my machine" problems.

**Impact on Agent Behavior:**
- Confident environment setup suggestions
- Quick validation of proposed changes
- Easy onboarding recommendations
- Reduced setup-related debugging

**Measurable Criteria:**
- Single command documented prominently in README
- **Examples:** `make setup`, `npm install`, `poetry install`, `./bootstrap.sh`
- **Command handles:**
  - Dependency installation
  - Virtual environment creation
  - Database setup/migrations
  - Configuration file creation (.env from .env.example)
  - Pre-commit hooks installation
- **Success criteria:** Working development environment in <5 minutes
- Idempotent (safe to run multiple times)

**Citations:**
- npm Blog: "Using Npm Scripts as a Build Tool"
- freeCodeCamp: "Want to know the easiest way to save time? Use make!"
- Medium: "Creating Reproducible Development Environments"

**Example:**
```makefile
# Good: Comprehensive Makefile
.PHONY: setup
setup:
	python -m venv venv
	. venv/bin/activate && pip install -r requirements.txt
	pre-commit install
	cp .env.example .env
	python manage.py migrate
	@echo "âœ“ Setup complete! Run 'make test' to verify."

.PHONY: test
test:
	pytest tests/ -v --cov

.PHONY: lint
lint:
	black --check .
	isort --check .
	flake8 .
	mypy .

.PHONY: format
format:
	black .
	isort .
```

---

### 8.2 Development Environment Documentation

**Definition:** Clear documentation of prerequisites, environment variables, and configuration.

**Why It Matters:** Environment differences cause "works on my machine" problems. Comprehensive docs enable reproducibility and faster debugging.

**Impact on Agent Behavior:**
- Accurate environment troubleshooting
- Better setup assistance for new contributors
- Environment-specific bug diagnosis
- Configuration recommendation accuracy

**Measurable Criteria:**
- **Prerequisites documented:**
  - Language/runtime version (Python 3.11+, Node.js 18+)
  - System dependencies (PostgreSQL, Redis, etc.)
  - Operating system requirements
- **Environment variables documented:**
  - .env.example file with all variables
  - Description of each variable
  - Required vs. optional clearly marked
  - Safe default values where applicable
- **Optional but helpful:**
  - IDE/editor setup (VS Code extensions, etc.)
  - Debugging configuration
  - Performance optimization tips

**Citations:**
- Medium: "Creating Reproducible Development Environments"
- InfoQ: "Reproducible Development with Containers"
- The Turing Way: "Reproducible Environments"

---

### 8.3 Container/Virtualization Setup

**Definition:** Docker/Podman configurations for consistent development environments.

**Why It Matters:** Containers provide portable, reproducible environments across operating systems. Development containers (devcontainers) are fully functional, batteries-included environments that are shared, versioned, and self-documenting.

**Impact on Agent Behavior:**
- Dockerfile improvement suggestions
- Container debugging assistance
- Consistent build recommendations
- Cross-platform development support

**Measurable Criteria:**
- Dockerfile or Containerfile in repository root
- docker-compose.yml for multi-service setups
- .devcontainer/devcontainer.json for VS Code/GitHub Codespaces
- **Dockerfile best practices:**
  - Multi-stage builds for smaller images
  - Non-root user
  - .dockerignore file
  - Explicit version tags (not :latest)
- Documentation on running containers
- Health checks defined

**Citations:**
- InfoQ: "Reproducible Development with Containers"
- Developer.com: "Creating a Reproducible and Portable Development Environment"
- Docker best practices documentation

---

## 9. ERROR HANDLING & DEBUGGING

### 9.1 Error Message Clarity

**Definition:** Descriptive error messages with context, remediation guidance, and relevant data.

**Why It Matters:** Clear errors enable AI to diagnose issues and suggest fixes. Vague errors ("Error 500", "Something went wrong") provide no actionable information.

**Impact on Agent Behavior:**
- Accurate root cause analysis
- Targeted solution proposals
- Faster debugging cycles
- Better user error handling suggestions

**Measurable Criteria:**
- **Include in error messages:**
  - What failed (operation/function)
  - Why it failed (validation, network, etc.)
  - How to fix it (actionable guidance)
  - Context: Request IDs, user IDs, timestamps, relevant parameters
- **Avoid:**
  - Generic messages ("Invalid input", "Error occurred")
  - Exposing internal stack traces to end users
  - Sensitive information in error messages
- **Provide:** Error codes for categorization
- Consistent error format across application

**Citations:**
- Honeycomb: "Engineers Checklist: Logging Best Practices"
- Paul Serban: "Error Logging Standards: A Practical Guide"
- Stack Overflow Blog: "Best practices for writing code comments"

**Example:**
```python
# Good: Descriptive error with context and guidance
raise ValueError(
    f"Invalid discount percentage: {discount_percent}. "
    f"Expected value between 0 and 100. "
    f"Received: {discount_percent} (type: {type(discount_percent).__name__}). "
    f"Fix: Ensure discount_percent is a number in range [0, 100]."
)

# Bad: Vague error
raise ValueError("Invalid input")

# Good: API error with context
{
    "error": {
        "code": "INVALID_DISCOUNT",
        "message": "Discount percentage must be between 0 and 100",
        "details": {
            "field": "discount_percent",
            "value": 150,
            "constraint": "0 <= value <= 100"
        },
        "request_id": "req_abc123"
    }
}
```

---

### 9.2 Structured Logging

**Definition:** Logging in structured format (JSON) with consistent field names and types.

**Why It Matters:** Structured logs are machine-parseable. AI can analyze logs to diagnose issues, identify patterns, suggest optimizations, and correlate events across distributed systems.

**Impact on Agent Behavior:**
- Log query and analysis capabilities
- Event correlation across services
- Pattern identification for debugging
- Data-driven optimization suggestions
- Anomaly detection

**Measurable Criteria:**
- Use structured logging library: structlog (Python), winston (Node.js), zap (Go)
- **Standard fields across all logs:**
  - timestamp (ISO 8601 format)
  - level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  - message (human-readable)
  - context: request_id, user_id, session_id, trace_id
- Consistent naming convention (snake_case or camelCase, not both)
- Log levels used appropriately
- **Never log sensitive data:** passwords, tokens, credit cards, PII (without anonymization)
- JSON format for production

**Citations:**
- Daily.dev: "12 Logging Best Practices: Do's & Don'ts"
- Dataset Blog: "Logging Best Practices: The 13 You Should Know"
- Technogise Medium: "Logging Practices: Guidelines for Developers"

**Example:**
```python
# Good: Structured logging
import structlog

logger = structlog.get_logger()

logger.info(
    "user_login_success",
    user_id="user_123",
    request_id="req_abc",
    duration_ms=45,
    ip_address="192.168.1.1"
)

# Output:
# {"timestamp": "2025-01-20T10:30:00Z", "level": "info", "event": "user_login_success",
#  "user_id": "user_123", "request_id": "req_abc", "duration_ms": 45, "ip_address": "192.168.1.1"}

# Bad: Unstructured logging
print("User user_123 logged in from 192.168.1.1 in 45ms")
```

---

## 10. API & INTERFACE DOCUMENTATION

### 10.1 OpenAPI/Swagger Specifications

**Definition:** Machine-readable API documentation in OpenAPI format (formerly Swagger).

**Why It Matters:** OpenAPI specs define everything needed to integrate with an API: authentication, endpoints, HTTP methods, request/response schemas, error codes. AI can read specs to generate client code, tests, and integration code automatically.

**Impact on Agent Behavior:**
- Auto-generation of SDKs and client libraries
- Request/response validation
- API mocking for testing
- Contract compliance verification
- Interactive API exploration

**Measurable Criteria:**
- OpenAPI 3.0+ specification file (openapi.yaml or openapi.json)
- **All endpoints documented with:**
  - Description and purpose
  - HTTP method (GET, POST, PUT, DELETE, PATCH)
  - Parameters (path, query, header)
  - Request body schema
  - Response schemas (success and error cases)
  - Authentication requirements
  - Example requests/responses
- Validation: Use Swagger Editor or Spectral
- Auto-generate from code annotations OR keep manually in sync
- Hosted documentation (Swagger UI, ReDoc)

**Citations:**
- Swagger Blog: "API Documentation Best Practices"
- APItoolkit: "OpenAPI Specification for API Development"
- APImatic: "14 Best Practices to Write OpenAPI for Better API Consumption"

**Example:**
```yaml
# Good: Comprehensive OpenAPI spec
openapi: 3.0.0
info:
  title: User API
  version: 1.0.0
paths:
  /users/{userId}:
    get:
      summary: Get user by ID
      parameters:
        - name: userId
          in: path
          required: true
          schema:
            type: string
      responses:
        '200':
          description: User found
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/User'
        '404':
          description: User not found
components:
  schemas:
    User:
      type: object
      required:
        - id
        - email
      properties:
        id:
          type: string
        email:
          type: string
          format: email
        name:
          type: string
```

---

### 10.2 GraphQL Schemas

**Definition:** Type definitions for GraphQL APIs using Schema Definition Language (SDL).

**Why It Matters:** GraphQL schemas are self-documenting and introspectable. AI can understand available queries, mutations, types, and relationships without exploring implementation code.

**Impact on Agent Behavior:**
- Generate type-safe queries
- Schema validation
- Performance optimization suggestions (N+1 query detection)
- Type-safe client generation
- API evolution guidance

**Measurable Criteria:**
- schema.graphql file in repository
- All types, queries, mutations include descriptions
- Use directives for:
  - Deprecation (@deprecated)
  - Authorization (@auth)
  - Field resolution hints
- Schema validation in CI/CD
- SDL-first approach (schema-first, not code-first)

**Citations:**
- GraphQL documentation: "Schema Definition Language"
- Apollo GraphQL: "Schema design best practices"
- Hasura GraphQL best practices

**Example:**
```graphql
# Good: Well-documented GraphQL schema
"""
Represents a user in the system
"""
type User {
  """
  Unique identifier for the user
  """
  id: ID!

  """
  User's email address (unique)
  """
  email: String!

  """
  User's display name
  """
  name: String

  """
  Posts created by this user
  """
  posts: [Post!]!
}

type Query {
  """
  Find a user by their unique ID
  """
  user(id: ID!): User

  """
  List all users with optional filtering
  """
  users(role: String, active: Boolean): [User!]!
}
```

---

## 11. MODULARITY & CODE ORGANIZATION

### 11.1 DRY Principle (Don't Repeat Yourself)

**Definition:** Every piece of knowledge has single, authoritative representation in the system.

**Why It Matters:** Research shows AI-generated code increases code churn and DRY violations (copy-paste instead of refactoring). Enforcing DRY in codebase teaches AI to refactor rather than duplicate.

**Impact on Agent Behavior:**
- Learns to extract shared logic
- Suggests refactorings instead of duplication
- Avoids creating duplicate implementations
- Better abstraction identification

**Measurable Criteria:**
- "Three Strikes" rule: Third duplicate occurrence triggers refactoring
- Tools detect duplication: SonarQube, PMD (Java), jscpd (JavaScript), pylint (Python)
- Shared logic extracted to:
  - Utility functions/modules
  - Base classes
  - Mixins/traits
  - Libraries
- **Balance:** Avoid premature abstraction ("prefer duplication over wrong abstraction")
- Target: <5% duplicate code

**Citations:**
- Wikipedia: "Don't repeat yourself"
- The Pragmatic Programmer by Hunt & Thomas
- Medium: "The DRY Principle and Incidental Duplication"
- Sandi Metz: "The Wrong Abstraction"

---

### 11.2 Consistent Naming Conventions

**Definition:** Systematic naming patterns for variables, functions, classes, files following language/framework conventions.

**Why It Matters:** Research shows identifier style affects recall and precision. Consistency reduces cognitive load. AI models recognize naming patterns from training on open-source code.

**Impact on Agent Behavior:**
- Accurate intent inference
- Appropriate name suggestions
- Code structure understanding
- Pattern recognition

**Measurable Criteria:**
- Follow language conventions:
  - **Python:** PEP 8 (snake_case functions, PascalCase classes, UPPER_CASE constants)
  - **JavaScript/TypeScript:** camelCase functions/variables, PascalCase classes
  - **Go:** mixedCaps (exported: UpperCase, unexported: lowerCase)
  - **Java:** camelCase methods, PascalCase classes, UPPER_CASE constants
- Use paired opposites consistently: add/remove, start/stop, begin/end, open/close
- Avoid abbreviations unless widely understood (HTTP, API, URL, ID)
- Enforce via linters: pylint, eslint, golint

**Citations:**
- Wikipedia: "Naming convention (programming)"
- Microsoft Learn: "General Naming Conventions"
- PEP 8 - Style Guide for Python Code
- Google Style Guides (Java, Python, JavaScript, Go)

**Example:**
```python
# Good: Consistent naming
class UserService:
    MAX_LOGIN_ATTEMPTS = 5

    def create_user(self, email: str) -> User:
        """Create new user."""
        pass

    def delete_user(self, user_id: str) -> None:
        """Delete existing user."""
        pass

# Bad: Inconsistent naming
class userservice:
    maxLoginAttempts = 5

    def CreateUser(self, e: str) -> User:
        pass

    def removeUser(self, uid: str) -> None:
        pass
```

---

### 11.3 Semantic File & Directory Naming

**Definition:** File names and directory structures that convey purpose and content clearly.

**Why It Matters:** Semantic organization helps AI locate relevant code quickly. Clear names reduce cognitive overhead and enable predictable file location.

**Impact on Agent Behavior:**
- Faster relevant file location
- Accurate placement suggestions for new code
- Better repository organization understanding
- Reduced search time

**Measurable Criteria:**
- **Feature-based organization:** Group related files by feature/domain, not technical layer
- **Clear, descriptive names:** `user_service.py` not `us.py`
- **Avoid abbreviations** unless standard in domain
- **Mirror test structure** to source structure:
  - `src/services/user_service.py` â†’ `tests/services/test_user_service.py`
- **Consistent file extensions:** .py, .js, .ts, .go
- **Module files:** `__init__.py`, index.js for package entry points

**Citations:**
- GitHub: kriasoft/Folder-Structure-Conventions
- Iterators: "Comprehensive Guide on Project Codebase Organization"
- Medium: "A Front-End Application Folder Structure that Makes Sense"

**Example:**
```
# Good: Feature-based, semantic organization
src/
â”œâ”€â”€ auth/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ login_service.py
â”‚   â”œâ”€â”€ oauth_provider.py
â”‚   â””â”€â”€ session_manager.py
â”œâ”€â”€ users/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ user_model.py
â”‚   â”œâ”€â”€ user_service.py
â”‚   â””â”€â”€ user_repository.py
â””â”€â”€ billing/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ payment_processor.py
    â””â”€â”€ invoice_generator.py

# Bad: Technical layer organization, unclear names
src/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ u.py
â”‚   â””â”€â”€ o.py
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ svc1.py
â”‚   â””â”€â”€ svc2.py
â””â”€â”€ utils/
    â””â”€â”€ helpers.py
```

---

## 12. CI/CD INTEGRATION

### 12.1 CI/CD Pipeline Visibility

**Definition:** Clear, well-documented CI/CD configuration files committed to repository.

**Why It Matters:** AI can understand build/test/deploy processes by reading CI configs. When builds fail, AI can suggest targeted fixes. Visible pipelines enable collaboration and debugging.

**Impact on Agent Behavior:**
- CI improvement proposals
- Pipeline failure debugging
- Workflow optimization suggestions
- Better understanding of deployment process

**Measurable Criteria:**
- CI config file in repository:
  - GitHub Actions: `.github/workflows/`
  - GitLab CI: `.gitlab-ci.yml`
  - CircleCI: `.circleci/config.yml`
- **Clear job/step names** (not "step1", "step2")
- **Comments explaining complex logic**
- **Fast feedback:** Tests complete <10 minutes
- **Fail fast:** Stop on first failure to save compute
- **Parallelization:** Run independent jobs concurrently
- **Caching:** Dependencies, build artifacts
- **Artifacts:** Test results, coverage reports, logs

**Citations:**
- CircleCI: "Monorepo dev practices"
- GitHub Actions documentation
- GitLab CI best practices
- Martin Fowler: "Continuous Integration"

---

### 12.2 Branch Protection & Status Checks

**Definition:** Required status checks and review approvals before merging to main/production branches.

**Why It Matters:** Prevents broken code from reaching production. Provides safety net for AI-generated code. Ensures quality gates are enforced.

**Impact on Agent Behavior:**
- Understanding of merge requirements
- Awareness of quality gates
- Suggestions aligned with branch policies
- Better PR creation (ensuring checks pass)

**Measurable Criteria:**
- Branch protection enabled for main/master/production
- **Required status checks:**
  - All tests passing
  - Linting/formatting passing
  - Code coverage threshold met
  - Security scanning passing
- **Required reviews:** At least 1 approval
- **No force pushes** to protected branches
- **No direct commits** to protected branches
- **Up-to-date branch** requirement (rebase/merge before merging)

**Citations:**
- GitHub Docs: "About protected branches"
- GitLab: "Protected branches"
- Industry best practices

---

## 13. SECURITY & COMPLIANCE

### 13.1 Security Scanning Automation

**Definition:** Automated security scans for vulnerabilities, secrets, and compliance issues in CI/CD.

**Why It Matters:** AI can accidentally introduce vulnerabilities (SQL injection, XSS, etc.). Research shows LLM-generated code has security weaknesses, particularly around outdated practices. Automated scanning provides safety net.

**Impact on Agent Behavior:**
- Security pattern learning
- Vulnerability avoidance
- Secure coding practice adoption
- Failed scans provide improvement feedback

**Measurable Criteria:**
- **Dependency scanning:** Snyk, Dependabot, npm audit, safety (Python)
- **Secret scanning:** GitLeaks, TruffleHog, detect-secrets
- **Static analysis:** Semgrep, CodeQL, Bandit (Python), gosec (Go)
- **Scans run on:**
  - Every PR (pre-merge)
  - Every commit to main
  - Scheduled (weekly/nightly)
- **Zero tolerance:** No high/critical vulnerabilities allowed to merge
- **SLA:** High severity vulnerabilities fixed within 7 days

**Citations:**
- ArXiv: "Security and Quality in LLM-Generated Code"
- ArXiv: "Security Degradation in Iterative AI Code Generation"
- GitHub Advanced Security documentation
- OWASP Top 10

---

### 13.2 Secrets Management

**Definition:** Proper handling of sensitive data (API keys, passwords, tokens) using secret management tools, not hardcoded values.

**Why It Matters:** Hardcoded secrets in code create security vulnerabilities. AI might accidentally suggest or expose secrets. Proper secrets management is critical security practice.

**Impact on Agent Behavior:**
- Avoids suggesting hardcoded secrets
- Recommends environment variables
- Identifies potential secret exposure
- Suggests secure alternatives

**Measurable Criteria:**
- **No secrets in code:** Use environment variables, secret managers
- **Tools:**
  - Development: .env files (not committed), direnv
  - Production: HashiCorp Vault, AWS Secrets Manager, Azure Key Vault, GCP Secret Manager
- **.env.example** committed (without real values)
- **.env** in .gitignore
- **Secret rotation** documented and automated
- **Pre-commit hook:** Detect-secrets or similar

**Citations:**
- OWASP: "Secrets Management Cheat Sheet"
- GitHub: "Removing sensitive data from a repository"
- HashiCorp Vault documentation

---

## 14. DOCUMENTATION PHILOSOPHY

### 14.1 "Why, Not What" Comments

**Definition:** Comments explain rationale and context, not behavior (which code already shows).

**Why It Matters:** AI can read code to understand "what" it does. Comments providing "why" give context for decisions, workarounds, constraints, and edge cases that aren't obvious from code alone.

**Impact on Agent Behavior:**
- Understanding of constraints and limitations
- Avoidance of "obvious" refactorings that break assumptions
- Preservation of original intent during modifications
- Better context for debugging and optimization

**Measurable Criteria:**
- **Comments explain:**
  - Why this approach was chosen (vs. alternatives)
  - Edge cases and gotchas
  - Performance considerations
  - Historical context (why workaround exists)
  - TODOs with context and rationale
- **Avoid:**
  - Redundant comments duplicating code
  - Commented-out code (use version control)
  - Obvious statements
- **Keep comments in sync** with code during changes

**Citations:**
- Stack Overflow Blog: "Best practices for writing code comments"
- Stepsize: "The Engineer's Guide to Writing Meaningful Code Comments"
- Boot.dev: "Best Practices for Commenting Code"

**Example:**
```python
# Good: Explains "why"
# Using binary search instead of hash table because dataset is
# read-once and memory-constrained (< 100MB available).
# Hash table would require 150MB for this dataset size.
result = binary_search(sorted_data, target)

# API returns 202 Accepted for async processing, but we need
# synchronous behavior for consistency. Poll until completion.
response = api.start_job()
while response.status == 202:
    time.sleep(1)
    response = api.check_status(response.job_id)

# Bad: Redundant, explains "what"
# Search for target in sorted_data
result = binary_search(sorted_data, target)

# Call the API
response = api.start_job()
```

---

## 15. PERFORMANCE & OBSERVABILITY

### 15.1 Performance Benchmarks

**Definition:** Automated performance tests tracking metrics like response time, throughput, memory usage.

**Why It Matters:** Performance regressions can slip in unnoticed. Benchmarks provide objective measurements. AI can suggest optimizations based on benchmark results.

**Impact on Agent Behavior:**
- Performance-aware optimization suggestions
- Regression detection
- Data-driven refactoring decisions
- Bottleneck identification

**Measurable Criteria:**
- Benchmark suite in repository
- **Tools:** pytest-benchmark (Python), Benchmark.js (JavaScript), testing.B (Go)
- Run benchmarks in CI for critical paths
- Track metrics over time
- Alert on regressions (>10% slowdown)

**Citations:**
- Google: "Benchmarking Best Practices"
- Python performance benchmarking docs
- Go benchmarking documentation

---

## IMPLEMENTATION PRIORITIES

### Tier 1: Essential (Must-Have)
**Highest impact, enables basic agent functionality:**

1. **CLAUDE.md** - 40% time savings, immediate context framing
2. **README with quick start** - Entry point understanding
3. **Type annotations** - Higher quality latent space, better comprehension
4. **Standard project layout** - Faster navigation
5. **Dependency lock files** - Reproducible builds

### Tier 2: Critical (Should-Have)
**Major quality improvements, safety nets:**

6. **Test coverage >70%** - Safety for refactoring
7. **Pre-commit hooks + CI/CD** - Automated quality enforcement
8. **Conventional commits** - Semantic versioning, history understanding
9. **Complete .gitignore** - Reduced context pollution
10. **One-command setup** - Easy environment reproduction

### Tier 3: Important (Nice-to-Have)
**Significant improvements in specific areas:**

11. **Cyclomatic complexity limits** - Better code comprehension
12. **Structured logging** - Machine-parseable debugging
13. **OpenAPI/GraphQL specs** - Auto-generated clients
14. **ADRs** - Architectural context
15. **Semantic naming** - Faster code location

### Tier 4: Advanced (Optimization)
**Refinement and optimization:**

16. **Security scanning** - Vulnerability prevention
17. **Performance benchmarks** - Regression detection
18. **Code smell elimination** - Higher quality baseline
19. **PR/Issue templates** - Consistent contributions
20. **Container setup** - Reproducible environments

---

## QUICKSTART: Making Your Codebase Agent-Ready

### Week 1: Foundation Documentation
```bash
# Create CLAUDE.md
cat > CLAUDE.md << 'EOF'
# Tech Stack
- [Your language/framework with versions]

# Standard Commands
- Setup: [command]
- Test: [command]
- Lint: [command]
- Build: [command]

# Repository Structure
- src/ - [description]
- tests/ - [description]

# Boundaries
- [Any off-limits areas]
EOF

# Update README
# Add: Installation, Quick Start, Testing sections

# Create .env.example
cp .env .env.example
# Remove sensitive values, keep variable names
```

### Week 2: Quality Automation
```bash
# Install pre-commit
pip install pre-commit

# Create .pre-commit-config.yaml
pre-commit sample-config > .pre-commit-config.yaml

# Add formatters, linters for your language
# Install hooks
pre-commit install

# Add commitlint (optional but recommended)
npm install -g @commitlint/cli @commitlint/config-conventional
```

### Week 3: Testing & Dependencies
```bash
# Measure test coverage
pytest --cov  # Python
jest --coverage  # JavaScript
go test -cover  # Go

# Generate lock file
pip freeze > requirements.txt  # Python
npm install  # Generates package-lock.json
go mod tidy  # Updates go.sum

# Add Dependabot
# Create .github/dependabot.yml
```

### Week 4: Structure & Types
```bash
# Refactor to standard layout (if needed)
# Add type annotations to public APIs
mypy --install-types  # Python
tsc --init  # TypeScript

# Create PR/Issue templates
mkdir -p .github/ISSUE_TEMPLATE
# Add bug_report.md, feature_request.md
# Add PULL_REQUEST_TEMPLATE.md
```

### Ongoing Maintenance
- Update CLAUDE.md as project evolves
- Create ADRs for architectural decisions
- Monitor code quality metrics (SonarQube, CodeClimate)
- Keep dependencies updated
- Review and improve test coverage

---

## MEASUREMENT & VALIDATION

### Agent-Ready Score Formula

```
Score = (
    Documentation * 0.25 +
    Code Quality * 0.20 +
    Testing * 0.20 +
    Structure * 0.15 +
    CI/CD * 0.10 +
    Security * 0.10
) * 100

Where each category is 0.0-1.0 based on attribute completion.
```

### Certification Levels

- **Platinum (90-100):** Exemplary agent-ready codebase
- **Gold (75-89):** Highly optimized for agents
- **Silver (60-74):** Well-suited for agent development
- **Bronze (40-59):** Basic agent compatibility
- **Needs Improvement (<40):** Significant agent friction

### Validation Checklist

**Documentation (25%):**
- [ ] CLAUDE.md exists and comprehensive
- [ ] README with quick start
- [ ] Inline documentation (docstrings) >80%
- [ ] ADRs for major decisions
- [ ] API specs (OpenAPI/GraphQL)

**Code Quality (20%):**
- [ ] Type annotations >80%
- [ ] Cyclomatic complexity <10
- [ ] Function length <50 lines
- [ ] Code smells <5 per 1000 LOC
- [ ] DRY violations minimal

**Testing (20%):**
- [ ] Test coverage >70%
- [ ] Descriptive test names
- [ ] Fast test execution (<10 min)
- [ ] Tests in CI/CD

**Structure (15%):**
- [ ] Standard project layout
- [ ] Semantic file/directory names
- [ ] Separation of concerns
- [ ] .gitignore complete

**CI/CD (10%):**
- [ ] Pre-commit hooks
- [ ] CI linting/testing
- [ ] Branch protection
- [ ] Automated dependency updates

**Security (10%):**
- [ ] Dependency scanning
- [ ] Secret scanning
- [ ] No hardcoded secrets
- [ ] Security scans in CI

---

## ANTI-PATTERNS TO AVOID

### Documentation Anti-Patterns
- âŒ No README or minimal README
- âŒ Outdated documentation
- âŒ No inline documentation
- âŒ Documentation in external wiki only

### Code Anti-Patterns
- âŒ God objects/functions (>500 lines)
- âŒ No type hints
- âŒ Magic numbers without explanation
- âŒ Unclear variable names (x, tmp, data)

### Testing Anti-Patterns
- âŒ No tests or minimal coverage (<30%)
- âŒ Test names like test1, test2
- âŒ Slow tests (>30 min)
- âŒ Flaky tests

### Structure Anti-Patterns
- âŒ Flat file structure
- âŒ Mixed concerns in single file
- âŒ Inconsistent naming
- âŒ Incomplete .gitignore

### Process Anti-Patterns
- âŒ No CI/CD
- âŒ Manual quality checks
- âŒ No branch protection
- âŒ Direct commits to main

---

## REFERENCES & CITATIONS

### Anthropic
- Anthropic Engineering Blog: "Claude Code Best Practices" (2025)

### Research Papers (ArXiv)
- "LongCodeBench: Evaluating Coding LLMs at 1M Context Windows" (2025)
- "TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories"
- "Automated Type Annotation in Python Using LLMs"
- "Security and Quality in LLM-Generated Code"
- "Security Degradation in Iterative AI Code Generation"

### Industry (Microsoft, Google, GitHub)
- Microsoft Learn: "Code metrics - Cyclomatic complexity"
- GitHub Blog: "How to write a great agents.md"
- GitHub: github/gitignore template collection
- Google SRE Book: Logging and monitoring best practices
- IBM Research: "Why larger LLM context windows are all the rage"

### Engineering Blogs
- Dropbox Tech Blog: "Our journey to type checking 4 million lines of Python"
- Salesforce Engineering: "How Cursor AI Cut Legacy Code Coverage Time by 85%"
- GitClear: "Coding on Copilot" whitepaper

### Standards & Specifications
- Conventional Commits specification v1.0.0
- OpenAPI Specification 3.0+
- PEP 8 - Style Guide for Python Code
- PEP 257 - Docstring Conventions

### Community Resources
- Real Python: "Python Application Layouts"
- GitHub: golang-standards/project-layout
- GitHub: joelparkerhenderson/architecture-decision-record
- GitHub: pre-commit/pre-commit

### Documentation
- Python: pytest, mypy, black, isort documentation
- JavaScript/TypeScript: ESLint, Prettier, TSDoc documentation
- Go: Official style guide, testing documentation
- Docker: Best practices documentation

---

## VERSION HISTORY

- **v1.0.0 (2025-01-20):** Initial comprehensive research compilation
  - 25 attributes identified and documented
  - 50+ authoritative sources cited
  - Measurement framework established
  - Implementation guide created

---

**Document prepared for:** agentready tool development
**Primary use case:** Scanning repositories for AI agent optimization
**Target agents:** Claude Code, Claude-based development assistants
**Methodology:** Evidence-based, cited research from authoritative sources
````

## File: src/agentready/data/default-weights.yaml
````yaml
# Default Tier-Based Weight Distribution
#
# This file defines the default weights for all 25 attributes.
# Weights are based on tier priority from research report:
#
# Tier 1 (Essential):  50% total (10% each, 5 attributes)
# Tier 2 (Critical):   30% total (3% each, 10 attributes)
# Tier 3 (Important):  15% total (3% each, 5 attributes)
# Tier 4 (Advanced):    5% total (1% each, 5 attributes)
#
# Missing essentials (especially CLAUDE.md at 10%) has 10x impact vs advanced features (1%)

# Tier 1 (Essential) - 50% total weight
claude_md_file: 0.10              # 1.1 - CLAUDE.md Configuration Files âš ï¸ CRITICAL
readme_structure: 0.10            # 2.1 - README Structure
type_annotations: 0.10            # 3.3 - Type Annotations
standard_layout: 0.10             # 4.1 - Standard Project Layouts
lock_files: 0.10                  # 6.1 - Lock Files for Reproducibility

# Tier 2 (Critical) - 30% total weight
test_coverage: 0.03               # 5.1 - Test Coverage Requirements
precommit_hooks: 0.03             # 5.3 - Pre-commit Hooks & CI/CD Linting
conventional_commits: 0.03        # 7.1 - Conventional Commit Messages
gitignore_completeness: 0.03      # 7.2 - .gitignore Completeness
one_command_setup: 0.03           # 8.1 - One-Command Build/Setup
concise_documentation: 0.03       # 1.2 - Concise Structured Documentation
inline_documentation: 0.03        # 2.2 - Inline Documentation
file_size_limits: 0.03            # 1.3 - File Size Limits
dependency_freshness: 0.03        # 6.2 - Dependency Freshness & Security
separation_concerns: 0.03         # 4.2 - Separation of Concerns

# Tier 3 (Important) - 15% total weight
cyclomatic_complexity: 0.03       # 3.1 - Cyclomatic Complexity Thresholds
structured_logging: 0.03          # 9.2 - Structured Logging
openapi_specs: 0.03               # 10.1 - OpenAPI/Swagger Specifications
architecture_decisions: 0.03      # 2.3 - Architecture Decision Records
semantic_naming: 0.03             # 11.3 - Semantic File & Directory Naming

# Tier 4 (Advanced) - 5% total weight
security_scanning: 0.01           # 13.1 - Security Scanning Automation
performance_benchmarks: 0.01      # 15.1 - Performance Benchmarks
code_smells: 0.01                 # 3.4 - Code Smell Elimination
issue_pr_templates: 0.01          # 7.3 - Issue & Pull Request Templates
container_setup: 0.01             # 8.3 - Container/Virtualization Setup
````

## File: src/agentready/models/assessment.py
````python
"""Assessment model representing complete repository evaluation."""

from dataclasses import dataclass
from datetime import datetime

from .config import Config
from .finding import Finding
from .repository import Repository


@dataclass
class Assessment:
    """Complete evaluation of a repository at a specific point in time.

    Attributes:
        repository: The repository assessed
        timestamp: When assessment was performed
        overall_score: Weighted average score 0-100
        certification_level: Platinum/Gold/Silver/Bronze based on score
        attributes_assessed: Number of successfully evaluated attributes
        attributes_skipped: Number of skipped attributes (tool missing, errors)
        attributes_total: Total attributes (should be 25)
        findings: Individual attribute results
        config: Custom configuration used (if any)
        duration_seconds: Time taken for assessment
    """

    repository: Repository
    timestamp: datetime
    overall_score: float
    certification_level: str
    attributes_assessed: int
    attributes_skipped: int
    attributes_total: int
    findings: list[Finding]
    config: Config | None
    duration_seconds: float

    VALID_LEVELS = {"Platinum", "Gold", "Silver", "Bronze", "Needs Improvement"}

    def __post_init__(self):
        """Validate assessment data after initialization."""
        if not 0.0 <= self.overall_score <= 100.0:
            raise ValueError(
                f"Overall score must be in range [0.0, 100.0]: {self.overall_score}"
            )

        if self.certification_level not in self.VALID_LEVELS:
            raise ValueError(
                f"Certification level must be one of {self.VALID_LEVELS}: "
                f"{self.certification_level}"
            )

        if self.attributes_assessed + self.attributes_skipped != self.attributes_total:
            raise ValueError(
                f"Assessed ({self.attributes_assessed}) + skipped "
                f"({self.attributes_skipped}) must equal total "
                f"({self.attributes_total})"
            )

        if len(self.findings) != self.attributes_total:
            raise ValueError(
                f"Findings count ({len(self.findings)}) must equal "
                f"attributes total ({self.attributes_total})"
            )

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "repository": self.repository.to_dict(),
            "timestamp": self.timestamp.isoformat(),
            "overall_score": self.overall_score,
            "certification_level": self.certification_level,
            "attributes_assessed": self.attributes_assessed,
            "attributes_skipped": self.attributes_skipped,
            "attributes_total": self.attributes_total,
            "findings": [f.to_dict() for f in self.findings],
            "config": self.config.to_dict() if self.config else None,
            "duration_seconds": self.duration_seconds,
        }

    @staticmethod
    def determine_certification_level(score: float) -> str:
        """Determine certification level based on overall score.

        Thresholds:
        - Platinum: 90-100
        - Gold: 75-89
        - Silver: 60-74
        - Bronze: 40-59
        - Needs Improvement: 0-39
        """
        if score >= 90:
            return "Platinum"
        elif score >= 75:
            return "Gold"
        elif score >= 60:
            return "Silver"
        elif score >= 40:
            return "Bronze"
        else:
            return "Needs Improvement"
````

## File: src/agentready/models/attribute.py
````python
"""Attribute model defining one of the 25 agent-ready quality attributes."""

from dataclasses import dataclass


@dataclass
class Attribute:
    """Defines an agent-ready quality attribute from the research report.

    Attributes:
        id: Unique identifier (e.g., "claude_md_file", "test_coverage")
        name: Human-readable name (e.g., "CLAUDE.md Configuration Files")
        category: Research report section (e.g., "Context Window Optimization")
        tier: Priority tier 1-4 (1=Essential, 4=Advanced)
        description: What this attribute measures
        criteria: Measurable criteria for passing
        default_weight: Default weight in scoring (0.0-1.0)
    """

    id: str
    name: str
    category: str
    tier: int
    description: str
    criteria: str
    default_weight: float

    def __post_init__(self):
        """Validate attribute data after initialization."""
        if not self.id.islower() or " " in self.id:
            raise ValueError(f"Attribute ID must be lowercase snake_case: {self.id}")

        if self.tier not in (1, 2, 3, 4):
            raise ValueError(f"Tier must be 1, 2, 3, or 4: {self.tier}")

        if not 0.0 <= self.default_weight <= 1.0:
            raise ValueError(
                f"Default weight must be in range [0.0, 1.0]: {self.default_weight}"
            )

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "id": self.id,
            "name": self.name,
            "category": self.category,
            "tier": self.tier,
            "description": self.description,
            "criteria": self.criteria,
            "default_weight": self.default_weight,
        }
````

## File: src/agentready/models/citation.py
````python
"""Citation model for authoritative sources from research report."""

from dataclasses import dataclass


@dataclass
class Citation:
    """Reference to authoritative source from research report.

    Attributes:
        source: Source name (e.g., "Anthropic Engineering Blog")
        title: Article/paper title
        url: Link to source (optional)
        relevance: Why this citation supports the attribute
    """

    source: str
    title: str
    url: str | None
    relevance: str

    def __post_init__(self):
        """Validate citation data after initialization."""
        if not self.source:
            raise ValueError("Citation source must be non-empty")

        if not self.title:
            raise ValueError("Citation title must be non-empty")

        if not self.relevance:
            raise ValueError("Citation relevance must be non-empty")

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "source": self.source,
            "title": self.title,
            "url": self.url,
            "relevance": self.relevance,
        }
````

## File: src/agentready/models/config.py
````python
"""Config model for user customization of assessment behavior."""

from dataclasses import dataclass
from pathlib import Path


@dataclass
class Config:
    """User configuration for customizing assessment behavior.

    Attributes:
        weights: Custom attribute weights (attribute_id â†’ weight)
        excluded_attributes: Attributes to skip
        language_overrides: Force language detection (lang â†’ [patterns])
        output_dir: Custom output directory (None uses default .agentready/)
    """

    weights: dict[str, float]
    excluded_attributes: list[str]
    language_overrides: dict[str, list[str]]
    output_dir: Path | None

    def __post_init__(self):
        """Validate config data after initialization."""
        # Validate weights are positive
        for attr_id, weight in self.weights.items():
            if weight <= 0:
                raise ValueError(f"Weight must be positive for {attr_id}: {weight}")
            if weight > 1.0:
                raise ValueError(f"Weight must be <= 1.0 for {attr_id}: {weight}")

        # Validate weights sum (with tolerance for floating point)
        if self.weights:
            total = sum(self.weights.values())
            tolerance = 0.001
            if abs(total - 1.0) > tolerance:
                raise ValueError(
                    f"Weights must sum to 1.0 (got {total:.4f}, "
                    f"difference: {total - 1.0:+.4f})"
                )

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "weights": self.weights,
            "excluded_attributes": self.excluded_attributes,
            "language_overrides": self.language_overrides,
            "output_dir": str(self.output_dir) if self.output_dir else None,
        }

    def get_weight(self, attribute_id: str, default: float) -> float:
        """Get weight for attribute, falling back to default if not specified."""
        return self.weights.get(attribute_id, default)

    def is_excluded(self, attribute_id: str) -> bool:
        """Check if attribute is excluded from assessment."""
        return attribute_id in self.excluded_attributes
````

## File: src/agentready/models/finding.py
````python
"""Finding and Remediation models for assessment results."""

from dataclasses import dataclass

from .attribute import Attribute
from .citation import Citation


@dataclass
class Remediation:
    """Actionable guidance for fixing a failing attribute.

    Attributes:
        summary: One-line summary of what to do
        steps: Ordered steps to remediate
        tools: Tools/packages needed (e.g., "black", "pytest-cov")
        commands: Example commands to run
        examples: Code/config examples
        citations: Links to documentation/research
    """

    summary: str
    steps: list[str]
    tools: list[str]
    commands: list[str]
    examples: list[str]
    citations: list[Citation]

    def __post_init__(self):
        """Validate remediation data after initialization."""
        if not self.summary:
            raise ValueError("Remediation summary must be non-empty")

        if not self.steps:
            raise ValueError("Remediation must have at least one step")

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "summary": self.summary,
            "steps": self.steps,
            "tools": self.tools,
            "commands": self.commands,
            "examples": self.examples,
            "citations": [c.to_dict() for c in self.citations],
        }


@dataclass
class Finding:
    """Result of assessing a single attribute against a repository.

    Attributes:
        attribute: The attribute being assessed
        status: One of: pass, fail, skipped, error, not_applicable
        score: Score 0-100, or None if skipped/error
        measured_value: Actual measurement (e.g., "847 lines", "63% coverage")
        threshold: Expected threshold (e.g., "<300 lines", ">80% coverage")
        evidence: Specific files/metrics supporting the finding
        remediation: How to fix if failing (None if passing)
        error_message: Error details if status="error"
    """

    attribute: Attribute
    status: str
    score: float | None
    measured_value: str | None
    threshold: str | None
    evidence: list[str]
    remediation: Remediation | None
    error_message: str | None

    VALID_STATUSES = {"pass", "fail", "skipped", "error", "not_applicable"}

    def __post_init__(self):
        """Validate finding data after initialization."""
        if self.status not in self.VALID_STATUSES:
            raise ValueError(
                f"Status must be one of {self.VALID_STATUSES}: {self.status}"
            )

        if self.status in ("pass", "fail"):
            if self.score is None:
                raise ValueError(f"Score required for status {self.status}")
            if not 0.0 <= self.score <= 100.0:
                raise ValueError(f"Score must be in range [0.0, 100.0]: {self.score}")

        if self.status == "error" and not self.error_message:
            raise ValueError("Error message required for status error")

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "attribute": self.attribute.to_dict(),
            "status": self.status,
            "score": self.score,
            "measured_value": self.measured_value,
            "threshold": self.threshold,
            "evidence": self.evidence,
            "remediation": self.remediation.to_dict() if self.remediation else None,
            "error_message": self.error_message,
        }

    @classmethod
    def not_applicable(cls, attribute: Attribute, reason: str = "") -> "Finding":
        """Create a not_applicable finding for language-specific attributes."""
        evidence = [reason] if reason else []
        return cls(
            attribute=attribute,
            status="not_applicable",
            score=None,
            measured_value=None,
            threshold=None,
            evidence=evidence,
            remediation=None,
            error_message=None,
        )

    @classmethod
    def skipped(
        cls, attribute: Attribute, reason: str, remediation: str = ""
    ) -> "Finding":
        """Create a skipped finding for missing tools or permission errors."""
        rem = None
        if remediation:
            rem = Remediation(
                summary=remediation,
                steps=[remediation],
                tools=[],
                commands=[],
                examples=[],
                citations=[],
            )

        return cls(
            attribute=attribute,
            status="skipped",
            score=None,
            measured_value=None,
            threshold=None,
            evidence=[reason],
            remediation=rem,
            error_message=None,
        )

    @classmethod
    def error(cls, attribute: Attribute, reason: str) -> "Finding":
        """Create an error finding for unexpected failures."""
        return cls(
            attribute=attribute,
            status="error",
            score=None,
            measured_value=None,
            threshold=None,
            evidence=[],
            remediation=None,
            error_message=reason,
        )
````

## File: src/agentready/models/repository.py
````python
"""Repository model representing the target git repository being assessed."""

from dataclasses import dataclass
from pathlib import Path


@dataclass
class Repository:
    """Represents a git repository being assessed.

    Attributes:
        path: Absolute path to repository root
        name: Repository name (derived from path)
        url: Remote origin URL if available
        branch: Current branch name
        commit_hash: Current HEAD commit SHA
        languages: Detected languages with file counts (e.g., {"Python": 42})
        total_files: Total files in repository (respecting .gitignore)
        total_lines: Total lines of code
    """

    path: Path
    name: str
    url: str | None
    branch: str
    commit_hash: str
    languages: dict[str, int]
    total_files: int
    total_lines: int

    def __post_init__(self):
        """Validate repository data after initialization."""
        if not self.path.exists():
            raise ValueError(f"Repository path does not exist: {self.path}")

        if not (self.path / ".git").exists():
            raise ValueError(f"Not a git repository: {self.path}")

        if self.total_files < 0:
            raise ValueError(f"Total files must be non-negative: {self.total_files}")

        if self.total_lines < 0:
            raise ValueError(f"Total lines must be non-negative: {self.total_lines}")

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "path": str(self.path),
            "name": self.name,
            "url": self.url,
            "branch": self.branch,
            "commit_hash": self.commit_hash,
            "languages": self.languages,
            "total_files": self.total_files,
            "total_lines": self.total_lines,
        }
````

## File: src/agentready/reporters/base.py
````python
"""Base reporter interface for generating assessment reports."""

from abc import ABC, abstractmethod
from pathlib import Path

from ..models.assessment import Assessment


class BaseReporter(ABC):
    """Abstract base class for all report generators.

    Reporters transform Assessment data into different output formats
    (HTML, Markdown, PDF, etc.) for human consumption.
    """

    @abstractmethod
    def generate(self, assessment: Assessment, output_path: Path) -> Path:
        """Generate report from assessment data.

        Args:
            assessment: Complete assessment with findings
            output_path: Path where report should be saved

        Returns:
            Path to generated report file

        Raises:
            IOError: If report cannot be written
        """
        pass
````

## File: src/agentready/reporters/markdown.py
````python
"""Markdown reporter for generating version-control-friendly assessment reports."""

from pathlib import Path

from ..models.assessment import Assessment
from .base import BaseReporter


class MarkdownReporter(BaseReporter):
    """Generates GitHub-Flavored Markdown reports.

    Features:
    - Version-control friendly (git diff shows progress)
    - Renders properly on GitHub/GitLab/Bitbucket
    - Tables for summary data
    - Collapsible details using HTML details/summary
    - Code blocks with syntax highlighting
    - Emoji indicators for status
    """

    def generate(self, assessment: Assessment, output_path: Path) -> Path:
        """Generate Markdown report from assessment data.

        Args:
            assessment: Complete assessment with findings
            output_path: Path where Markdown file should be saved

        Returns:
            Path to generated Markdown file

        Raises:
            IOError: If Markdown cannot be written
        """
        sections = []

        # Header
        sections.append(self._generate_header(assessment))

        # Summary
        sections.append(self._generate_summary(assessment))

        # Certification Ladder
        sections.append(self._generate_certification_ladder(assessment))

        # Findings by Category
        sections.append(self._generate_findings(assessment))

        # Next Steps
        sections.append(self._generate_next_steps(assessment))

        # Footer
        sections.append(self._generate_footer(assessment))

        # Combine all sections
        markdown_content = "\n\n".join(sections)

        # Write to file
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(markdown_content)

        return output_path

    def _generate_header(self, assessment: Assessment) -> str:
        """Generate report header with repository info."""
        return f"""# ðŸ¤– AgentReady Assessment Report

**Repository**: {assessment.repository.name}
**Path**: `{assessment.repository.path}`
**Branch**: {assessment.repository.branch}
**Commit**: {assessment.repository.commit_hash[:8]}
**Date**: {assessment.timestamp.strftime('%Y-%m-%d %H:%M:%S')}

---"""

    def _generate_summary(self, assessment: Assessment) -> str:
        """Generate summary section with key metrics."""
        return f"""## ðŸ“Š Summary

| Metric | Value |
|--------|-------|
| **Overall Score** | **{assessment.overall_score:.1f}/100** |
| **Certification Level** | **{assessment.certification_level}** |
| **Attributes Assessed** | {assessment.attributes_assessed}/{assessment.attributes_total} |
| **Attributes Skipped** | {assessment.attributes_skipped} |
| **Assessment Duration** | {assessment.duration_seconds:.1f}s |

### Languages Detected

{self._format_languages(assessment.repository.languages)}

### Repository Stats

- **Total Files**: {assessment.repository.total_files:,}
- **Total Lines**: {assessment.repository.total_lines:,}"""

    def _format_languages(self, languages: dict[str, int]) -> str:
        """Format language detection results."""
        if not languages:
            return "No languages detected"

        lines = []
        for lang, count in sorted(languages.items(), key=lambda x: x[1], reverse=True):
            lines.append(f"- **{lang}**: {count} files")
        return "\n".join(lines)

    def _generate_certification_ladder(self, assessment: Assessment) -> str:
        """Generate certification ladder visualization."""
        levels = [
            ("Platinum", "ðŸ’Ž", "90-100", assessment.certification_level == "Platinum"),
            ("Gold", "ðŸ¥‡", "75-89", assessment.certification_level == "Gold"),
            ("Silver", "ðŸ¥ˆ", "60-74", assessment.certification_level == "Silver"),
            ("Bronze", "ðŸ¥‰", "40-59", assessment.certification_level == "Bronze"),
            (
                "Needs Improvement",
                "âš ï¸",
                "0-39",
                assessment.certification_level == "Needs Improvement",
            ),
        ]

        lines = ["## ðŸŽ–ï¸ Certification Ladder", ""]
        for name, emoji, range_str, is_active in levels:
            marker = "**â†’ YOUR LEVEL â†**" if is_active else ""
            lines.append(f"- {emoji} **{name}** ({range_str}) {marker}")

        return "\n".join(lines)

    def _generate_findings(self, assessment: Assessment) -> str:
        """Generate detailed findings by category."""
        sections = ["## ðŸ“‹ Detailed Findings"]

        # Group findings by category
        by_category = {}
        for finding in assessment.findings:
            category = finding.attribute.category
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(finding)

        # Generate section for each category
        for category in sorted(by_category.keys()):
            findings = by_category[category]
            sections.append(f"\n### {category}")

            # Summary table for this category
            sections.append("\n| Attribute | Tier | Status | Score |")
            sections.append("|-----------|------|--------|-------|")

            for finding in findings:
                status_emoji = self._get_status_emoji(finding.status)
                score_display = (
                    f"{finding.score:.0f}" if finding.score is not None else "â€”"
                )
                tier_badge = f"T{finding.attribute.tier}"

                sections.append(
                    f"| {finding.attribute.name} | {tier_badge} | {status_emoji} {finding.status} | {score_display} |"
                )

            # Detailed findings (only for fail/error)
            for finding in findings:
                if finding.status in ("fail", "error"):
                    sections.append(self._generate_finding_detail(finding))

        return "\n".join(sections)

    def _get_status_emoji(self, status: str) -> str:
        """Get emoji for finding status."""
        emoji_map = {
            "pass": "âœ…",
            "fail": "âŒ",
            "skipped": "âŠ˜",
            "not_applicable": "âŠ˜",
            "error": "âš ï¸",
        }
        return emoji_map.get(status, "â“")

    def _generate_finding_detail(self, finding) -> str:
        """Generate detailed finding section."""
        lines = [
            f"\n#### {self._get_status_emoji(finding.status)} {finding.attribute.name}"
        ]

        # Basic info
        if finding.measured_value:
            lines.append(
                f"\n**Measured**: {finding.measured_value} (Threshold: {finding.threshold})"
            )

        # Evidence
        if finding.evidence:
            lines.append("\n**Evidence**:")
            for item in finding.evidence:
                lines.append(f"- {item}")

        # Remediation
        if finding.remediation:
            lines.append(
                "\n<details><summary><strong>ðŸ“ Remediation Steps</strong></summary>\n"
            )
            lines.append(f"\n{finding.remediation.summary}\n")

            if finding.remediation.steps:
                for i, step in enumerate(finding.remediation.steps, 1):
                    lines.append(f"{i}. {step}")

            if finding.remediation.commands:
                lines.append("\n**Commands**:\n")
                lines.append("```bash")
                lines.append("\n".join(finding.remediation.commands))
                lines.append("```")

            if finding.remediation.examples:
                lines.append("\n**Examples**:\n")
                for example in finding.remediation.examples:
                    lines.append("```")
                    lines.append(example)
                    lines.append("```")

            lines.append("\n</details>")

        # Error message
        if finding.error_message:
            lines.append(f"\n**Error**: {finding.error_message}")

        return "\n".join(lines)

    def _generate_next_steps(self, assessment: Assessment) -> str:
        """Generate prioritized next steps based on failures."""
        # Find all failing attributes
        failures = [
            f for f in assessment.findings if f.status == "fail" and f.score is not None
        ]

        if not failures:
            return """## âœ¨ Next Steps

**Congratulations!** All assessed attributes are passing. Consider:
- Implementing currently skipped attributes
- Maintaining these standards as the codebase evolves"""

        # Sort by tier (lower tier = higher priority) and score (lower score = more important)
        failures.sort(key=lambda f: (f.attribute.tier, f.score or 0))

        lines = [
            "## ðŸŽ¯ Next Steps",
            "",
            "**Priority Improvements** (highest impact first):",
            "",
        ]

        for i, finding in enumerate(failures[:5], 1):  # Top 5 only
            potential_points = finding.attribute.default_weight * 100
            lines.append(
                f"{i}. **{finding.attribute.name}** (Tier {finding.attribute.tier}) - "
                f"+{potential_points:.1f} points potential"
            )
            if finding.remediation:
                lines.append(f"   - {finding.remediation.summary}")

        return "\n".join(lines)

    def _generate_footer(self, assessment: Assessment) -> str:
        """Generate report footer."""
        return f"""---

## ðŸ“ Assessment Metadata

- **Tool Version**: AgentReady v1.0.0
- **Research Report**: Bundled version
- **Repository Snapshot**: {assessment.repository.commit_hash}
- **Assessment Duration**: {assessment.duration_seconds:.1f}s

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)"""
````

## File: src/agentready/services/language_detector.py
````python
"""Language detection service using file extension analysis."""

import subprocess
from collections import defaultdict
from pathlib import Path


class LanguageDetector:
    """Detects programming languages in a repository.

    Uses file extension mapping and respects .gitignore patterns
    via git ls-files for accuracy.
    """

    # Extension to language mapping
    EXTENSION_MAP = {
        ".py": "Python",
        ".pyx": "Python",
        ".pyi": "Python",
        ".js": "JavaScript",
        ".jsx": "JavaScript",
        ".mjs": "JavaScript",
        ".cjs": "JavaScript",
        ".ts": "TypeScript",
        ".tsx": "TypeScript",
        ".go": "Go",
        ".java": "Java",
        ".kt": "Kotlin",
        ".kts": "Kotlin",
        ".c": "C",
        ".h": "C",  # Ambiguous but default to C
        ".cpp": "C++",
        ".cc": "C++",
        ".cxx": "C++",
        ".hpp": "C++",
        ".hxx": "C++",
        ".rs": "Rust",
        ".rb": "Ruby",
        ".php": "PHP",
        ".swift": "Swift",
        ".r": "R",
        ".R": "R",
        ".cs": "C#",
        ".scala": "Scala",
        ".sh": "Shell",
        ".bash": "Shell",
        ".zsh": "Shell",
        ".md": "Markdown",
        ".yaml": "YAML",
        ".yml": "YAML",
        ".json": "JSON",
        ".toml": "TOML",
        ".xml": "XML",
    }

    def __init__(self, repository_path: Path):
        """Initialize language detector for repository.

        Args:
            repository_path: Path to git repository root
        """
        self.repository_path = repository_path
        self.minimum_file_threshold = 3  # Need 3+ files to count as "using language"

    def detect_languages(self) -> dict[str, int]:
        """Detect languages in repository with file counts.

        Returns:
            Dictionary mapping language name to file count
            (e.g., {"Python": 42, "JavaScript": 18})

        Only includes languages with >= minimum_file_threshold files.
        """
        language_counts = defaultdict(int)

        # Try git ls-files first (respects .gitignore)
        try:
            result = subprocess.run(
                ["git", "ls-files"],
                cwd=self.repository_path,
                capture_output=True,
                text=True,
                timeout=30,
                check=True,
            )
            files = result.stdout.strip().split("\n")
        except (subprocess.SubprocessError, FileNotFoundError):
            # Fall back to pathlib walk (less accurate)
            files = [
                str(f.relative_to(self.repository_path))
                for f in self.repository_path.rglob("*")
                if f.is_file()
            ]

        # Count files by language
        for file_path in files:
            if not file_path.strip():
                continue

            path = Path(file_path)
            suffix = path.suffix.lower()

            if suffix in self.EXTENSION_MAP:
                language = self.EXTENSION_MAP[suffix]
                language_counts[language] += 1

        # Filter by minimum threshold
        return {
            lang: count
            for lang, count in language_counts.items()
            if count >= self.minimum_file_threshold
        }

    def count_total_files(self) -> int:
        """Count total files in repository (respecting .gitignore).

        Returns:
            Total file count
        """
        try:
            result = subprocess.run(
                ["git", "ls-files"],
                cwd=self.repository_path,
                capture_output=True,
                text=True,
                timeout=30,
                check=True,
            )
            files = result.stdout.strip().split("\n")
            return len([f for f in files if f.strip()])
        except (subprocess.SubprocessError, FileNotFoundError):
            # Fall back to pathlib
            return sum(1 for _ in self.repository_path.rglob("*") if _.is_file())

    def count_total_lines(self) -> int:
        """Count total lines of code in repository.

        Returns:
            Total line count (excluding empty lines and comments)

        Note: This is a simple implementation. For production use,
        consider using a dedicated tool like cloc or tokei.
        """
        total_lines = 0

        try:
            result = subprocess.run(
                ["git", "ls-files"],
                cwd=self.repository_path,
                capture_output=True,
                text=True,
                timeout=30,
                check=True,
            )
            files = result.stdout.strip().split("\n")
        except (subprocess.SubprocessError, FileNotFoundError):
            files = [
                str(f.relative_to(self.repository_path))
                for f in self.repository_path.rglob("*")
                if f.is_file()
            ]

        for file_path in files:
            if not file_path.strip():
                continue

            full_path = self.repository_path / file_path

            # Only count text files (skip binaries)
            try:
                with open(full_path, "r", encoding="utf-8", errors="ignore") as f:
                    total_lines += sum(1 for line in f if line.strip())
            except (OSError, UnicodeDecodeError):
                # Skip binary files or unreadable files
                continue

        return total_lines
````

## File: src/agentready/services/research_loader.py
````python
"""Research loader service for loading and validating research reports."""

import re
from dataclasses import dataclass
from pathlib import Path


@dataclass
class ResearchMetadata:
    """Metadata extracted from research report."""

    version: str
    date: str
    attribute_count: int
    tier_count: int
    reference_count: int


class ResearchLoader:
    """Loads and validates bundled research report.

    Supports loading from bundled data, custom paths, and URLs.
    Validates structure per FR-024.
    """

    def __init__(self, data_dir: Path | None = None):
        """Initialize research loader.

        Args:
            data_dir: Directory containing research report
                     (defaults to package data directory)
        """
        if data_dir is None:
            # Default to package data directory
            self.data_dir = Path(__file__).parent.parent / "data"
        else:
            self.data_dir = data_dir

        self.research_file = self.data_dir / "agent-ready-codebase-attributes.md"

    def load_research_report(self) -> str:
        """Load research report content.

        Returns:
            Research report markdown content

        Raises:
            FileNotFoundError: If research report not found
        """
        if not self.research_file.exists():
            raise FileNotFoundError(f"Research report not found: {self.research_file}")

        with open(self.research_file, "r", encoding="utf-8") as f:
            return f.read()

    def extract_metadata(self, content: str) -> ResearchMetadata:
        """Extract metadata from research report.

        Args:
            content: Research report markdown content

        Returns:
            ResearchMetadata with version, date, counts

        Raises:
            ValueError: If metadata cannot be extracted
        """
        # Extract version and date from YAML frontmatter
        frontmatter_match = re.search(
            r"^---\s*\nversion:\s*([^\n]+)\s*\ndate:\s*([^\n]+)\s*\n---",
            content,
            re.MULTILINE,
        )

        if frontmatter_match:
            version = frontmatter_match.group(1).strip()
            date = frontmatter_match.group(2).strip()
        else:
            # Default version if not found
            version = "1.0.0"
            date = "unknown"

        # Count attributes (look for ### headings with numbering like "1.1", "2.3", etc.)
        attribute_pattern = r"^###\s+\d+\.\d+\s+"
        attribute_count = len(re.findall(attribute_pattern, content, re.MULTILINE))

        # Count tiers (look for tier headings)
        tier_pattern = r"^###\s+Tier\s+\d+:"
        tier_count = len(re.findall(tier_pattern, content, re.MULTILINE))

        # Count references (look for citation patterns)
        reference_pattern = r"^\d+\.\s+\[.+?\]\(.+?\)"
        reference_count = len(re.findall(reference_pattern, content, re.MULTILINE))

        return ResearchMetadata(
            version=version,
            date=date,
            attribute_count=attribute_count,
            tier_count=tier_count,
            reference_count=reference_count,
        )

    def validate_structure(self, content: str) -> tuple[bool, list[str], list[str]]:
        """Validate research report structure.

        Args:
            content: Research report markdown content

        Returns:
            Tuple of (is_valid, errors, warnings)
            - errors: Blocking issues that prevent usage
            - warnings: Non-critical issues that can be ignored

        Validation rules per FR-024:
        - Errors: Missing metadata, incorrect attribute count, missing measurable criteria
        - Warnings: Missing impact sections, fewer references than recommended
        """
        errors = []
        warnings = []

        metadata = self.extract_metadata(content)

        # Check attribute count (should be 25)
        if metadata.attribute_count != 25:
            errors.append(f"Expected 25 attributes, found {metadata.attribute_count}")

        # Check tier count (should be 4)
        if metadata.tier_count < 4:
            errors.append(f"Expected 4 tiers, found {metadata.tier_count}")

        # Check reference count (recommend 20+)
        if metadata.reference_count < 20:
            warnings.append(
                f"Only {metadata.reference_count} references "
                f"(recommend 20+ for evidence-based design)"
            )

        # Check for "Measurable Criteria" sections
        measurable_criteria_pattern = r"\*\*Measurable Criteria:\*\*"
        criteria_count = len(
            re.findall(measurable_criteria_pattern, content, re.MULTILINE)
        )

        if criteria_count < 25:
            errors.append(
                f"Missing 'Measurable Criteria' sections "
                f"(found {criteria_count}/25)"
            )

        # Check for "Impact on Agent Behavior" sections (warning only)
        impact_pattern = r"\*\*Impact on Agent Behavior:\*\*"
        impact_count = len(re.findall(impact_pattern, content, re.MULTILINE))

        if impact_count < 25:
            warnings.append(
                f"{25 - impact_count} attributes missing "
                f"'Impact on Agent Behavior' sections"
            )

        is_valid = len(errors) == 0

        return is_valid, errors, warnings

    def load_and_validate(
        self,
    ) -> tuple[str, ResearchMetadata, bool, list[str], list[str]]:
        """Load research report and validate structure.

        Returns:
            Tuple of (content, metadata, is_valid, errors, warnings)

        Raises:
            FileNotFoundError: If research report not found
        """
        content = self.load_research_report()
        metadata = self.extract_metadata(content)
        is_valid, errors, warnings = self.validate_structure(content)

        return content, metadata, is_valid, errors, warnings
````

## File: src/agentready/services/scanner.py
````python
"""Scanner service orchestrating the assessment workflow."""

import time
from datetime import datetime
from pathlib import Path

import git

from ..models.assessment import Assessment
from ..models.config import Config
from ..models.finding import Finding
from ..models.repository import Repository
from .language_detector import LanguageDetector
from .scorer import Scorer


class MissingToolError(Exception):
    """Raised when a required tool is missing."""

    def __init__(self, tool_name: str, install_command: str = ""):
        self.tool_name = tool_name
        self.install_command = install_command
        super().__init__(f"Missing tool: {tool_name}")


class Scanner:
    """Orchestrates assessment workflow with try-assess-skip error handling.

    Responsibilities:
    - Validate repository structure
    - Detect languages and metadata
    - Execute assessors with graceful degradation
    - Calculate scores and certification level
    - Track progress
    """

    def __init__(self, repository_path: Path, config: Config | None = None):
        """Initialize scanner for repository.

        Args:
            repository_path: Path to git repository root
            config: User configuration (optional)

        Raises:
            ValueError: If repository is invalid
        """
        self.repository_path = repository_path
        self.config = config
        self.scorer = Scorer()

        # Validate repository
        self._validate_repository()

    def _validate_repository(self):
        """Validate repository has .git directory per FR-017.

        Raises:
            ValueError: If not a valid git repository
        """
        if not self.repository_path.exists():
            raise ValueError(f"Repository path does not exist: {self.repository_path}")

        if not (self.repository_path / ".git").exists():
            raise ValueError(f"Not a git repository: {self.repository_path}")

    def scan(self, assessors: list, verbose: bool = False) -> Assessment:
        """Execute full assessment workflow.

        Args:
            assessors: List of assessor instances to run
            verbose: Enable detailed progress logging

        Returns:
            Complete Assessment with findings and scores

        Flow:
        1. Build Repository model (detect languages, metadata)
        2. For each assessor: try assess â†’ catch errors â†’ create finding
        3. Calculate overall score with weighted average
        4. Determine certification level
        5. Return Assessment
        """
        start_time = time.time()

        if verbose:
            print(f"Scanning repository: {self.repository_path.name}")

        # Build Repository model
        repository = self._build_repository_model(verbose)

        if verbose:
            print(f"Languages detected: {', '.join(repository.languages.keys())}")
            print(f"\nEvaluating {len(assessors)} attributes...")

        # Execute assessors with graceful degradation
        findings = []
        for assessor in assessors:
            finding = self._execute_assessor(assessor, repository, verbose)
            findings.append(finding)

        # Calculate scores
        overall_score = self.scorer.calculate_overall_score(findings, self.config)
        certification_level = self.scorer.determine_certification_level(overall_score)

        # Count assessed vs skipped
        assessed, skipped = self.scorer.count_assessed_attributes(findings)

        duration = time.time() - start_time

        if verbose:
            print(f"\nAssessment complete in {duration:.1f}s")
            print(f"Overall Score: {overall_score}/100 ({certification_level})")
            print(
                f"Attributes Assessed: {assessed}/{len(findings)} ({skipped} skipped)"
            )

        return Assessment(
            repository=repository,
            timestamp=datetime.now(),
            overall_score=overall_score,
            certification_level=certification_level,
            attributes_assessed=assessed,
            attributes_skipped=skipped,
            attributes_total=len(findings),
            findings=findings,
            config=self.config,
            duration_seconds=round(duration, 1),
        )

    def _build_repository_model(self, verbose: bool = False) -> Repository:
        """Build Repository model with metadata and language detection.

        Args:
            verbose: Enable progress logging

        Returns:
            Repository model
        """
        if verbose:
            print("Detecting languages and repository metadata...")

        # Git metadata
        repo = git.Repo(self.repository_path)
        name = self.repository_path.name
        branch = repo.active_branch.name
        commit_hash = repo.head.commit.hexsha

        # Get remote URL (if available)
        try:
            url = repo.remotes.origin.url if repo.remotes else None
        except Exception:
            url = None

        # Language detection
        detector = LanguageDetector(self.repository_path)
        languages = detector.detect_languages()
        total_files = detector.count_total_files()
        total_lines = detector.count_total_lines()

        return Repository(
            path=self.repository_path,
            name=name,
            url=url,
            branch=branch,
            commit_hash=commit_hash,
            languages=languages,
            total_files=total_files,
            total_lines=total_lines,
        )

    def _execute_assessor(
        self, assessor, repository: Repository, verbose: bool = False
    ) -> Finding:
        """Execute single assessor with error handling.

        Implements try-assess-skip pattern per research.md:
        - Check if applicable
        - Try to assess
        - Catch errors â†’ return Finding.skipped() or Finding.error()

        Args:
            assessor: Assessor instance
            repository: Repository model
            verbose: Enable progress logging

        Returns:
            Finding (pass/fail/skipped/error/not_applicable)
        """
        attr_id = assessor.attribute_id

        if verbose:
            print(f"  [{attr_id}] ", end="", flush=True)

        # Check if applicable (language-specific checks)
        try:
            if not assessor.is_applicable(repository):
                if verbose:
                    print("not applicable")
                return Finding.not_applicable(
                    assessor.attribute,
                    reason=f"Not applicable to {list(repository.languages.keys())}",
                )
        except Exception as e:
            if verbose:
                print(f"error (applicability check failed)")
            return Finding.error(
                assessor.attribute, reason=f"Applicability check failed: {str(e)}"
            )

        # Try to assess
        try:
            finding = assessor.assess(repository)

            if verbose:
                if finding.status == "pass":
                    print(f"pass ({finding.score:.0f})")
                elif finding.status == "fail":
                    print(f"fail ({finding.score:.0f})")
                elif finding.status == "skipped":
                    print("skipped")
                else:
                    print(finding.status)

            return finding

        except MissingToolError as e:
            if verbose:
                print(f"skipped (missing {e.tool_name})")

            return Finding.skipped(
                assessor.attribute,
                reason=f"Missing tool: {e.tool_name}",
                remediation=(
                    f"Install with: {e.install_command}" if e.install_command else ""
                ),
            )

        except PermissionError as e:
            if verbose:
                print("skipped (permission denied)")

            return Finding.skipped(
                assessor.attribute,
                reason=f"Permission denied: {getattr(e, 'filename', 'unknown')}",
            )

        except Exception as e:
            if verbose:
                print(f"error ({type(e).__name__})")

            return Finding.error(assessor.attribute, reason=str(e))
````

## File: src/agentready/services/scorer.py
````python
"""Scorer service for calculating weighted scores and certification levels."""

from pathlib import Path

import yaml

from ..models.assessment import Assessment
from ..models.config import Config
from ..models.finding import Finding


class Scorer:
    """Calculates weighted scores and determines certification levels.

    Implements tier-based weight distribution with heavy penalties for
    missing essentials (especially CLAUDE.md at 10% weight).
    """

    def __init__(self, default_weights_path: Path | None = None):
        """Initialize scorer with default weights.

        Args:
            default_weights_path: Path to default-weights.yaml
                                 (defaults to package data directory)
        """
        if default_weights_path is None:
            data_dir = Path(__file__).parent.parent / "data"
            default_weights_path = data_dir / "default-weights.yaml"

        self.default_weights = self._load_weights(default_weights_path)

    def _load_weights(self, path: Path) -> dict[str, float]:
        """Load default weights from YAML file.

        Args:
            path: Path to weights YAML file

        Returns:
            Dictionary mapping attribute ID to weight

        Raises:
            FileNotFoundError: If weights file not found
            ValueError: If weights are invalid
        """
        if not path.exists():
            raise FileNotFoundError(f"Weights file not found: {path}")

        with open(path, "r", encoding="utf-8") as f:
            weights = yaml.safe_load(f)

        # Validate weights sum to 1.0
        total = sum(weights.values())
        tolerance = 0.001

        if abs(total - 1.0) > tolerance:
            raise ValueError(
                f"Default weights must sum to 1.0 (got {total:.4f}, "
                f"difference: {total - 1.0:+.4f})"
            )

        return weights

    def merge_and_rescale_weights(self, config: Config | None) -> dict[str, float]:
        """Merge config overrides with tier defaults and rescale to sum to 1.0.

        Args:
            config: User configuration with weight overrides (or None)

        Returns:
            Final weights dictionary (complete, sums to 1.0)

        Algorithm:
        1. Start with tier defaults
        2. Override with config values
        3. Rescale to sum to 1.0
        """
        # Start with tier defaults
        final_weights = self.default_weights.copy()

        # Override with config values
        if config and config.weights:
            final_weights.update(config.weights)

        # Rescale to sum to 1.0
        total = sum(final_weights.values())
        rescaled = {attr: w / total for attr, w in final_weights.items()}

        return rescaled

    def calculate_overall_score(
        self, findings: list[Finding], config: Config | None = None
    ) -> float:
        """Calculate weighted overall score from findings.

        Args:
            findings: List of assessment findings
            config: User configuration with custom weights (optional)

        Returns:
            Overall score 0-100

        Per FR-027: Only include successfully evaluated attributes in score.
        Skipped/error attributes are excluded from denominator.
        """
        weights = self.merge_and_rescale_weights(config)

        # Calculate weighted score
        total_score = 0.0
        total_weight = 0.0

        for finding in findings:
            attr_id = finding.attribute.id

            # Skip attributes that weren't assessed
            if finding.status in ("skipped", "error", "not_applicable"):
                continue

            # Get weight for this attribute
            weight = weights.get(attr_id, finding.attribute.default_weight)

            # Add to weighted score
            if finding.score is not None:
                total_score += finding.score * weight
                total_weight += weight

        # Normalize score (divide by total weight of assessed attributes)
        if total_weight > 0:
            normalized_score = total_score / total_weight
        else:
            normalized_score = 0.0

        return round(normalized_score, 1)

    def count_assessed_attributes(self, findings: list[Finding]) -> tuple[int, int]:
        """Count assessed and skipped attributes.

        Args:
            findings: List of assessment findings

        Returns:
            Tuple of (assessed_count, skipped_count)
        """
        assessed = sum(1 for f in findings if f.status in ("pass", "fail"))

        skipped = sum(
            1 for f in findings if f.status in ("skipped", "error", "not_applicable")
        )

        return assessed, skipped

    def determine_certification_level(self, score: float) -> str:
        """Determine certification level based on score.

        Uses Assessment.determine_certification_level() for consistency.

        Args:
            score: Overall score 0-100

        Returns:
            Certification level string
        """
        return Assessment.determine_certification_level(score)
````

## File: tests/integration/__init__.py
````python
"""Integration tests for end-to-end workflows."""
````

## File: tests/integration/test_scan_workflow.py
````python
"""Integration tests for complete scan workflow."""

from pathlib import Path

import pytest

from agentready.assessors.documentation import CLAUDEmdAssessor, READMEAssessor
from agentready.reporters.html import HTMLReporter
from agentready.reporters.markdown import MarkdownReporter
from agentready.services.scanner import Scanner


class TestScanWorkflow:
    """Test end-to-end scanning workflow."""

    def test_scan_current_repository(self):
        """Test scanning the agentready repository itself."""
        repo_path = Path(__file__).parent.parent.parent

        # Create scanner
        scanner = Scanner(repo_path, config=None)

        # Use minimal assessors for faster test
        assessors = [CLAUDEmdAssessor(), READMEAssessor()]

        # Run scan
        assessment = scanner.scan(assessors, verbose=False)

        # Verify assessment structure
        assert assessment.repository.name == "agentready"
        assert assessment.overall_score >= 0.0
        assert assessment.overall_score <= 100.0
        assert assessment.attributes_total == len(assessors)
        assert assessment.certification_level in [
            "Platinum",
            "Gold",
            "Silver",
            "Bronze",
            "Needs Improvement",
        ]

    def test_html_report_generation(self, tmp_path):
        """Test HTML report generation."""
        repo_path = Path(__file__).parent.parent.parent

        scanner = Scanner(repo_path, config=None)
        assessors = [CLAUDEmdAssessor()]
        assessment = scanner.scan(assessors, verbose=False)

        # Generate HTML report
        reporter = HTMLReporter()
        output_file = tmp_path / "test_report.html"
        result = reporter.generate(assessment, output_file)

        # Verify file was created
        assert result.exists()
        assert result.stat().st_size > 0

        # Verify HTML content
        with open(result, "r") as f:
            content = f.read()
            assert "AgentReady Assessment Report" in content
            assert assessment.repository.name in content

    def test_markdown_report_generation(self, tmp_path):
        """Test Markdown report generation."""
        repo_path = Path(__file__).parent.parent.parent

        scanner = Scanner(repo_path, config=None)
        assessors = [CLAUDEmdAssessor()]
        assessment = scanner.scan(assessors, verbose=False)

        # Generate Markdown report
        reporter = MarkdownReporter()
        output_file = tmp_path / "test_report.md"
        result = reporter.generate(assessment, output_file)

        # Verify file was created
        assert result.exists()
        assert result.stat().st_size > 0

        # Verify Markdown content
        with open(result, "r") as f:
            content = f.read()
            assert "# ðŸ¤– AgentReady Assessment Report" in content
            assert "## ðŸ“Š Summary" in content
            assert assessment.repository.name in content
````

## File: tests/unit/__init__.py
````python
"""Unit tests for individual components."""
````

## File: tests/unit/test_assessors_structure.py
````python
"""Tests for structure assessors."""

from pathlib import Path

import pytest

from agentready.assessors.structure import StandardLayoutAssessor
from agentready.models.repository import Repository


class TestStandardLayoutAssessor:
    """Test StandardLayoutAssessor."""

    def test_recognizes_tests_directory(self, tmp_path):
        """Test that assessor recognizes tests/ directory."""
        # Create repository with tests/ directory
        git_dir = tmp_path / ".git"
        git_dir.mkdir()
        (tmp_path / "src").mkdir()
        (tmp_path / "tests").mkdir()

        repo = Repository(
            path=tmp_path,
            name="test-repo",
            url=None,
            branch="main",
            commit_hash="abc123",
            languages={"Python": 100},
            total_files=10,
            total_lines=100,
        )

        assessor = StandardLayoutAssessor()
        finding = assessor.assess(repo)

        assert finding.status == "pass"
        assert finding.score == 100.0
        assert "2/2" in finding.measured_value

    def test_recognizes_test_directory(self, tmp_path):
        """Test that assessor recognizes test/ directory (not just tests/)."""
        # Create repository with test/ directory only
        git_dir = tmp_path / ".git"
        git_dir.mkdir()
        (tmp_path / "src").mkdir()
        (tmp_path / "test").mkdir()  # Note: singular 'test', not 'tests'

        repo = Repository(
            path=tmp_path,
            name="test-repo",
            url=None,
            branch="main",
            commit_hash="abc123",
            languages={"JavaScript": 100},
            total_files=10,
            total_lines=100,
        )

        assessor = StandardLayoutAssessor()
        finding = assessor.assess(repo)

        # Should pass - test/ is valid
        assert finding.status == "pass"
        assert finding.score == 100.0
        assert "2/2" in finding.measured_value

    def test_fails_without_standard_directories(self, tmp_path):
        """Test that assessor fails when standard directories missing."""
        # Create repository with no standard directories
        git_dir = tmp_path / ".git"
        git_dir.mkdir()
        (tmp_path / "lib").mkdir()  # Non-standard directory

        repo = Repository(
            path=tmp_path,
            name="test-repo",
            url=None,
            branch="main",
            commit_hash="abc123",
            languages={"Python": 100},
            total_files=10,
            total_lines=100,
        )

        assessor = StandardLayoutAssessor()
        finding = assessor.assess(repo)

        assert finding.status == "fail"
        assert finding.score == 0.0
        assert finding.remediation is not None

    def test_partial_score_with_only_src(self, tmp_path):
        """Test partial score when only src/ exists."""
        # Create repository with only src/
        git_dir = tmp_path / ".git"
        git_dir.mkdir()
        (tmp_path / "src").mkdir()

        repo = Repository(
            path=tmp_path,
            name="test-repo",
            url=None,
            branch="main",
            commit_hash="abc123",
            languages={"Python": 100},
            total_files=10,
            total_lines=100,
        )

        assessor = StandardLayoutAssessor()
        finding = assessor.assess(repo)

        # Should fail but have partial score
        assert finding.status == "fail"  # Less than 75%
        assert 0.0 < finding.score < 100.0
        assert "1/2" in finding.measured_value

    def test_evidence_shows_both_test_variants(self, tmp_path):
        """Test that evidence shows check for both tests/ and test/."""
        git_dir = tmp_path / ".git"
        git_dir.mkdir()
        (tmp_path / "src").mkdir()
        (tmp_path / "test").mkdir()

        repo = Repository(
            path=tmp_path,
            name="test-repo",
            url=None,
            branch="main",
            commit_hash="abc123",
            languages={"Python": 100},
            total_files=10,
            total_lines=100,
        )

        assessor = StandardLayoutAssessor()
        finding = assessor.assess(repo)

        # Evidence should mention tests/
        evidence_str = " ".join(finding.evidence)
        assert "tests/" in evidence_str or "test/" in evidence_str
        assert "âœ“" in evidence_str  # Should show checkmark for test dir
````

## File: tests/unit/test_models.py
````python
"""Unit tests for data models."""

from datetime import datetime
from pathlib import Path

import pytest

from agentready.models.assessment import Assessment
from agentready.models.attribute import Attribute
from agentready.models.config import Config
from agentready.models.finding import Citation, Finding, Remediation
from agentready.models.repository import Repository


class TestRepository:
    """Test Repository model."""

    def test_repository_creation(self, tmp_path):
        """Test creating a valid repository."""
        # Create a fake git repo
        git_dir = tmp_path / ".git"
        git_dir.mkdir()

        repo = Repository(
            path=tmp_path,
            name="test-repo",
            url="https://github.com/test/repo",
            branch="main",
            commit_hash="abc123",
            languages={"Python": 10},
            total_files=15,
            total_lines=500,
        )

        assert repo.name == "test-repo"
        assert repo.total_files == 15
        assert "Python" in repo.languages

    def test_repository_invalid_path(self):
        """Test repository with invalid path."""
        with pytest.raises(ValueError, match="does not exist"):
            Repository(
                path=Path("/nonexistent"),
                name="test",
                url=None,
                branch="main",
                commit_hash="abc",
                languages={},
                total_files=0,
                total_lines=0,
            )

    def test_repository_to_dict(self, tmp_path):
        """Test repository serialization."""
        git_dir = tmp_path / ".git"
        git_dir.mkdir()

        repo = Repository(
            path=tmp_path,
            name="test",
            url=None,
            branch="main",
            commit_hash="abc123",
            languages={"Python": 5},
            total_files=10,
            total_lines=200,
        )

        data = repo.to_dict()
        assert data["name"] == "test"
        assert data["languages"] == {"Python": 5}


class TestAttribute:
    """Test Attribute model."""

    def test_attribute_creation(self):
        """Test creating a valid attribute."""
        attr = Attribute(
            id="test_attr",
            name="Test Attribute",
            category="Testing",
            tier=1,
            description="A test attribute",
            criteria="Must pass test",
            default_weight=0.10,
        )

        assert attr.id == "test_attr"
        assert attr.tier == 1
        assert attr.default_weight == 0.10

    def test_attribute_invalid_tier(self):
        """Test attribute with invalid tier."""
        with pytest.raises(ValueError, match="Tier must be"):
            Attribute(
                id="test",
                name="Test",
                category="Test",
                tier=5,  # Invalid
                description="Test",
                criteria="Test",
                default_weight=0.10,
            )

    def test_attribute_invalid_weight(self):
        """Test attribute with invalid weight."""
        with pytest.raises(ValueError, match="weight must be"):
            Attribute(
                id="test",
                name="Test",
                category="Test",
                tier=1,
                description="Test",
                criteria="Test",
                default_weight=1.5,  # Invalid
            )


class TestFinding:
    """Test Finding model."""

    def test_finding_pass(self):
        """Test creating a passing finding."""
        attr = Attribute(
            id="test",
            name="Test",
            category="Test",
            tier=1,
            description="Test",
            criteria="Test",
            default_weight=0.10,
        )

        finding = Finding(
            attribute=attr,
            status="pass",
            score=100.0,
            measured_value="present",
            threshold="present",
            evidence=["File found"],
            remediation=None,
            error_message=None,
        )

        assert finding.status == "pass"
        assert finding.score == 100.0

    def test_finding_fail_with_remediation(self):
        """Test creating a failing finding with remediation."""
        attr = Attribute(
            id="test",
            name="Test",
            category="Test",
            tier=1,
            description="Test",
            criteria="Test",
            default_weight=0.10,
        )

        remediation = Remediation(
            summary="Fix the issue",
            steps=["Step 1", "Step 2"],
            tools=["tool1"],
            commands=["command1"],
            examples=["example1"],
            citations=[],
        )

        finding = Finding(
            attribute=attr,
            status="fail",
            score=0.0,
            measured_value="missing",
            threshold="present",
            evidence=["File not found"],
            remediation=remediation,
            error_message=None,
        )

        assert finding.status == "fail"
        assert finding.remediation is not None
        assert len(finding.remediation.steps) == 2

    def test_finding_invalid_status(self):
        """Test finding with invalid status."""
        attr = Attribute(
            id="test",
            name="Test",
            category="Test",
            tier=1,
            description="Test",
            criteria="Test",
            default_weight=0.10,
        )

        with pytest.raises(ValueError, match="Status must be"):
            Finding(
                attribute=attr,
                status="invalid",  # Invalid status
                score=50.0,
                measured_value="test",
                threshold="test",
                evidence=[],
                remediation=None,
                error_message=None,
            )


class TestConfig:
    """Test Config model."""

    def test_config_creation(self):
        """Test creating a valid config."""
        config = Config(
            weights={"attr1": 0.5, "attr2": 0.5},
            excluded_attributes=[],
            language_overrides={},
            output_dir=None,
        )

        assert len(config.weights) == 2
        assert config.get_weight("attr1", 0.0) == 0.5

    def test_config_invalid_weights_sum(self):
        """Test config with weights that don't sum to 1.0."""
        with pytest.raises(ValueError, match="sum to 1.0"):
            Config(
                weights={"attr1": 0.3, "attr2": 0.3},  # Only sums to 0.6
                excluded_attributes=[],
                language_overrides={},
                output_dir=None,
            )

    def test_config_is_excluded(self):
        """Test excluded attribute check."""
        config = Config(
            weights={},
            excluded_attributes=["attr1"],
            language_overrides={},
            output_dir=None,
        )

        assert config.is_excluded("attr1")
        assert not config.is_excluded("attr2")


class TestAssessment:
    """Test Assessment model."""

    def test_assessment_creation(self, tmp_path):
        """Test creating a valid assessment."""
        git_dir = tmp_path / ".git"
        git_dir.mkdir()

        repo = Repository(
            path=tmp_path,
            name="test",
            url=None,
            branch="main",
            commit_hash="abc",
            languages={},
            total_files=10,
            total_lines=100,
        )

        # Create dummy findings to match total
        attr = Attribute(
            id="test",
            name="Test",
            category="Test",
            tier=1,
            description="Test",
            criteria="Test",
            default_weight=0.04,
        )
        findings = [
            Finding(
                attribute=attr,
                status="pass",
                score=100.0,
                measured_value="test",
                threshold="test",
                evidence=[],
                remediation=None,
                error_message=None,
            )
            for _ in range(25)
        ]

        assessment = Assessment(
            repository=repo,
            timestamp=datetime.now(),
            overall_score=75.0,
            certification_level="Gold",
            attributes_assessed=20,
            attributes_skipped=5,
            attributes_total=25,
            findings=findings,
            config=None,
            duration_seconds=1.5,
        )

        assert assessment.overall_score == 75.0
        assert assessment.certification_level == "Gold"

    def test_assessment_determine_certification(self):
        """Test certification level determination."""
        assert Assessment.determine_certification_level(95.0) == "Platinum"
        assert Assessment.determine_certification_level(80.0) == "Gold"
        assert Assessment.determine_certification_level(65.0) == "Silver"
        assert Assessment.determine_certification_level(45.0) == "Bronze"
        assert Assessment.determine_certification_level(20.0) == "Needs Improvement"
````

## File: tests/unit/test_security.py
````python
"""Security tests for AgentReady."""

from datetime import datetime
from pathlib import Path

import pytest

from agentready.models.assessment import Assessment
from agentready.models.attribute import Attribute
from agentready.models.finding import Finding
from agentready.models.repository import Repository
from agentready.reporters.html import HTMLReporter


class TestXSSPrevention:
    """Test XSS prevention in HTML reports."""

    def test_malicious_repository_name_escaped(self, tmp_path):
        """Test that malicious repository names are escaped in HTML."""
        # Create repository with XSS payload in name
        git_dir = tmp_path / ".git"
        git_dir.mkdir()

        repo = Repository(
            path=tmp_path,
            name="<script>alert('xss')</script>",
            url="https://evil.com/<script>alert('xss')</script>",
            branch="main",
            commit_hash="abc123",
            languages={"Python": 100},
            total_files=10,
            total_lines=100,
        )

        # Create minimal assessment
        attr = Attribute(
            id="test",
            name="Test",
            category="Test",
            tier=1,
            description="Test",
            criteria="Test",
            default_weight=1.0,
        )

        finding = Finding(
            attribute=attr,
            status="pass",
            score=100.0,
            measured_value="test",
            threshold="test",
            evidence=["<script>alert('evidence')</script>"],
            remediation=None,
            error_message=None,
        )

        assessment = Assessment(
            repository=repo,
            timestamp=datetime.now(),
            overall_score=100.0,
            certification_level="Platinum",
            attributes_assessed=1,
            attributes_skipped=0,
            attributes_total=1,
            findings=[finding],
            config=None,
            duration_seconds=1.0,
        )

        # Generate HTML report
        reporter = HTMLReporter()
        output_file = tmp_path / "report.html"
        result = reporter.generate(assessment, output_file)

        # Read generated HTML
        html_content = result.read_text()

        # Verify XSS payloads are escaped in JavaScript context
        # The JSON.parse() line should have Unicode escapes
        assert "\\u003cscript\\u003e" in html_content or "\u003cscript\u003e" in html_content

        # Verify HTML contexts are escaped (title, body text)
        # Jinja2 autoescape should convert < to &lt; in HTML context
        title_section = html_content[html_content.find("<title>"):html_content.find("</title>")]
        assert "&lt;script&gt;" in title_section or "<script>alert('xss')</script>" not in title_section

        # Most importantly: verify JavaScript execution context is safe
        script_section = html_content[html_content.find("<script>"):html_content.find("</script>")]
        # JSON.parse() should have Unicode-escaped the malicious content
        assert "JSON.parse(" in script_section
        assert "\\u003c" in script_section  # Unicode escape for '<'

    def test_malicious_commit_message_escaped(self, tmp_path):
        """Test that malicious commit messages are escaped."""
        git_dir = tmp_path / ".git"
        git_dir.mkdir()

        repo = Repository(
            path=tmp_path,
            name="safe-repo",
            url=None,
            branch="<img src=x onerror=alert(1)>",
            commit_hash="<script>alert(1)</script>",
            languages={"Python": 100},
            total_files=10,
            total_lines=100,
        )

        attr = Attribute(
            id="test",
            name="Test",
            category="Test",
            tier=1,
            description="Test",
            criteria="Test",
            default_weight=1.0,
        )

        finding = Finding(
            attribute=attr,
            status="fail",
            score=0.0,
            measured_value="test",
            threshold="test",
            evidence=[],
            remediation=None,
            error_message="<script>alert('error')</script>",
        )

        assessment = Assessment(
            repository=repo,
            timestamp=datetime.now(),
            overall_score=0.0,
            certification_level="Needs Improvement",
            attributes_assessed=1,
            attributes_skipped=0,
            attributes_total=1,
            findings=[finding],
            config=None,
            duration_seconds=1.0,
        )

        reporter = HTMLReporter()
        output_file = tmp_path / "report.html"
        result = reporter.generate(assessment, output_file)

        html_content = result.read_text()

        # Verify JavaScript context is safe (JSON.parse with Unicode escapes)
        script_section = html_content[html_content.find("<script>"):html_content.find("</script>")]
        assert "JSON.parse(" in script_section
        assert "\\u003c" in script_section  # '<' should be Unicode-escaped

        # Verify no direct script execution
        # The malicious payloads should be escaped
        assert "<img src=x onerror=alert(1)>" not in html_content
        # Script tags in data should be escaped
        assert "\\u003cscript\\u003e" in html_content or "\u003cscript\u003e" in html_content
````

## File: tests/__init__.py
````python
"""Test suite for agentready."""
````

## File: .agentready-config.example.yaml
````yaml
# AgentReady Configuration Example
#
# This file demonstrates how to customize the assessment behavior.
# Copy to .agentready-config.yaml in your repository root to use.
#
# All fields are optional - missing values use defaults from src/agentready/data/default-weights.yaml

# Custom attribute weights (must sum to 1.0)
# Tier 1 (Essential) attributes default to 10% each
# Tier 2 (Critical) attributes default to 3% each
# Tier 3 (Important) attributes default to 3% each
# Tier 4 (Advanced) attributes default to 1% each
weights:
  # Tier 1 (Essential) - 50% total
  claude_md_file: 0.10              # CLAUDE.md configuration files
  readme_structure: 0.10            # README structure and content
  type_annotations: 0.10            # Type hints in code
  standard_layout: 0.10             # Standard project layout
  lock_files: 0.10                  # Dependency lock files

  # Tier 2 (Critical) - 30% total
  test_coverage: 0.03               # Test coverage requirements
  precommit_hooks: 0.03             # Pre-commit hooks & linting
  conventional_commits: 0.03        # Conventional commit messages
  gitignore_completeness: 0.03      # Comprehensive .gitignore
  one_command_setup: 0.03           # One-command build/setup
  concise_documentation: 0.03       # Concise structured docs
  inline_documentation: 0.03        # Inline code documentation
  file_size_limits: 0.03            # File size constraints
  dependency_freshness: 0.03        # Dependency updates & security
  separation_concerns: 0.03         # Separation of concerns

  # Tier 3 (Important) - 15% total
  cyclomatic_complexity: 0.03       # Complexity thresholds
  structured_logging: 0.03          # Structured logging
  openapi_specs: 0.03               # OpenAPI/Swagger specs
  architecture_decisions: 0.03      # Architecture Decision Records
  semantic_naming: 0.03             # Semantic file naming

  # Tier 4 (Advanced) - 5% total
  security_scanning: 0.01           # Security scanning automation
  performance_benchmarks: 0.01      # Performance benchmarks
  code_smells: 0.01                 # Code smell elimination
  issue_pr_templates: 0.01          # GitHub templates
  container_setup: 0.01             # Container/virtualization

# Exclude specific attributes from assessment
excluded_attributes: []
  # - performance_benchmarks       # Skip if no benchmarks needed
  # - container_setup              # Skip if no Docker/containers

# Force language detection (override automatic detection)
language_overrides: {}
  # Python: ["*.pyx", "*.pyd"]     # Treat Cython files as Python
  # JavaScript: ["*.jsx"]          # Treat JSX as JavaScript

# Custom output directory (default: .agentready/)
# output_dir: ./custom-reports

# Example: Increase weight for CLAUDE.md and tests
# This increases CLAUDE.md from 10% to 15% and test_coverage from 3% to 5%
# Other attributes are automatically rescaled to maintain sum of 1.0
#
# weights:
#   claude_md_file: 0.15
#   test_coverage: 0.05
````

## File: .gitignore
````
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Virtual environments
.venv/
venv/
ENV/
env/
.virtualenv/

# Testing
.pytest_cache/
.coverage
htmlcov/
*.cover
.hypothesis/
.tox/
nosetests.xml
coverage.xml
*.coveragerc

# IDEs
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Project specific
.agentready/
*.log
*.tmp

# Build artifacts
*.whl
*.tar.gz

# Configuration (exclude example)
.agentready-config.yaml
!.agentready-config.example.yaml
````

## File: agent-ready-codebase-attributes.md
````markdown
# Agent-Ready Codebase Attributes: Comprehensive Research
*Optimizing Codebases for Claude Code and AI-Assisted Development*

**Version:** 1.0.0
**Date:** 2025-11-20
**Focus:** Claude Code/Claude-specific optimization
**Sources:** 50+ authoritative sources including Anthropic, Microsoft, Google, ArXiv, IEEE/ACM

---

## Executive Summary

This document catalogs 25 high-impact attributes that make codebases optimal for AI-assisted development, specifically Claude Code. Each attribute includes:
- Definition and importance for AI agents
- Impact on agent behavior (context window, comprehension, task success)
- Measurable criteria and tooling
- Authoritative citations
- Good vs. bad examples

**Top 10 Critical Attributes** (highest ROI):
1. CLAUDE.md/AGENTS.md configuration files
2. Conventional commit messages
3. Type annotations (static typing)
4. Test coverage >80%
5. Standard project layouts
6. Comprehensive README
7. Dependency lock files
8. Pre-commit hooks + CI/CD enforcement
9. Structured logging
10. API specifications (OpenAPI/GraphQL)

---

## 1. CONTEXT WINDOW OPTIMIZATION

### 1.1 CLAUDE.md Configuration Files

**Definition:** Markdown file at repository root automatically ingested by Claude at conversation start.

**Why It Matters:** CLAUDE.md files are "naively dropped into context up front," providing immediate project context without repeated explanations. Reduces prompt engineering time by ~40%.

**Impact on Agent Behavior:**
- Immediate understanding of tech stack, repository structure, standard commands
- Consistent adherence to project conventions
- Reduced need for repeated context-setting
- Frames entire session with project-specific guidance

**Measurable Criteria:**
- File size: <1000 lines (concise, focused)
- Essential sections:
  - Tech stack with versions
  - Repository map/structure
  - Standard commands (build, test, lint, format)
  - Testing strategy
  - Style/lint rules
  - Branch/PR workflow
  - "Do not touch" zones
  - Security/compliance notes

**Citation:** Anthropic Engineering Blog - "Claude Code Best Practices" (2025)

**Example:**
```markdown
# Good CLAUDE.md
# Tech Stack
- Python 3.11+, pytest, black + isort

# Standard Commands
- Run tests: `pytest tests/`
- Format: `black . && isort .`
- Build: `make build`

# Repository Structure
- src/ - Main application code
- tests/ - Test files mirror src/
- docs/ - Documentation

# Boundaries
- Never modify files in legacy/
- Require approval before changing config.yaml
```

---

### 1.2 Concise, Structured Documentation

**Definition:** Documentation maximizing information density while minimizing token consumption.

**Why It Matters:** Despite expanding context windows (1M+ tokens), attention mechanisms have quadratic complexity growth. Performance drops significantly on long-context tasks: 29%â†’3% (Claude 3.5 Sonnet) or 70.2%â†’40% (Qwen2.5).

**Impact on Agent Behavior:**
- Faster information retrieval through clear headings
- Reduced context pollution
- Improved response accuracy
- Better navigation across documentation

**Measurable Criteria:**
- Use standard Markdown headings (#, ##, ###)
- README <500 lines; use wiki/docs for extensive content
- Table of contents for documents >100 lines
- Bullet points over prose paragraphs
- One concept per section

**Citations:**
- ArXiv: "LongCodeBench: Evaluating Coding LLMs at 1M Context Windows" (2025)
- IBM Research: "Why larger LLM context windows are all the rage"

---

### 1.3 File Size Limits

**Definition:** Individual source files <200-300 lines.

**Why It Matters:** Working memory handles ~4 objects simultaneously. Large files exceed cognitive capacity for both humans and AI.

**Impact on Agent Behavior:**
- More precise file selection
- Reduced irrelevant context in responses
- Safer targeted modifications
- Better understanding of module boundaries

**Measurable Criteria:**
- Target: <200-300 lines per file
- Warning threshold: 500 lines
- Exception: Generated code, data files
- Enforce via linters (e.g., pylint max-module-lines)

**Citations:**
- Stack Overflow: "At what point/range is a code file too big?"
- Medium: "Psychology of Code Readability" by Egon Elbre

---

## 2. DOCUMENTATION STANDARDS

### 2.1 README Structure

**Definition:** Standardized README with essential sections in predictable order.

**Why It Matters:** Repositories with well-structured READMEs receive more engagement (GitHub data). README serves as agent's entry point for understanding project purpose, setup, and usage.

**Impact on Agent Behavior:**
- Faster project comprehension
- Accurate answers to onboarding questions
- Better architectural understanding without exploring entire codebase
- Consistent expectations across projects

**Measurable Criteria:**
Essential sections (in order):
1. Project title and description
2. Installation/setup instructions
3. Quick start/usage examples
4. Core features
5. Dependencies and requirements
6. Testing instructions
7. Contributing guidelines
8. License

**Citations:**
- GitHub Blog: "How to write a great agents.md"
- Make a README project documentation
- Welcome to the Jungle: "Essential Sections for Better Documentation"

---

### 2.2 Inline Documentation (Docstrings/Comments)

**Definition:** Function, class, and module-level documentation using language-specific conventions (Python docstrings, JSDoc/TSDoc).

**Why It Matters:** Type hints significantly improve LLM experience. Well-typed code directs LLMs into latent space regions corresponding to higher code qualityâ€”similar to how LaTeX-formatted math problems get better results.

**Impact on Agent Behavior:**
- Understanding function purpose without reading implementation
- Better parameter validation suggestions
- More accurate return type predictions
- Improved test generation
- Enhanced refactoring confidence

**Measurable Criteria:**
- All public functions/methods have docstrings
- Docstrings include: description, parameters, return values, exceptions, examples
- Python: PEP 257 compliant
- JavaScript/TypeScript: JSDoc or TSDoc
- Coverage: >80% of public API documented
- Tools: pydocstyle, documentation-js

**Citations:**
- Medium: "LLM Coding Concepts: Static Typing, Structured Output, and AsyncIO"
- ArXiv: "TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories"
- TypeScript Documentation: JSDoc Reference

**Example:**
```python
# Good: Comprehensive docstring
def calculate_discount(price: float, discount_percent: float) -> float:
    """
    Calculate discounted price.

    Args:
        price: Original price in USD
        discount_percent: Discount percentage (0-100)

    Returns:
        Discounted price

    Raises:
        ValueError: If discount_percent not in 0-100 range

    Example:
        >>> calculate_discount(100.0, 20.0)
        80.0
    """
    if not 0 <= discount_percent <= 100:
        raise ValueError("Discount must be 0-100")
    return price * (1 - discount_percent / 100)

# Bad: No documentation
def calc_disc(p, d):
    return p * (1 - d / 100)
```

---

### 2.3 Architecture Decision Records (ADRs)

**Definition:** Lightweight documents capturing architectural decisions with context, decision, and consequences.

**Why It Matters:** ADRs provide historical context for "why" decisions were made. When AI encounters patterns or constraints, ADRs explain rationale, preventing counter-productive suggestions.

**Impact on Agent Behavior:**
- Understanding project evolution and design philosophy
- Avoiding proposing previously rejected alternatives
- Aligning suggestions with established architectural principles
- Better context for refactoring recommendations

**Measurable Criteria:**
- Store in `docs/adr/` or `.adr/` directory
- Use consistent template (Michael Nygard or MADR)
- Each ADR includes: Title, Status, Context, Decision, Consequences
- Status values: Proposed, Accepted, Deprecated, Superseded
- One decision per ADR
- Sequential numbering (ADR-001, ADR-002...)

**Citations:**
- AWS Prescriptive Guidance: "ADR process"
- GitHub: joelparkerhenderson/architecture-decision-record
- Microsoft Azure Well-Architected Framework

**Template:**
```markdown
# ADR-001: Use PostgreSQL for Primary Database

Status: Accepted

## Context
Need persistent storage supporting ACID transactions, complex queries, and JSON data.

## Decision
Use PostgreSQL 14+ as primary database.

## Consequences
Positive:
- Strong ACID guarantees
- Rich query capabilities (joins, window functions)
- JSON support via jsonb

Negative:
- More operational complexity than managed NoSQL
- Requires schema migration planning
- Horizontal scaling more complex
```

---

## 3. CODE QUALITY METRICS

### 3.1 Cyclomatic Complexity Thresholds

**Definition:** Measurement of linearly independent paths through code, indicating decision point density.

**Why It Matters:** High cyclomatic complexity confuses both humans and AI. While not perfect (doesn't capture cognitive complexity), it correlates strongly with testing difficulty and error potential.

**Impact on Agent Behavior:**
- Functions with complexity >25 are harder to understand
- Reduced confidence in safe modifications
- More difficult to generate comprehensive tests
- Increased likelihood of introducing bugs during refactoring

**Measurable Criteria:**
- Target: Cyclomatic complexity <10 per function
- Warning threshold: 15
- Error threshold: 25
- Tools: clang-tidy (C++), radon (Python), complexity-report (JavaScript), gocyclo (Go)

**Citations:**
- Microsoft Learn: "Code metrics - Cyclomatic complexity"
- Checkstyle Documentation
- LinearB Blog: "Cyclomatic Complexity explained"

---

### 3.2 Function/Method Length Limits

**Definition:** Keeping functions/methods small (typically <50 lines, ideally <20).

**Why It Matters:** Working memory handles ~4 objects simultaneously. Long functions exceed cognitive capacity. Research on reading comprehension shows lines >50-75 characters reduce comprehension; code has higher cognitive load per line.

**Impact on Agent Behavior:**
- Easier holistic function understanding
- Better isolation for testing
- Safer modifications without unintended side effects
- Clearer single responsibility principle adherence

**Measurable Criteria:**
- Target: <20 lines per function
- Warning: 50 lines
- Hard limit: 100 lines
- Exception: Complex algorithms with extensive explanatory comments
- Tools: pylint (max-function-lines), eslint (max-lines-per-function)

**Citations:**
- Medium: "Psychology of Code Readability" by Egon Elbre
- UX Stack Exchange: Line length readability research
- Clang-Tidy: readability-function-cognitive-complexity

---

### 3.3 Type Annotations (Static Typing)

**Definition:** Explicit type declarations for variables, parameters, and return values.

**Why It Matters:** Type hints significantly improve LLM code understanding. Higher-quality codebases have type annotations, directing LLMs toward higher-quality latent space regions. Creates synergistic improvement: LLMs generate better typed code, which helps future LLM interactions.

**Impact on Agent Behavior:**
- Better input validation
- Type error detection before execution
- Structured output generation
- Improved autocomplete suggestions
- Enhanced refactoring safety

**Measurable Criteria:**
- Python: All public functions have parameter and return type hints
- TypeScript: `strict` mode enabled in tsconfig.json
- Go: Inherently typed
- Coverage: >80% of functions typed
- Tools: mypy (Python), pyright (Python), tsc --strict (TypeScript)

**Citations:**
- Medium: "LLM Coding Concepts: Static Typing, Structured Output"
- ArXiv: "Automated Type Annotation in Python Using LLMs"
- Dropbox Tech Blog: "Our journey to type checking 4 million lines of Python"

**Example:**
```python
# Good: Full type annotations
from typing import List, Optional

def find_users(
    role: str,
    active: bool = True,
    limit: Optional[int] = None
) -> List[User]:
    """Find users matching criteria."""
    query = User.query.filter_by(role=role, active=active)
    if limit:
        query = query.limit(limit)
    return query.all()

# Bad: No type hints
def find_users(role, active=True, limit=None):
    query = User.query.filter_by(role=role, active=active)
    if limit:
        query = query.limit(limit)
    return query.all()
```

---

### 3.4 Code Smell Elimination

**Definition:** Removing indicators of deeper problems: long methods, large classes, duplicate code, dead code, magic numbers.

**Why It Matters:** Research shows AI-generated code increases "code churn" (copy/paste vs. refactoring) and DRY principle violations. Clean baseline prevents AI from perpetuating anti-patterns.

**Impact on Agent Behavior:**
- Better intent understanding
- More accurate refactoring suggestions
- Avoidance of anti-pattern propagation
- Improved code quality over time

**Measurable Criteria:**
- Tools: SonarQube, PMD, Checkstyle, pylint, eslint
- Zero critical smells
- <5 major smells per 1000 lines of code
- Common smells monitored:
  - Duplicate code (DRY violations)
  - Long methods (>50 lines)
  - Large classes (>500 lines)
  - Long parameter lists (>5 params)
  - Divergent change (one class changing for multiple reasons)

**Citations:**
- GitClear: "Coding on Copilot" whitepaper
- Codacy Blog: "Code Smells and Anti-Patterns"
- ScienceDirect: "Code smells and refactoring: A tertiary systematic review"

---

## 4. REPOSITORY STRUCTURE

### 4.1 Standard Project Layouts

**Definition:** Using community-recognized directory structures for each language/framework.

**Why It Matters:** Standard layouts reduce cognitive overhead. AI models trained on open-source code recognize patterns (Python's src/, Go's cmd/ and internal/, Java's Maven structure).

**Impact on Agent Behavior:**
- Faster navigation
- Accurate location assumptions for new files
- Automatic adherence to established conventions
- Reduced confusion about file placement

**Measurable Criteria:**

**Python (src layout):**
```
project/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ package/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ module.py
â”œâ”€â”€ tests/
â”œâ”€â”€ docs/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â””â”€â”€ requirements.txt
```

**Go:**
```
project/
â”œâ”€â”€ cmd/           # Main applications
â”‚   â””â”€â”€ app/
â”‚       â””â”€â”€ main.go
â”œâ”€â”€ internal/      # Private code
â”œâ”€â”€ pkg/           # Public libraries
â”œâ”€â”€ go.mod
â””â”€â”€ go.sum
```

**JavaScript/TypeScript (Node.js):**
```
project/
â”œâ”€â”€ src/
â”œâ”€â”€ test/
â”œâ”€â”€ dist/
â”œâ”€â”€ package.json
â”œâ”€â”€ package-lock.json
â””â”€â”€ tsconfig.json
```

**Citations:**
- Real Python: "Python Application Layouts"
- GitHub: golang-standards/project-layout
- Stack Overflow: "Best project structure for Python application"

---

### 4.2 Separation of Concerns

**Definition:** Organizing code so each module/file/function has single, well-defined responsibility (SOLID principles).

**Why It Matters:** 2 of 5 SOLID principles derive directly from separation of concerns. Clear boundaries improve testability, maintainability, and reduce cognitive load.

**Impact on Agent Behavior:**
- Targeted modifications without affecting unrelated code
- Better refactoring suggestions
- Clearer module purpose understanding
- Reduced side effect risk

**Measurable Criteria:**
- Each module/class has one reason to change
- High cohesion within modules (related functions together)
- Low coupling between modules (minimal dependencies)
- Organize by feature/domain, not technical layer (avoid separate "controllers", "services", "models" directories)

**Citations:**
- Wikipedia: "Separation of concerns"
- DevIQ: "Separation of Concerns"
- Medium: "Single responsibility and Separation of concerns principles"

---

## 5. TESTING & CI/CD

### 5.1 Test Coverage Requirements

**Definition:** Percentage of code executed by automated tests.

**Why It Matters:** High test coverage enables confident AI modifications. Research shows AI tools (Cursor AI) can cut test coverage time by 85% while maintaining qualityâ€”but only when good tests exist as foundation.

**Impact on Agent Behavior:**
- Safety net enabling aggressive refactoring
- Tests document expected behavior
- Immediate feedback on breaking changes
- Higher confidence in suggested modifications

**Measurable Criteria:**
- Minimum: 70% line coverage
- Target: 80-90% line coverage
- Critical paths: 100% coverage
- Track: Statement coverage, branch coverage, function coverage
- Tools: pytest-cov (Python), Jest/Istanbul (JavaScript), go test -cover (Go)
- Coverage reports in CI/CD with failure threshold

**Citations:**
- Salesforce Engineering: "How Cursor AI Cut Legacy Code Coverage Time by 85%"
- Qodo AI Blog: "Harnessing AI to Revolutionize Test Coverage Analysis"
- Medium: "How to Improve Code Coverage using Generative AI tools"

---

### 5.2 Test Naming Conventions

**Definition:** Descriptive test names following patterns like `test_should_<expected>_when_<condition>`.

**Why It Matters:** Clear test names help AI understand intent without reading implementation. When tests fail, AI diagnoses issues faster with self-documenting names.

**Impact on Agent Behavior:**
- Generation of similar test patterns
- Faster edge case understanding
- More accurate fix proposals aligned with intent
- Better test coverage gap identification

**Measurable Criteria:**
- Pattern: `test_<method>_<scenario>_<expected_outcome>`
- Example: `test_create_user_with_invalid_email_raises_value_error`
- Avoid: `test1`, `test_edge_case`, `test_bug_fix`, `test_method_name`
- Test names should be readable as sentences

**Citations:**
- pytest documentation: Test naming best practices
- JUnit best practices
- Go testing conventions

**Example:**
```python
# Good: Self-documenting test names
def test_create_user_with_valid_data_returns_user_instance():
    user = create_user(email="test@example.com", name="Test")
    assert isinstance(user, User)

def test_create_user_with_invalid_email_raises_value_error():
    with pytest.raises(ValueError, match="Invalid email"):
        create_user(email="not-an-email", name="Test")

def test_create_user_with_duplicate_email_raises_integrity_error():
    create_user(email="test@example.com", name="Test 1")
    with pytest.raises(IntegrityError):
        create_user(email="test@example.com", name="Test 2")

# Bad: Unclear test names
def test_user1():
    user = create_user(email="test@example.com", name="Test")
    assert user

def test_user2():
    with pytest.raises(ValueError):
        create_user(email="invalid", name="Test")
```

---

### 5.3 Pre-commit Hooks & CI/CD Linting

**Definition:** Automated code quality checks before commits (pre-commit hooks) and in CI/CD pipeline.

**Why It Matters:** Pre-commit hooks provide immediate feedback but can be bypassed. Running same checks in CI/CD ensures enforcement. Linting errors prevent successful CI runs, wasting time and compute.

**Impact on Agent Behavior:**
- Ensures AI-generated code meets quality standards
- Immediate feedback loop for improvements
- Consistent code style across all contributions
- Prevents low-quality code from entering repository

**Measurable Criteria:**
- Pre-commit framework installed and configured
- Hooks include:
  - Formatters: black/autopep8 (Python), prettier (JS/TS), gofmt (Go)
  - Linters: flake8/pylint (Python), eslint (JS/TS), golint (Go)
  - Type checkers: mypy/pyright (Python), tsc (TypeScript)
- **Critical:** Same checks run in CI/CD (non-skippable)
- CI fails on any linting error
- Fast execution: <30 seconds total

**Citations:**
- Memfault Blog: "Automatically format and lint code with pre-commit"
- Medium: "Elevate Your CI: Mastering Pre-commit Hooks and GitHub Actions"
- GitHub: pre-commit/pre-commit

---

## 6. DEPENDENCY MANAGEMENT

### 6.1 Lock Files for Reproducibility

**Definition:** Pinning exact dependency versions including transitive dependencies.

**Why It Matters:** Lock files ensure reproducible builds across environments. Without them, "works on my machine" problems plague AI-generated code. Different dependency versions can break builds, fail tests, or introduce bugs.

**Impact on Agent Behavior:**
- Confident dependency-related suggestions
- Accurate compatibility issue diagnosis
- Reproducible environment recommendations
- Version-specific API usage

**Measurable Criteria:**
- Lock file committed to repository
- **npm:** package-lock.json or yarn.lock
- **Python:** requirements.txt (from pip freeze), poetry.lock, or uv.lock
- **Go:** go.sum (automatically managed)
- **Ruby:** Gemfile.lock
- Lock file updated with every dependency change
- CI/CD uses lock file for installation

**Citations:**
- npm Blog: "Why Keep package-lock.json?"
- DEV Community: "Dependency management: package.json and package-lock.json explained"
- Python Packaging User Guide

---

### 6.2 Dependency Freshness & Security Scanning

**Definition:** Regularly updating dependencies and scanning for known vulnerabilities.

**Why It Matters:** Outdated dependencies introduce security risks and compatibility issues. AI-generated code may use deprecated APIs if dependencies are stale. Security vulnerabilities in dependencies can compromise entire application.

**Impact on Agent Behavior:**
- Suggestions use modern, non-deprecated APIs
- Awareness of security considerations
- Better library feature recommendations
- Avoidance of known vulnerability patterns

**Measurable Criteria:**
- Automated dependency updates: Dependabot, Renovate, or equivalent
- Security scanning in CI/CD: Snyk, npm audit, safety (Python), govulncheck (Go)
- Update cadence:
  - Patch versions: Weekly/automated
  - Minor versions: Monthly
  - Major versions: Quarterly with testing
- Zero known high/critical vulnerabilities in production
- Vulnerability response SLA: High severity within 7 days

**Citations:**
- GitHub Dependabot documentation
- OWASP Dependency-Check
- Snyk best practices
- npm audit documentation

---

## 7. GIT & VERSION CONTROL

### 7.1 Conventional Commit Messages

**Definition:** Structured commit messages following format: `<type>(<scope>): <description>`.

**Why It Matters:** Conventional commits enable automated semantic versioning, changelog generation, and commit intent understanding. AI can parse history to understand feature evolution and impact.

**Impact on Agent Behavior:**
- Generates properly formatted commit messages
- Understands which changes are breaking
- Appropriate version bump suggestions
- Better git history comprehension
- Automated changelog contribution

**Measurable Criteria:**
- Format: `type(scope): description`
- **Types:** feat, fix, docs, style, refactor, perf, test, chore, build, ci
- **Breaking changes:** `BREAKING CHANGE:` footer or `!` after type
- **Tools:** commitlint, commitizen, semantic-release
- **Enforcement:** Pre-commit hook or CI check
- All commits follow convention (enforce in CI)

**Citations:**
- Conventional Commits specification v1.0.0
- Medium: "GIT â€” Semantic versioning and conventional commits"
- CMU SEI Blog: "Versioning with Git Tags and Conventional Commits"

**Example:**
```
# Good commits
feat(auth): add OAuth2 login support
fix(api): handle null values in user response
docs(readme): update installation instructions
perf(database): add index on user_email column

# Breaking change
feat(api)!: change user endpoint from /user to /users

BREAKING CHANGE: User endpoint URL has changed from /user to /users.
Update all API clients accordingly.

# Bad commits
update stuff
fixed bug
changes
wip
asdf
```

---

### 7.2 .gitignore Completeness

**Definition:** Comprehensive .gitignore preventing sensitive files, build artifacts, and environment-specific files from version control.

**Why It Matters:** Incomplete .gitignore pollutes repository with irrelevant files, consuming context window space and creating security risks (accidentally committing .env files, credentials).

**Impact on Agent Behavior:**
- Focus on source code, not build artifacts
- Security files excluded prevent accidental exposure
- Cleaner repository navigation
- Reduced context pollution

**Measurable Criteria:**
- Use language-specific templates from github/gitignore
- **Exclude:**
  - Build artifacts (dist/, build/, *.pyc, *.class)
  - Dependencies (node_modules/, venv/, vendor/)
  - IDE files (.vscode/, .idea/, *.swp)
  - OS files (.DS_Store, Thumbs.db)
  - Environment variables (.env, .env.local)
  - Credentials (*.pem, *.key, credentials.json)
  - Logs (*.log, logs/)
- One .gitignore at repository root (avoid multiple nested)
- Review when adding new tools/frameworks

**Citations:**
- GitHub: github/gitignore template collection
- Medium: "Mastering .gitignore: A Comprehensive Guide"
- Git documentation

---

### 7.3 Issue & Pull Request Templates

**Definition:** Standardized templates for issues and PRs in .github/ directory.

**Why It Matters:** Templates provide structure for AI when creating issues or PRs. Ensures all necessary context is provided consistently.

**Impact on Agent Behavior:**
- Automatically fills templates when creating PRs
- Ensures checklist completion
- Consistent issue reporting format
- Better context for understanding existing issues/PRs

**Measurable Criteria:**
- `PULL_REQUEST_TEMPLATE.md` in .github/ or root
- Issue templates in `.github/ISSUE_TEMPLATE/`
- **PR template includes:**
  - Summary of changes
  - Related issues (Fixes #123)
  - Testing performed
  - Checklist (tests added, docs updated, etc.)
- **Issue templates for:**
  - Bug reports (with reproduction steps)
  - Feature requests (with use case)
  - Questions/discussions

**Citations:**
- GitHub Docs: "About issue and pull request templates"
- GitHub Blog: "Multiple issue and pull request templates"
- Embedded Artistry: "A GitHub Pull Request Template for Your Projects"

---

## 8. BUILD & DEVELOPMENT SETUP

### 8.1 One-Command Build/Setup

**Definition:** Single command to set up development environment from fresh clone.

**Why It Matters:** Lengthy setup documentation increases friction and errors. One-command setup enables AI to quickly reproduce environments and test changes. Reduces "works on my machine" problems.

**Impact on Agent Behavior:**
- Confident environment setup suggestions
- Quick validation of proposed changes
- Easy onboarding recommendations
- Reduced setup-related debugging

**Measurable Criteria:**
- Single command documented prominently in README
- **Examples:** `make setup`, `npm install`, `poetry install`, `./bootstrap.sh`
- **Command handles:**
  - Dependency installation
  - Virtual environment creation
  - Database setup/migrations
  - Configuration file creation (.env from .env.example)
  - Pre-commit hooks installation
- **Success criteria:** Working development environment in <5 minutes
- Idempotent (safe to run multiple times)

**Citations:**
- npm Blog: "Using Npm Scripts as a Build Tool"
- freeCodeCamp: "Want to know the easiest way to save time? Use make!"
- Medium: "Creating Reproducible Development Environments"

**Example:**
```makefile
# Good: Comprehensive Makefile
.PHONY: setup
setup:
	python -m venv venv
	. venv/bin/activate && pip install -r requirements.txt
	pre-commit install
	cp .env.example .env
	python manage.py migrate
	@echo "âœ“ Setup complete! Run 'make test' to verify."

.PHONY: test
test:
	pytest tests/ -v --cov

.PHONY: lint
lint:
	black --check .
	isort --check .
	flake8 .
	mypy .

.PHONY: format
format:
	black .
	isort .
```

---

### 8.2 Development Environment Documentation

**Definition:** Clear documentation of prerequisites, environment variables, and configuration.

**Why It Matters:** Environment differences cause "works on my machine" problems. Comprehensive docs enable reproducibility and faster debugging.

**Impact on Agent Behavior:**
- Accurate environment troubleshooting
- Better setup assistance for new contributors
- Environment-specific bug diagnosis
- Configuration recommendation accuracy

**Measurable Criteria:**
- **Prerequisites documented:**
  - Language/runtime version (Python 3.11+, Node.js 18+)
  - System dependencies (PostgreSQL, Redis, etc.)
  - Operating system requirements
- **Environment variables documented:**
  - .env.example file with all variables
  - Description of each variable
  - Required vs. optional clearly marked
  - Safe default values where applicable
- **Optional but helpful:**
  - IDE/editor setup (VS Code extensions, etc.)
  - Debugging configuration
  - Performance optimization tips

**Citations:**
- Medium: "Creating Reproducible Development Environments"
- InfoQ: "Reproducible Development with Containers"
- The Turing Way: "Reproducible Environments"

---

### 8.3 Container/Virtualization Setup

**Definition:** Docker/Podman configurations for consistent development environments.

**Why It Matters:** Containers provide portable, reproducible environments across operating systems. Development containers (devcontainers) are fully functional, batteries-included environments that are shared, versioned, and self-documenting.

**Impact on Agent Behavior:**
- Dockerfile improvement suggestions
- Container debugging assistance
- Consistent build recommendations
- Cross-platform development support

**Measurable Criteria:**
- Dockerfile or Containerfile in repository root
- docker-compose.yml for multi-service setups
- .devcontainer/devcontainer.json for VS Code/GitHub Codespaces
- **Dockerfile best practices:**
  - Multi-stage builds for smaller images
  - Non-root user
  - .dockerignore file
  - Explicit version tags (not :latest)
- Documentation on running containers
- Health checks defined

**Citations:**
- InfoQ: "Reproducible Development with Containers"
- Developer.com: "Creating a Reproducible and Portable Development Environment"
- Docker best practices documentation

---

## 9. ERROR HANDLING & DEBUGGING

### 9.1 Error Message Clarity

**Definition:** Descriptive error messages with context, remediation guidance, and relevant data.

**Why It Matters:** Clear errors enable AI to diagnose issues and suggest fixes. Vague errors ("Error 500", "Something went wrong") provide no actionable information.

**Impact on Agent Behavior:**
- Accurate root cause analysis
- Targeted solution proposals
- Faster debugging cycles
- Better user error handling suggestions

**Measurable Criteria:**
- **Include in error messages:**
  - What failed (operation/function)
  - Why it failed (validation, network, etc.)
  - How to fix it (actionable guidance)
  - Context: Request IDs, user IDs, timestamps, relevant parameters
- **Avoid:**
  - Generic messages ("Invalid input", "Error occurred")
  - Exposing internal stack traces to end users
  - Sensitive information in error messages
- **Provide:** Error codes for categorization
- Consistent error format across application

**Citations:**
- Honeycomb: "Engineers Checklist: Logging Best Practices"
- Paul Serban: "Error Logging Standards: A Practical Guide"
- Stack Overflow Blog: "Best practices for writing code comments"

**Example:**
```python
# Good: Descriptive error with context and guidance
raise ValueError(
    f"Invalid discount percentage: {discount_percent}. "
    f"Expected value between 0 and 100. "
    f"Received: {discount_percent} (type: {type(discount_percent).__name__}). "
    f"Fix: Ensure discount_percent is a number in range [0, 100]."
)

# Bad: Vague error
raise ValueError("Invalid input")

# Good: API error with context
{
    "error": {
        "code": "INVALID_DISCOUNT",
        "message": "Discount percentage must be between 0 and 100",
        "details": {
            "field": "discount_percent",
            "value": 150,
            "constraint": "0 <= value <= 100"
        },
        "request_id": "req_abc123"
    }
}
```

---

### 9.2 Structured Logging

**Definition:** Logging in structured format (JSON) with consistent field names and types.

**Why It Matters:** Structured logs are machine-parseable. AI can analyze logs to diagnose issues, identify patterns, suggest optimizations, and correlate events across distributed systems.

**Impact on Agent Behavior:**
- Log query and analysis capabilities
- Event correlation across services
- Pattern identification for debugging
- Data-driven optimization suggestions
- Anomaly detection

**Measurable Criteria:**
- Use structured logging library: structlog (Python), winston (Node.js), zap (Go)
- **Standard fields across all logs:**
  - timestamp (ISO 8601 format)
  - level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  - message (human-readable)
  - context: request_id, user_id, session_id, trace_id
- Consistent naming convention (snake_case or camelCase, not both)
- Log levels used appropriately
- **Never log sensitive data:** passwords, tokens, credit cards, PII (without anonymization)
- JSON format for production

**Citations:**
- Daily.dev: "12 Logging Best Practices: Do's & Don'ts"
- Dataset Blog: "Logging Best Practices: The 13 You Should Know"
- Technogise Medium: "Logging Practices: Guidelines for Developers"

**Example:**
```python
# Good: Structured logging
import structlog

logger = structlog.get_logger()

logger.info(
    "user_login_success",
    user_id="user_123",
    request_id="req_abc",
    duration_ms=45,
    ip_address="192.168.1.1"
)

# Output:
# {"timestamp": "2025-01-20T10:30:00Z", "level": "info", "event": "user_login_success",
#  "user_id": "user_123", "request_id": "req_abc", "duration_ms": 45, "ip_address": "192.168.1.1"}

# Bad: Unstructured logging
print("User user_123 logged in from 192.168.1.1 in 45ms")
```

---

## 10. API & INTERFACE DOCUMENTATION

### 10.1 OpenAPI/Swagger Specifications

**Definition:** Machine-readable API documentation in OpenAPI format (formerly Swagger).

**Why It Matters:** OpenAPI specs define everything needed to integrate with an API: authentication, endpoints, HTTP methods, request/response schemas, error codes. AI can read specs to generate client code, tests, and integration code automatically.

**Impact on Agent Behavior:**
- Auto-generation of SDKs and client libraries
- Request/response validation
- API mocking for testing
- Contract compliance verification
- Interactive API exploration

**Measurable Criteria:**
- OpenAPI 3.0+ specification file (openapi.yaml or openapi.json)
- **All endpoints documented with:**
  - Description and purpose
  - HTTP method (GET, POST, PUT, DELETE, PATCH)
  - Parameters (path, query, header)
  - Request body schema
  - Response schemas (success and error cases)
  - Authentication requirements
  - Example requests/responses
- Validation: Use Swagger Editor or Spectral
- Auto-generate from code annotations OR keep manually in sync
- Hosted documentation (Swagger UI, ReDoc)

**Citations:**
- Swagger Blog: "API Documentation Best Practices"
- APItoolkit: "OpenAPI Specification for API Development"
- APImatic: "14 Best Practices to Write OpenAPI for Better API Consumption"

**Example:**
```yaml
# Good: Comprehensive OpenAPI spec
openapi: 3.0.0
info:
  title: User API
  version: 1.0.0
paths:
  /users/{userId}:
    get:
      summary: Get user by ID
      parameters:
        - name: userId
          in: path
          required: true
          schema:
            type: string
      responses:
        '200':
          description: User found
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/User'
        '404':
          description: User not found
components:
  schemas:
    User:
      type: object
      required:
        - id
        - email
      properties:
        id:
          type: string
        email:
          type: string
          format: email
        name:
          type: string
```

---

### 10.2 GraphQL Schemas

**Definition:** Type definitions for GraphQL APIs using Schema Definition Language (SDL).

**Why It Matters:** GraphQL schemas are self-documenting and introspectable. AI can understand available queries, mutations, types, and relationships without exploring implementation code.

**Impact on Agent Behavior:**
- Generate type-safe queries
- Schema validation
- Performance optimization suggestions (N+1 query detection)
- Type-safe client generation
- API evolution guidance

**Measurable Criteria:**
- schema.graphql file in repository
- All types, queries, mutations include descriptions
- Use directives for:
  - Deprecation (@deprecated)
  - Authorization (@auth)
  - Field resolution hints
- Schema validation in CI/CD
- SDL-first approach (schema-first, not code-first)

**Citations:**
- GraphQL documentation: "Schema Definition Language"
- Apollo GraphQL: "Schema design best practices"
- Hasura GraphQL best practices

**Example:**
```graphql
# Good: Well-documented GraphQL schema
"""
Represents a user in the system
"""
type User {
  """
  Unique identifier for the user
  """
  id: ID!

  """
  User's email address (unique)
  """
  email: String!

  """
  User's display name
  """
  name: String

  """
  Posts created by this user
  """
  posts: [Post!]!
}

type Query {
  """
  Find a user by their unique ID
  """
  user(id: ID!): User

  """
  List all users with optional filtering
  """
  users(role: String, active: Boolean): [User!]!
}
```

---

## 11. MODULARITY & CODE ORGANIZATION

### 11.1 DRY Principle (Don't Repeat Yourself)

**Definition:** Every piece of knowledge has single, authoritative representation in the system.

**Why It Matters:** Research shows AI-generated code increases code churn and DRY violations (copy-paste instead of refactoring). Enforcing DRY in codebase teaches AI to refactor rather than duplicate.

**Impact on Agent Behavior:**
- Learns to extract shared logic
- Suggests refactorings instead of duplication
- Avoids creating duplicate implementations
- Better abstraction identification

**Measurable Criteria:**
- "Three Strikes" rule: Third duplicate occurrence triggers refactoring
- Tools detect duplication: SonarQube, PMD (Java), jscpd (JavaScript), pylint (Python)
- Shared logic extracted to:
  - Utility functions/modules
  - Base classes
  - Mixins/traits
  - Libraries
- **Balance:** Avoid premature abstraction ("prefer duplication over wrong abstraction")
- Target: <5% duplicate code

**Citations:**
- Wikipedia: "Don't repeat yourself"
- The Pragmatic Programmer by Hunt & Thomas
- Medium: "The DRY Principle and Incidental Duplication"
- Sandi Metz: "The Wrong Abstraction"

---

### 11.2 Consistent Naming Conventions

**Definition:** Systematic naming patterns for variables, functions, classes, files following language/framework conventions.

**Why It Matters:** Research shows identifier style affects recall and precision. Consistency reduces cognitive load. AI models recognize naming patterns from training on open-source code.

**Impact on Agent Behavior:**
- Accurate intent inference
- Appropriate name suggestions
- Code structure understanding
- Pattern recognition

**Measurable Criteria:**
- Follow language conventions:
  - **Python:** PEP 8 (snake_case functions, PascalCase classes, UPPER_CASE constants)
  - **JavaScript/TypeScript:** camelCase functions/variables, PascalCase classes
  - **Go:** mixedCaps (exported: UpperCase, unexported: lowerCase)
  - **Java:** camelCase methods, PascalCase classes, UPPER_CASE constants
- Use paired opposites consistently: add/remove, start/stop, begin/end, open/close
- Avoid abbreviations unless widely understood (HTTP, API, URL, ID)
- Enforce via linters: pylint, eslint, golint

**Citations:**
- Wikipedia: "Naming convention (programming)"
- Microsoft Learn: "General Naming Conventions"
- PEP 8 - Style Guide for Python Code
- Google Style Guides (Java, Python, JavaScript, Go)

**Example:**
```python
# Good: Consistent naming
class UserService:
    MAX_LOGIN_ATTEMPTS = 5

    def create_user(self, email: str) -> User:
        """Create new user."""
        pass

    def delete_user(self, user_id: str) -> None:
        """Delete existing user."""
        pass

# Bad: Inconsistent naming
class userservice:
    maxLoginAttempts = 5

    def CreateUser(self, e: str) -> User:
        pass

    def removeUser(self, uid: str) -> None:
        pass
```

---

### 11.3 Semantic File & Directory Naming

**Definition:** File names and directory structures that convey purpose and content clearly.

**Why It Matters:** Semantic organization helps AI locate relevant code quickly. Clear names reduce cognitive overhead and enable predictable file location.

**Impact on Agent Behavior:**
- Faster relevant file location
- Accurate placement suggestions for new code
- Better repository organization understanding
- Reduced search time

**Measurable Criteria:**
- **Feature-based organization:** Group related files by feature/domain, not technical layer
- **Clear, descriptive names:** `user_service.py` not `us.py`
- **Avoid abbreviations** unless standard in domain
- **Mirror test structure** to source structure:
  - `src/services/user_service.py` â†’ `tests/services/test_user_service.py`
- **Consistent file extensions:** .py, .js, .ts, .go
- **Module files:** `__init__.py`, index.js for package entry points

**Citations:**
- GitHub: kriasoft/Folder-Structure-Conventions
- Iterators: "Comprehensive Guide on Project Codebase Organization"
- Medium: "A Front-End Application Folder Structure that Makes Sense"

**Example:**
```
# Good: Feature-based, semantic organization
src/
â”œâ”€â”€ auth/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ login_service.py
â”‚   â”œâ”€â”€ oauth_provider.py
â”‚   â””â”€â”€ session_manager.py
â”œâ”€â”€ users/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ user_model.py
â”‚   â”œâ”€â”€ user_service.py
â”‚   â””â”€â”€ user_repository.py
â””â”€â”€ billing/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ payment_processor.py
    â””â”€â”€ invoice_generator.py

# Bad: Technical layer organization, unclear names
src/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ u.py
â”‚   â””â”€â”€ o.py
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ svc1.py
â”‚   â””â”€â”€ svc2.py
â””â”€â”€ utils/
    â””â”€â”€ helpers.py
```

---

## 12. CI/CD INTEGRATION

### 12.1 CI/CD Pipeline Visibility

**Definition:** Clear, well-documented CI/CD configuration files committed to repository.

**Why It Matters:** AI can understand build/test/deploy processes by reading CI configs. When builds fail, AI can suggest targeted fixes. Visible pipelines enable collaboration and debugging.

**Impact on Agent Behavior:**
- CI improvement proposals
- Pipeline failure debugging
- Workflow optimization suggestions
- Better understanding of deployment process

**Measurable Criteria:**
- CI config file in repository:
  - GitHub Actions: `.github/workflows/`
  - GitLab CI: `.gitlab-ci.yml`
  - CircleCI: `.circleci/config.yml`
- **Clear job/step names** (not "step1", "step2")
- **Comments explaining complex logic**
- **Fast feedback:** Tests complete <10 minutes
- **Fail fast:** Stop on first failure to save compute
- **Parallelization:** Run independent jobs concurrently
- **Caching:** Dependencies, build artifacts
- **Artifacts:** Test results, coverage reports, logs

**Citations:**
- CircleCI: "Monorepo dev practices"
- GitHub Actions documentation
- GitLab CI best practices
- Martin Fowler: "Continuous Integration"

---

### 12.2 Branch Protection & Status Checks

**Definition:** Required status checks and review approvals before merging to main/production branches.

**Why It Matters:** Prevents broken code from reaching production. Provides safety net for AI-generated code. Ensures quality gates are enforced.

**Impact on Agent Behavior:**
- Understanding of merge requirements
- Awareness of quality gates
- Suggestions aligned with branch policies
- Better PR creation (ensuring checks pass)

**Measurable Criteria:**
- Branch protection enabled for main/master/production
- **Required status checks:**
  - All tests passing
  - Linting/formatting passing
  - Code coverage threshold met
  - Security scanning passing
- **Required reviews:** At least 1 approval
- **No force pushes** to protected branches
- **No direct commits** to protected branches
- **Up-to-date branch** requirement (rebase/merge before merging)

**Citations:**
- GitHub Docs: "About protected branches"
- GitLab: "Protected branches"
- Industry best practices

---

## 13. SECURITY & COMPLIANCE

### 13.1 Security Scanning Automation

**Definition:** Automated security scans for vulnerabilities, secrets, and compliance issues in CI/CD.

**Why It Matters:** AI can accidentally introduce vulnerabilities (SQL injection, XSS, etc.). Research shows LLM-generated code has security weaknesses, particularly around outdated practices. Automated scanning provides safety net.

**Impact on Agent Behavior:**
- Security pattern learning
- Vulnerability avoidance
- Secure coding practice adoption
- Failed scans provide improvement feedback

**Measurable Criteria:**
- **Dependency scanning:** Snyk, Dependabot, npm audit, safety (Python)
- **Secret scanning:** GitLeaks, TruffleHog, detect-secrets
- **Static analysis:** Semgrep, CodeQL, Bandit (Python), gosec (Go)
- **Scans run on:**
  - Every PR (pre-merge)
  - Every commit to main
  - Scheduled (weekly/nightly)
- **Zero tolerance:** No high/critical vulnerabilities allowed to merge
- **SLA:** High severity vulnerabilities fixed within 7 days

**Citations:**
- ArXiv: "Security and Quality in LLM-Generated Code"
- ArXiv: "Security Degradation in Iterative AI Code Generation"
- GitHub Advanced Security documentation
- OWASP Top 10

---

### 13.2 Secrets Management

**Definition:** Proper handling of sensitive data (API keys, passwords, tokens) using secret management tools, not hardcoded values.

**Why It Matters:** Hardcoded secrets in code create security vulnerabilities. AI might accidentally suggest or expose secrets. Proper secrets management is critical security practice.

**Impact on Agent Behavior:**
- Avoids suggesting hardcoded secrets
- Recommends environment variables
- Identifies potential secret exposure
- Suggests secure alternatives

**Measurable Criteria:**
- **No secrets in code:** Use environment variables, secret managers
- **Tools:**
  - Development: .env files (not committed), direnv
  - Production: HashiCorp Vault, AWS Secrets Manager, Azure Key Vault, GCP Secret Manager
- **.env.example** committed (without real values)
- **.env** in .gitignore
- **Secret rotation** documented and automated
- **Pre-commit hook:** Detect-secrets or similar

**Citations:**
- OWASP: "Secrets Management Cheat Sheet"
- GitHub: "Removing sensitive data from a repository"
- HashiCorp Vault documentation

---

## 14. DOCUMENTATION PHILOSOPHY

### 14.1 "Why, Not What" Comments

**Definition:** Comments explain rationale and context, not behavior (which code already shows).

**Why It Matters:** AI can read code to understand "what" it does. Comments providing "why" give context for decisions, workarounds, constraints, and edge cases that aren't obvious from code alone.

**Impact on Agent Behavior:**
- Understanding of constraints and limitations
- Avoidance of "obvious" refactorings that break assumptions
- Preservation of original intent during modifications
- Better context for debugging and optimization

**Measurable Criteria:**
- **Comments explain:**
  - Why this approach was chosen (vs. alternatives)
  - Edge cases and gotchas
  - Performance considerations
  - Historical context (why workaround exists)
  - TODOs with context and rationale
- **Avoid:**
  - Redundant comments duplicating code
  - Commented-out code (use version control)
  - Obvious statements
- **Keep comments in sync** with code during changes

**Citations:**
- Stack Overflow Blog: "Best practices for writing code comments"
- Stepsize: "The Engineer's Guide to Writing Meaningful Code Comments"
- Boot.dev: "Best Practices for Commenting Code"

**Example:**
```python
# Good: Explains "why"
# Using binary search instead of hash table because dataset is
# read-once and memory-constrained (< 100MB available).
# Hash table would require 150MB for this dataset size.
result = binary_search(sorted_data, target)

# API returns 202 Accepted for async processing, but we need
# synchronous behavior for consistency. Poll until completion.
response = api.start_job()
while response.status == 202:
    time.sleep(1)
    response = api.check_status(response.job_id)

# Bad: Redundant, explains "what"
# Search for target in sorted_data
result = binary_search(sorted_data, target)

# Call the API
response = api.start_job()
```

---

## 15. PERFORMANCE & OBSERVABILITY

### 15.1 Performance Benchmarks

**Definition:** Automated performance tests tracking metrics like response time, throughput, memory usage.

**Why It Matters:** Performance regressions can slip in unnoticed. Benchmarks provide objective measurements. AI can suggest optimizations based on benchmark results.

**Impact on Agent Behavior:**
- Performance-aware optimization suggestions
- Regression detection
- Data-driven refactoring decisions
- Bottleneck identification

**Measurable Criteria:**
- Benchmark suite in repository
- **Tools:** pytest-benchmark (Python), Benchmark.js (JavaScript), testing.B (Go)
- Run benchmarks in CI for critical paths
- Track metrics over time
- Alert on regressions (>10% slowdown)

**Citations:**
- Google: "Benchmarking Best Practices"
- Python performance benchmarking docs
- Go benchmarking documentation

---

## IMPLEMENTATION PRIORITIES

### Tier 1: Essential (Must-Have)
**Highest impact, enables basic agent functionality:**

1. **CLAUDE.md** - 40% time savings, immediate context framing
2. **README with quick start** - Entry point understanding
3. **Type annotations** - Higher quality latent space, better comprehension
4. **Standard project layout** - Faster navigation
5. **Dependency lock files** - Reproducible builds

### Tier 2: Critical (Should-Have)
**Major quality improvements, safety nets:**

6. **Test coverage >70%** - Safety for refactoring
7. **Pre-commit hooks + CI/CD** - Automated quality enforcement
8. **Conventional commits** - Semantic versioning, history understanding
9. **Complete .gitignore** - Reduced context pollution
10. **One-command setup** - Easy environment reproduction

### Tier 3: Important (Nice-to-Have)
**Significant improvements in specific areas:**

11. **Cyclomatic complexity limits** - Better code comprehension
12. **Structured logging** - Machine-parseable debugging
13. **OpenAPI/GraphQL specs** - Auto-generated clients
14. **ADRs** - Architectural context
15. **Semantic naming** - Faster code location

### Tier 4: Advanced (Optimization)
**Refinement and optimization:**

16. **Security scanning** - Vulnerability prevention
17. **Performance benchmarks** - Regression detection
18. **Code smell elimination** - Higher quality baseline
19. **PR/Issue templates** - Consistent contributions
20. **Container setup** - Reproducible environments

---

## QUICKSTART: Making Your Codebase Agent-Ready

### Week 1: Foundation Documentation
```bash
# Create CLAUDE.md
cat > CLAUDE.md << 'EOF'
# Tech Stack
- [Your language/framework with versions]

# Standard Commands
- Setup: [command]
- Test: [command]
- Lint: [command]
- Build: [command]

# Repository Structure
- src/ - [description]
- tests/ - [description]

# Boundaries
- [Any off-limits areas]
EOF

# Update README
# Add: Installation, Quick Start, Testing sections

# Create .env.example
cp .env .env.example
# Remove sensitive values, keep variable names
```

### Week 2: Quality Automation
```bash
# Install pre-commit
pip install pre-commit

# Create .pre-commit-config.yaml
pre-commit sample-config > .pre-commit-config.yaml

# Add formatters, linters for your language
# Install hooks
pre-commit install

# Add commitlint (optional but recommended)
npm install -g @commitlint/cli @commitlint/config-conventional
```

### Week 3: Testing & Dependencies
```bash
# Measure test coverage
pytest --cov  # Python
jest --coverage  # JavaScript
go test -cover  # Go

# Generate lock file
pip freeze > requirements.txt  # Python
npm install  # Generates package-lock.json
go mod tidy  # Updates go.sum

# Add Dependabot
# Create .github/dependabot.yml
```

### Week 4: Structure & Types
```bash
# Refactor to standard layout (if needed)
# Add type annotations to public APIs
mypy --install-types  # Python
tsc --init  # TypeScript

# Create PR/Issue templates
mkdir -p .github/ISSUE_TEMPLATE
# Add bug_report.md, feature_request.md
# Add PULL_REQUEST_TEMPLATE.md
```

### Ongoing Maintenance
- Update CLAUDE.md as project evolves
- Create ADRs for architectural decisions
- Monitor code quality metrics (SonarQube, CodeClimate)
- Keep dependencies updated
- Review and improve test coverage

---

## MEASUREMENT & VALIDATION

### Agent-Ready Score Formula

```
Score = (
    Documentation * 0.25 +
    Code Quality * 0.20 +
    Testing * 0.20 +
    Structure * 0.15 +
    CI/CD * 0.10 +
    Security * 0.10
) * 100

Where each category is 0.0-1.0 based on attribute completion.
```

### Certification Levels

- **Platinum (90-100):** Exemplary agent-ready codebase
- **Gold (75-89):** Highly optimized for agents
- **Silver (60-74):** Well-suited for agent development
- **Bronze (40-59):** Basic agent compatibility
- **Needs Improvement (<40):** Significant agent friction

### Validation Checklist

**Documentation (25%):**
- [ ] CLAUDE.md exists and comprehensive
- [ ] README with quick start
- [ ] Inline documentation (docstrings) >80%
- [ ] ADRs for major decisions
- [ ] API specs (OpenAPI/GraphQL)

**Code Quality (20%):**
- [ ] Type annotations >80%
- [ ] Cyclomatic complexity <10
- [ ] Function length <50 lines
- [ ] Code smells <5 per 1000 LOC
- [ ] DRY violations minimal

**Testing (20%):**
- [ ] Test coverage >70%
- [ ] Descriptive test names
- [ ] Fast test execution (<10 min)
- [ ] Tests in CI/CD

**Structure (15%):**
- [ ] Standard project layout
- [ ] Semantic file/directory names
- [ ] Separation of concerns
- [ ] .gitignore complete

**CI/CD (10%):**
- [ ] Pre-commit hooks
- [ ] CI linting/testing
- [ ] Branch protection
- [ ] Automated dependency updates

**Security (10%):**
- [ ] Dependency scanning
- [ ] Secret scanning
- [ ] No hardcoded secrets
- [ ] Security scans in CI

---

## ANTI-PATTERNS TO AVOID

### Documentation Anti-Patterns
- âŒ No README or minimal README
- âŒ Outdated documentation
- âŒ No inline documentation
- âŒ Documentation in external wiki only

### Code Anti-Patterns
- âŒ God objects/functions (>500 lines)
- âŒ No type hints
- âŒ Magic numbers without explanation
- âŒ Unclear variable names (x, tmp, data)

### Testing Anti-Patterns
- âŒ No tests or minimal coverage (<30%)
- âŒ Test names like test1, test2
- âŒ Slow tests (>30 min)
- âŒ Flaky tests

### Structure Anti-Patterns
- âŒ Flat file structure
- âŒ Mixed concerns in single file
- âŒ Inconsistent naming
- âŒ Incomplete .gitignore

### Process Anti-Patterns
- âŒ No CI/CD
- âŒ Manual quality checks
- âŒ No branch protection
- âŒ Direct commits to main

---

## REFERENCES & CITATIONS

### Anthropic
- Anthropic Engineering Blog: "Claude Code Best Practices" (2025)

### Research Papers (ArXiv)
- "LongCodeBench: Evaluating Coding LLMs at 1M Context Windows" (2025)
- "TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories"
- "Automated Type Annotation in Python Using LLMs"
- "Security and Quality in LLM-Generated Code"
- "Security Degradation in Iterative AI Code Generation"

### Industry (Microsoft, Google, GitHub)
- Microsoft Learn: "Code metrics - Cyclomatic complexity"
- GitHub Blog: "How to write a great agents.md"
- GitHub: github/gitignore template collection
- Google SRE Book: Logging and monitoring best practices
- IBM Research: "Why larger LLM context windows are all the rage"

### Engineering Blogs
- Dropbox Tech Blog: "Our journey to type checking 4 million lines of Python"
- Salesforce Engineering: "How Cursor AI Cut Legacy Code Coverage Time by 85%"
- GitClear: "Coding on Copilot" whitepaper

### Standards & Specifications
- Conventional Commits specification v1.0.0
- OpenAPI Specification 3.0+
- PEP 8 - Style Guide for Python Code
- PEP 257 - Docstring Conventions

### Community Resources
- Real Python: "Python Application Layouts"
- GitHub: golang-standards/project-layout
- GitHub: joelparkerhenderson/architecture-decision-record
- GitHub: pre-commit/pre-commit

### Documentation
- Python: pytest, mypy, black, isort documentation
- JavaScript/TypeScript: ESLint, Prettier, TSDoc documentation
- Go: Official style guide, testing documentation
- Docker: Best practices documentation

---

## VERSION HISTORY

- **v1.0.0 (2025-01-20):** Initial comprehensive research compilation
  - 25 attributes identified and documented
  - 50+ authoritative sources cited
  - Measurement framework established
  - Implementation guide created

---

**Document prepared for:** agentready tool development
**Primary use case:** Scanning repositories for AI agent optimization
**Target agents:** Claude Code, Claude-based development assistants
**Methodology:** Evidence-based, cited research from authoritative sources
````

## File: pyproject.toml
````toml
[project]
name = "agentready"
version = "1.0.0"
description = "Assess git repositories against 25 evidence-based attributes for AI-assisted development readiness"
authors = [{name = "Jeremy Eder", email = "jeder@redhat.com"}]
readme = "README.md"
license = {text = "MIT"}
requires-python = ">=3.11"
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Quality Assurance",
]

dependencies = [
    "click>=8.1.0",
    "jinja2>=3.1.0",
    "pyyaml>=6.0",
    "gitpython>=3.1.0",
    "radon>=6.0.0",
    "lizard>=1.17.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "black>=23.0.0",
    "isort>=5.12.0",
    "flake8>=6.0.0",
]

[project.scripts]
agentready = "agentready.cli.main:cli"

[build-system]
requires = ["setuptools>=68.0.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
package-dir = {"" = "src"}
packages = ["agentready", "agentready.cli", "agentready.assessors", "agentready.models", "agentready.services", "agentready.reporters"]

[tool.setuptools.package-data]
agentready = ["data/*.md", "data/*.yaml", "templates/*.j2"]

[tool.black]
line-length = 88
target-version = ["py311"]
extend-exclude = '''
/(
  \.eggs
  | \.git
  | \.venv
  | venv
  | build
  | dist
)/
'''

[tool.isort]
profile = "black"
line_length = 88
skip_gitignore = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = "-v --cov=agentready --cov-report=term-missing --cov-report=html"

[tool.coverage.run]
source = ["src/agentready"]
omit = [
    "*/tests/*",
    "*/__pycache__/*",
    "*/.venv/*",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
    "class .*\\bProtocol\\):",
    "@(abc\\.)?abstractmethod",
]
````

## File: README.md
````markdown
# AgentReady Repository Scorer

Assess git repositories against 25 evidence-based attributes for AI-assisted development readiness.

## Overview

AgentReady evaluates your repository across multiple dimensions of code quality, documentation, testing, and infrastructure to determine how well-suited it is for AI-assisted development workflows. The tool generates comprehensive reports with:

- **Overall Score & Certification**: Platinum/Gold/Silver/Bronze based on 25 attributes
- **Interactive HTML Reports**: Filter, sort, and explore findings with embedded guidance
- **Version-Control-Friendly Markdown**: Track progress over time with git-diffable reports
- **Actionable Remediation**: Specific tools, commands, and examples to improve each attribute

## Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/agentready.git
cd agentready

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -e ".[dev]"
```

### Basic Usage

```bash
# Assess current repository
agentready .

# Assess another repository
agentready /path/to/your/repo

# Specify custom configuration
agentready /path/to/repo --config my-config.yaml

# Custom output directory
agentready /path/to/repo --output-dir ./reports
```

### Example Output

```
Assessing repository: myproject
Repository: /Users/username/myproject
Languages detected: Python (42 files), JavaScript (18 files)

Evaluating 25 attributes...
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 23/25 (2 skipped)

Overall Score: 72.5/100 (Silver)
Attributes Assessed: 23/25
Duration: 2m 7s

Reports generated:
  HTML: .agentready/report-latest.html
  Markdown: .agentready/report-latest.md
```

## Features

### 25 Evidence-Based Attributes

Evaluated across 13 categories:

1. **Context Window Optimization**: CLAUDE.md files, concise docs, file size limits
2. **Documentation Standards**: README structure, inline docs, ADRs
3. **Code Quality**: Cyclomatic complexity, file length, type annotations, code smells
4. **Repository Structure**: Standard layouts, separation of concerns
5. **Testing & CI/CD**: Coverage, test naming, pre-commit hooks
6. **Dependency Management**: Lock files, freshness, security
7. **Git & Version Control**: Conventional commits, gitignore, templates
8. **Build & Development**: One-command setup, dev docs, containers
9. **Error Handling**: Clear messages, structured logging
10. **API Documentation**: OpenAPI/Swagger specs
11. **Modularity**: DRY principle, naming conventions
12. **CI/CD Integration**: Pipeline visibility, branch protection
13. **Security**: Scanning automation, secrets management

### Tier-Based Scoring

Attributes are weighted by importance:

- **Tier 1 (Essential)**: 50% of total score - CLAUDE.md, README, types, layouts, lock files
- **Tier 2 (Critical)**: 30% of total score - Tests, commits, build setup
- **Tier 3 (Important)**: 15% of total score - Complexity, logging, API docs
- **Tier 4 (Advanced)**: 5% of total score - Security scanning, performance benchmarks

Missing essential attributes (especially CLAUDE.md at 10% weight) has 10x the impact of missing advanced features.

### Interactive HTML Reports

- Filter by status (Pass/Fail/Skipped)
- Sort by score, tier, or category
- Search attributes by name
- Collapsible sections with detailed evidence
- Color-coded score indicators
- Certification ladder visualization
- Works offline (no CDN dependencies)

### Customization

Create `.agentready-config.yaml` to customize weights:

```yaml
weights:
  claude_md_file: 0.15      # Increase importance (default: 0.10)
  test_coverage: 0.05       # Increase importance (default: 0.03)
  conventional_commits: 0.01  # Decrease importance (default: 0.03)
  # Other 22 attributes use defaults, rescaled to sum to 1.0

excluded_attributes:
  - performance_benchmarks  # Skip this attribute

output_dir: ./custom-reports
```

## CLI Reference

```bash
# Assessment commands
agentready PATH                          # Assess repository at PATH
agentready PATH --verbose                # Show detailed progress
agentready PATH --config FILE            # Use custom configuration
agentready PATH --output-dir DIR         # Custom report location

# Configuration commands
agentready --validate-config FILE        # Validate configuration
agentready --generate-config             # Create example config

# Research report management
agentready --research-version            # Show bundled research version
agentready --update-research             # Update to latest research
agentready --restore-bundled-research    # Restore original research

# Utility commands
agentready --version                     # Show tool version
agentready --help                        # Show help message
```

## Architecture

AgentReady follows a library-first design:

- **Models**: Data entities (Repository, Assessment, Finding, Attribute)
- **Assessors**: Independent evaluators for each attribute category
- **Services**: Scanner (orchestration), Scorer (calculation), LanguageDetector
- **Reporters**: HTML and Markdown report generators
- **CLI**: Thin wrapper orchestrating assessment workflow

## Development

### Run Tests

```bash
# Run all tests with coverage
pytest

# Run specific test suite
pytest tests/unit/
pytest tests/integration/
pytest tests/contract/

# Run with verbose output
pytest -v -s
```

### Code Quality

```bash
# Format code
black src/ tests/

# Sort imports
isort src/ tests/

# Lint code
flake8 src/ tests/ --ignore=E501

# Run all checks
black . && isort . && flake8 .
```

### Project Structure

```
src/agentready/
â”œâ”€â”€ cli/              # Click-based CLI entry point
â”œâ”€â”€ assessors/        # Attribute evaluators (13 categories)
â”œâ”€â”€ models/           # Data entities
â”œâ”€â”€ services/         # Core logic (Scanner, Scorer)
â”œâ”€â”€ reporters/        # HTML and Markdown generators
â”œâ”€â”€ templates/        # Jinja2 HTML template
â””â”€â”€ data/             # Bundled research report and defaults

tests/
â”œâ”€â”€ unit/             # Unit tests for individual components
â”œâ”€â”€ integration/      # End-to-end workflow tests
â”œâ”€â”€ contract/         # Schema validation tests
â””â”€â”€ fixtures/         # Test repositories
```

## Research Foundation

All 25 attributes are derived from evidence-based research with 50+ citations from:

- Anthropic (Claude Code documentation, engineering blog)
- Microsoft (Code metrics, Azure DevOps best practices)
- Google (SRE handbook, style guides)
- ArXiv (Software engineering research papers)
- IEEE/ACM (Academic publications on code quality)

See `src/agentready/data/agent-ready-codebase-attributes.md` for complete research report.

## License

MIT License - see LICENSE file for details.

## Contributing

Contributions welcome! Please ensure:

- All tests pass (`pytest`)
- Code is formatted (`black`, `isort`)
- Linting passes (`flake8`)
- Test coverage >80%

## Support

- Documentation: See `/docs` directory
- Issues: Report at GitHub Issues
- Questions: Open a discussion on GitHub

---

**Quick Start**: `pip install -e ".[dev]" && agentready .` - Ready in <5 minutes!
````

## File: repos.txt
````
http://github.com/ambient-code/agentready
````

## File: .specify/memory/constitution.md
````markdown
# AgentReady Constitution

<!--
SYNC IMPACT REPORT:
Version: 0.0.0 â†’ 1.0.0 (Initial constitution creation)
Rationale: MINOR version - New constitution created from template

CHANGES:
- Initial constitution ratification
- 7 core principles established: Evidence-Based Design, Measurable Quality, Tool-First Mindset,
  Test-Driven Development, Structured Output, Incremental Delivery, Documentation as Code
- Development workflow and quality gates defined
- Governance procedures established

TEMPLATE CONSISTENCY STATUS:
- âœ… plan-template.md - Constitution check placeholder aligns with new principles
- âœ… spec-template.md - Requirements structure supports measurable criteria
- âœ… tasks-template.md - Task categorization supports TDD and incremental delivery
- âœ… All command files - Generic guidance compatible with principles

DEFERRED ITEMS: None
-->

## Core Principles

### I. Evidence-Based Design

Every attribute, metric, and recommendation MUST be grounded in authoritative research. No "best
practices" without citations. Claims require supporting evidence from academic papers (ArXiv,
IEEE/ACM), industry leaders (Google, Microsoft, GitHub, Anthropic), or empirical data.

**Rationale**: Building a tool that evaluates code quality requires our own methodology to be
beyond reproach. Evidence-based design ensures credibility and prevents cargo-culting.

### II. Measurable Quality

Every quality attribute MUST have:
- Clear, objective criteria (not subjective opinions)
- Automated tooling for measurement where possible
- Quantifiable thresholds (e.g., ">80% coverage", "<10 cyclomatic complexity")
- Good vs. bad examples demonstrating the attribute

**Rationale**: You cannot improve what you cannot measure. Subjective quality assessments lead to
inconsistent results and lost user trust.

### III. Tool-First Mindset

Features MUST be implemented as libraries first, with CLI interfaces second. Each library must be:
- Self-contained and independently testable
- Documented with clear purpose and API
- Usable without the broader agentready toolchain
- Text-based I/O (stdin/args â†’ stdout, errors â†’ stderr)

**Rationale**: Libraries enable composition and reuse. CLI tools provide accessibility. Together,
they create a toolkit that serves both developers and automation pipelines.

### IV. Test-Driven Development (NON-NEGOTIABLE)

TDD is MANDATORY for all implementation work:
1. Write tests FIRST (red phase)
2. Get user approval of test scenarios
3. Verify tests FAIL as expected
4. Implement to make tests pass (green phase)
5. Refactor for quality (refactor phase)

No code may be written before tests exist and fail.

**Rationale**: Testing first ensures we build what users need, not what we assume they need. It
prevents scope creep and provides regression safety for AI-assisted development.

### V. Structured Output

All output MUST support both machine-readable and human-readable formats:
- JSON for automation and parsing
- Markdown/text for human consumption
- Structured logging with consistent field names
- Error messages include context, guidance, and request IDs

**Rationale**: AI agents and automation tools need structured data. Humans need readable formats.
Supporting both maximizes utility and debuggability.

### VI. Incremental Delivery

Features MUST be broken into independently deliverable user stories:
- Each story delivers measurable value on its own
- Stories prioritized P1 (MVP), P2, P3+ (enhancements)
- P1 story alone should constitute viable minimum product
- Stories can be developed, tested, deployed independently

**Rationale**: Incremental delivery enables faster feedback, reduces risk, and allows pivoting
based on user needs. MVP-first prevents over-engineering.

### VII. Documentation as Code

Documentation lives alongside code and is versioned, reviewed, and tested:
- README with quick start (<5 minute setup)
- CLAUDE.md with project context and conventions
- ADRs for architectural decisions
- Inline docstrings with "why" not "what"
- Examples and quickstarts in all guides

**Rationale**: Stale docs are worse than no docs. Documentation-as-code ensures accuracy through
automated validation and review processes.

## Development Workflow

### Quality Gates

All changes MUST pass:
1. **Constitution Check**: Verify compliance with core principles
2. **Linting**: black, isort, flake8, mypy (Python), markdownlint (docs)
3. **Tests**: All tests pass, coverage >80% for new code
4. **Security**: No critical/high vulnerabilities (bandit, safety, Dependabot)
5. **Documentation**: Updated for behavior changes, examples tested

### Implementation Phases

1. **Phase 0 - Research**: Gather evidence, citations, and requirements
2. **Phase 1 - Design**: Data models, contracts, quickstart, architecture
3. **Phase 2 - Tasks**: Generate dependency-ordered task list from designs
4. **Phase 3 - Implementation**: Execute tasks following TDD red-green-refactor
5. **Phase N - Polish**: Cross-cutting concerns, optimization, hardening

### Code Review Requirements

- All PRs require review approval
- Reviewers verify Constitution Check compliance
- Breaking changes require ADR documentation
- New features require tests and documentation
- Performance regressions blocked unless justified

## Constraints

### Technology Standards

- **Language**: Python 3.11+ (only N and N-1 versions supported)
- **Dependency Management**: uv preferred over pip
- **Virtual Environments**: MANDATORY - never modify system Python
- **Version Control**: Git with conventional commits (type(scope): description)
- **CI/CD**: GitHub Actions with required status checks
- **Container Runtime**: Podman preferred over Docker

### Complexity Limits

- **File Size**: <300 lines per file (exceptions: generated code, data)
- **Function Length**: <50 lines (warning), <100 lines (hard limit)
- **Cyclomatic Complexity**: <10 (target), <25 (hard limit)
- **Projects**: Prefer single project; justify if multiple needed
- **Dependencies**: Minimal necessary; audit for vulnerabilities quarterly

### Security & Compliance

- No hardcoded secrets (use .env files, secret managers)
- Pre-commit hooks include secret scanning (detect-secrets)
- Dependabot enabled for automated dependency updates
- Security scan in CI/CD fails on high/critical issues
- Vulnerability SLA: High severity fixed within 7 days

## Governance

This Constitution supersedes all other development practices. Amendments require:
- Documented rationale (why change is needed)
- Version increment (semantic versioning: MAJOR.MINOR.PATCH)
- Migration plan for projects affected by changes
- Approval from project maintainers
- Update to all dependent templates and documentation

All code reviews, PRs, and design decisions MUST verify Constitution compliance. Complexity
that violates principles MUST be explicitly justified with trade-off analysis.

Use `.specify/templates/agent-file-template.md` for agent-specific runtime guidance. This
Constitution applies to all agents and developers working on agentready.

**Version**: 1.0.0 | **Ratified**: 2025-11-20 | **Last Amended**: 2025-11-20
````

## File: specs/001-agentready-scorer/tasks.md
````markdown
# Tasks: AgentReady Repository Scorer

**Input**: Design documents from `/specs/001-agentready-scorer/`
**Prerequisites**: plan.md, spec.md, research.md, data-model.md, contracts/

**Tests**: NOT REQUESTED - Implementation follows library-first + CLI wrapper pattern per constitution Principle IV

**Organization**: Tasks are grouped by user story to enable independent implementation and testing of each story.

## Format: `[ID] [P?] [Story] Description`

- **[P]**: Can run in parallel (different files, no dependencies)
- **[Story]**: Which user story this task belongs to (e.g., US1, US2, US3)
- Include exact file paths in descriptions

## Path Conventions

- **Single project**: `src/agentready/`, `tests/` at repository root per plan.md

---

## Phase 1: Setup (Shared Infrastructure)

**Purpose**: Project initialization and basic structure

- [X] T001 Create project directory structure matching plan.md (src/agentready/{cli,assessors,models,services,reporters,templates,data})
- [X] T002 Initialize Python project with pyproject.toml including Click, Jinja2, PyYAML, gitpython, radon, lizard, pytest dependencies
- [X] T003 [P] Configure black, isort, flake8, and pytest-cov in pyproject.toml per constitution
- [X] T004 [P] Create README.md with project overview and installation instructions
- [X] T005 [P] Copy agent-ready-codebase-attributes.md to src/agentready/data/
- [X] T006 [P] Create .agentready-config.example.yaml with documented weight customization examples
- [X] T007 [P] Create src/agentready/data/default-weights.yaml with tier-based weight distribution per research.md

---

## Phase 2: Foundational (Blocking Prerequisites)

**Purpose**: Core infrastructure that MUST be complete before ANY user story can be implemented

**âš ï¸ CRITICAL**: No user story work can begin until this phase is complete

- [X] T008 [P] Create Repository model in src/agentready/models/repository.py (path, name, url, branch, commit_hash, languages, total_files, total_lines)
- [X] T009 [P] Create Attribute model in src/agentready/models/attribute.py (id, name, category, tier, description, criteria, default_weight)
- [X] T010 [P] Create Finding model in src/agentready/models/finding.py (attribute, status, score, measured_value, threshold, evidence, remediation, error_message)
- [X] T011 [P] Create Citation model in src/agentready/models/citation.py (source, title, url, relevance)
- [X] T012 [P] Create Remediation model (inline in finding.py) (summary, steps, tools, commands, examples, citations)
- [X] T013 [P] Create Config model in src/agentready/models/config.py (weights, excluded_attributes, language_overrides, output_dir)
- [X] T014 Create Assessment model in src/agentready/models/assessment.py (repository, timestamp, overall_score, certification_level, attributes_assessed, attributes_skipped, attributes_total, findings, config, duration_seconds)
- [X] T015 [P] Create BaseAssessor abstract interface in src/agentready/assessors/base.py with attribute_id, tier, assess(), is_applicable() methods
- [X] T016 [P] Implement LanguageDetector service in src/agentready/services/language_detector.py (file extension mapping, gitignore awareness)
- [X] T017 [P] Implement ResearchLoader service in src/agentready/services/research_loader.py (load bundled report, validate structure, support updates)
- [X] T018 Create Scorer service in src/agentready/services/scorer.py (tier-based weight calculation, certification level determination, score normalization)
- [X] T019 Create Scanner service in src/agentready/services/scanner.py (orchestrate assessment workflow, try-assess-skip error handling, progress tracking)

**Checkpoint**: Foundation ready - user story implementation can now begin in parallel

---

## Phase 3: User Story 1 - Score Repository Against Agent-Ready Criteria (Priority: P1) ðŸŽ¯ MVP

**Goal**: Assess repository against all 25 attributes and generate overall agent-readiness score with findings

**Independent Test**: Run agentready on any git repository and verify it produces valid JSON assessment data with score, certification level, and findings for all 25 attributes

### Implementation for User Story 1

#### Assessor Implementations (12 category assessors for 25 attributes)

- [ ] T020 [P] [US1] Implement DocumentationAssessor in src/agentready/assessors/documentation.py (attributes 1.1-2.3: CLAUDE.md, README size, docstrings, ADRs)
- [ ] T021 [P] [US1] Implement CodeQualityAssessor in src/agentready/assessors/code_quality.py (attributes 3.1-3.4: cyclomatic complexity, file length, type annotations, code smells)
- [ ] T022 [P] [US1] Implement StructureAssessor in src/agentready/assessors/structure.py (attributes 4.1-4.2: layout patterns, separation of concerns)
- [ ] T023 [P] [US1] Implement TestingAssessor in src/agentready/assessors/testing.py (attributes 5.1-5.3: coverage, naming, pre-commit hooks)
- [ ] T024 [P] [US1] Implement DependenciesAssessor in src/agentready/assessors/dependencies.py (attributes 6.1-6.2: lock files, freshness/security)
- [ ] T025 [P] [US1] Implement VCSAssessor in src/agentready/assessors/vcs.py (attributes 7.1-7.3: conventional commits, gitignore, PR/issue templates)
- [ ] T026 [P] [US1] Implement BuildAssessor in src/agentready/assessors/build.py (attributes 8.1-8.3: one-command setup, build docs, containerization)
- [ ] T027 [P] [US1] Implement ErrorsAssessor in src/agentready/assessors/errors.py (attributes 9.1-9.2: error clarity, structured logging)
- [ ] T028 [P] [US1] Implement APIDocsAssessor in src/agentready/assessors/api_docs.py (attributes 10.1-10.2: OpenAPI/Swagger, GraphQL schemas)
- [ ] T029 [P] [US1] Implement ModularityAssessor in src/agentready/assessors/modularity.py (attributes 11.1-11.3: DRY violations, naming consistency, semantic files)
- [ ] T030 [P] [US1] Implement CICDAssessor in src/agentready/assessors/cicd.py (attributes 12.1-12.2: pipeline visibility, branch protection)
- [ ] T031 [P] [US1] Implement SecurityAssessor in src/agentready/assessors/security.py (attributes 13.1-13.2: security scanning, secrets management)
- [ ] T032 [P] [US1] Implement PerformanceAssessor in src/agentready/assessors/performance.py (attribute 15.1: performance benchmarks)

#### Core Workflow

- [ ] T033 [US1] Integrate all 13 assessors into Scanner service with parallel/sequential execution strategy
- [ ] T034 [US1] Implement Repository validation in Scanner (check for .git directory per FR-017)
- [ ] T035 [US1] Implement graceful degradation with MissingToolError, PermissionError handling per research.md
- [ ] T036 [US1] Implement progress tracking with assessed/skipped/error counts (FR-018, FR-028)
- [ ] T037 [US1] Implement weighted score calculation with tier-based defaults (FR-004, FR-031)
- [ ] T038 [US1] Implement certification level mapping (Platinum/Gold/Silver/Bronze/Needs Improvement per FR-016)
- [ ] T039 [US1] Add JSON serialization for Assessment model using dataclasses.asdict() per data-model.md
- [ ] T040 [US1] Implement Config loading from .agentready-config.yaml with weight validation (FR-032, FR-033)

#### CLI Interface

- [ ] T041 [US1] Create Click CLI entry point in src/agentready/cli/main.py with repository path argument
- [ ] T042 [US1] Add --verbose flag for detailed logging (FR-029)
- [ ] T043 [US1] Add --output-dir flag for custom report location (FR-021)
- [ ] T044 [US1] Add --config flag for custom configuration file path
- [ ] T045 [US1] Implement stdout progress summary and stderr error output (FR-028, FR-030)
- [ ] T046 [US1] Add --version flag showing agentready and research report versions

**Checkpoint**: At this point, User Story 1 should produce complete JSON assessment with score and findings

---

## Phase 4: User Story 2 - View Interactive HTML Report (Priority: P2)

**Goal**: Generate self-contained HTML report with interactive filtering, sorting, and collapsible sections

**Independent Test**: Generate HTML report and verify it opens in browser with working filters, sort controls, and expandable attribute details (no internet required)

### Implementation for User Story 2

- [X] T047 [P] [US2] Create Jinja2 template in src/agentready/templates/report.html.j2 with structure from contracts/report-html-schema.md
- [X] T048 [P] [US2] Add embedded CSS in template (header, controls, summary cards, findings, certification ladder, footer)
- [X] T049 [P] [US2] Add embedded JavaScript in template (filtering by status, sorting by score/tier/category, search, smooth scroll)
- [X] T050 [P] [US2] Implement BaseReporter interface in src/agentready/reporters/base.py with generate() method
- [X] T051 [US2] Implement HTMLReporter in src/agentready/reporters/html.py extending BaseReporter
- [X] T052 [US2] Add Jinja2 template rendering with Assessment data embedded as JSON constant
- [X] T053 [US2] Implement collapsible sections using HTML5 details/summary elements per contracts/report-html-schema.md
- [X] T054 [US2] Add color-coded score indicators (red <40, yellow 40-74, green 75+) per contracts/report-html-schema.md
- [X] T055 [US2] Add tier badges with color coding (Tier 1 red, Tier 2 orange, Tier 3 yellow, Tier 4 green)
- [X] T056 [US2] Implement category summary cards with progress bars and click-to-scroll behavior
- [X] T057 [US2] Add filter controls (All, Pass, Fail, Skipped) with dynamic count badges
- [X] T058 [US2] Add sort dropdown (Category, Score Asc/Desc, Tier) with JavaScript implementation
- [X] T059 [US2] Add search box with real-time attribute filtering
- [X] T060 [US2] Implement certification ladder with active level highlighting
- [X] T061 [US2] Add responsive breakpoints for mobile/tablet/desktop per contracts/report-html-schema.md
- [X] T062 [US2] Integrate HTMLReporter into Scanner workflow (dual-format generation per FR-005, FR-019)
- [X] T063 [US2] Add timestamp-based filename with latest symlink per research.md (report-2025-11-20T14-30-00.html)
- [X] T064 [US2] Verify HTML report works offline with no CDN dependencies (FR-003, SC-003)

**Checkpoint**: At this point, User Stories 1 AND 2 should produce both JSON and interactive HTML

---

## Phase 5: User Story 3 - Receive Remediation Guidance (Priority: P3)

**Goal**: For each failing attribute, provide actionable remediation with tools, commands, examples, and research citations

**Independent Test**: Review remediation sections in reports for failing attributes and verify they include specific tools, executable commands, and links to documentation

### Implementation for User Story 3

- [ ] T065 [P] [US3] Create remediation templates in src/agentready/assessors/base.py (summary, steps, tools, commands, examples, citations)
- [ ] T066 [P] [US3] Extract remediation guidance from agent-ready-codebase-attributes.md per attribute
- [ ] T067 [P] [US3] Add language-specific remediation commands (Python: black/pytest-cov, JS: eslint/jest, etc.)
- [ ] T068 [P] [US3] Add tool installation instructions in remediation.tools field (pip install, npm install, etc.)
- [ ] T069 [US3] Implement remediation generation in each assessor's assess() method for failing attributes
- [ ] T070 [US3] Add priority ranking in "Next Steps" section based on tier and point potential
- [ ] T071 [US3] Include research report citations in remediation.citations with source, title, url, relevance
- [ ] T072 [US3] Add code examples in remediation.examples showing before/after transformations
- [ ] T073 [US3] Update HTML template to display remediation in collapsible details blocks per contracts/report-html-schema.md
- [ ] T074 [US3] Add syntax highlighting for code blocks in remediation commands/examples using CSS
- [ ] T075 [US3] Verify all citations from research report preserved in outputs (SC-009)

**Checkpoint**: All user stories should now provide complete scoring, interactive viewing, and actionable remediation

---

## Phase 6: User Story 4 - Read Markdown Report for Version Control (Priority: P4)

**Goal**: Generate GitHub-Flavored Markdown report that renders properly in version control platforms and enables diff-based progress tracking

**Independent Test**: Generate Markdown report, commit to git, and verify it renders with tables, formatting, and links intact on GitHub

### Implementation for User Story 4

- [ ] T076 [P] [US4] Implement MarkdownReporter in src/agentready/reporters/markdown.py extending BaseReporter
- [ ] T077 [US4] Generate header section with repository metadata, timestamp, overall score per contracts/report-markdown-schema.md
- [ ] T078 [US4] Generate summary table with categories, scores, status emoji (âœ…âš ï¸âŒ)
- [ ] T079 [US4] Generate detailed findings with attribute subsections using proper heading hierarchy
- [ ] T080 [US4] Add collapsible remediation using details/summary HTML tags (valid in GFM)
- [ ] T081 [US4] Format evidence as bulleted lists with emoji indicators
- [ ] T082 [US4] Format remediation steps as ordered lists with code blocks (bash, python, yaml syntax highlighting)
- [ ] T083 [US4] Add certification level section with score ladder and "YOUR LEVEL" marker
- [ ] T084 [US4] Add "Next Steps" section with top 3-5 improvements ranked by tier and point potential
- [ ] T085 [US4] Generate footer with assessment metadata (duration, research version, repository snapshot)
- [ ] T086 [US4] Preserve all external citation links with absolute URLs per contracts/report-markdown-schema.md
- [ ] T087 [US4] Integrate MarkdownReporter into Scanner workflow (dual-format atomic generation per FR-019)
- [ ] T088 [US4] Add timestamp-based filename with latest symlink (report-2025-11-20T14-30-00.md)
- [ ] T089 [US4] Verify Markdown renders correctly on GitHub/GitLab/Bitbucket (SC-004)

**Checkpoint**: All four user stories complete - MVP delivers scoring, HTML viewing, remediation, and version control

---

## Phase 7: Polish & Cross-Cutting Concerns

**Purpose**: Improvements that affect multiple user stories

- [ ] T090 [P] Create comprehensive unit tests in tests/unit/ for models, services, assessors (>80% coverage per constitution)
- [ ] T091 [P] Create integration tests in tests/integration/test_scan_workflow.py using fixture repositories per research.md
- [ ] T092 [P] Create contract tests in tests/contract/ validating report schemas against contracts/
- [ ] T093 [P] Create test fixtures in tests/fixtures/repositories/ (minimal-python, gold-standard-python, polyglot, edge-cases)
- [ ] T094 Add --update-research CLI command to download latest research report (FR-023, FR-024)
- [ ] T095 Add --generate-config CLI command to create example .agentready-config.yaml
- [ ] T096 Add --validate-config CLI command to check configuration file validity (FR-033)
- [ ] T097 Implement report cleanup logic (keep last N runs) per research.md
- [ ] T098 Add JSON sidecar files for automation integration (assessment-{timestamp}.json)
- [ ] T099 Optimize file system scanning with caching per research.md (don't traverse multiple times)
- [ ] T100 Add performance benchmarks ensuring <5 minutes for <10k files (SC-001)
- [ ] T101 Verify deterministic scoring (same input â†’ same output) (SC-010)
- [ ] T102 Add language-specific analyzer lazy loading per research.md
- [ ] T103 [P] Update README.md with full usage documentation and examples
- [ ] T104 [P] Validate quickstart.md instructions work end-to-end (<5 minutes install to report)
- [ ] T105 Run black, isort, flake8 on entire codebase per constitution
- [ ] T106 Run pytest with coverage report ensuring >80% test coverage
- [ ] T107 Create package distribution configuration for PyPI (setup.py or pyproject.toml build config)
- [ ] T108 Add entry point configuration for `agentready` command in pyproject.toml

---

## Dependencies & Execution Order

### Phase Dependencies

- **Setup (Phase 1)**: No dependencies - can start immediately
- **Foundational (Phase 2)**: Depends on Setup completion - BLOCKS all user stories
- **User Stories (Phase 3-6)**: All depend on Foundational phase completion
  - User Story 1 (P1): Scoring engine - Can start after Phase 2
  - User Story 2 (P2): HTML reports - Depends on US1 JSON output
  - User Story 3 (P3): Remediation - Depends on US1 findings, enhances US2 HTML
  - User Story 4 (P4): Markdown reports - Depends on US1 JSON output
- **Polish (Phase 7)**: Depends on all desired user stories being complete

### User Story Dependencies

- **User Story 1 (P1)**: FOUNDATIONAL - Scoring engine required by all other stories
- **User Story 2 (P2)**: Consumes US1 JSON output, independently adds HTML generation
- **User Story 3 (P3)**: Enhances US1 findings and US2/US4 reports with remediation
- **User Story 4 (P4)**: Consumes US1 JSON output, independently adds Markdown generation

### Within Each User Story

- **US1**: Models â†’ Services â†’ Assessors â†’ Scanner â†’ CLI (sequential dependencies)
- **US2**: Template â†’ Reporter â†’ Integration (sequential)
- **US3**: Remediation templates â†’ Assessor integration â†’ Report integration (sequential)
- **US4**: Markdown reporter â†’ Integration (sequential)

### Parallel Opportunities

- **Phase 1 (Setup)**: All tasks [P] can run in parallel (T003-T007)
- **Phase 2 (Foundational)**: All model creation tasks (T008-T014) can run in parallel, all service tasks (T016-T019) can run in parallel
- **Phase 3 (US1)**: All 13 assessor implementations (T020-T032) can run fully in parallel (different files)
- **Phase 4 (US2)**: Template design (T047-T049) can run in parallel, reporter features (T051-T064) sequential
- **Phase 5 (US3)**: Remediation extraction (T065-T068) can run in parallel
- **Phase 6 (US4)**: Markdown sections (T076-T086) largely parallelizable
- **Phase 7 (Polish)**: Test creation (T090-T093) can run in parallel, documentation (T103-T104) can run in parallel

**Note**: User Story 2 and User Story 4 could potentially be worked on in parallel after US1 completes, as both independently transform US1's JSON output to different formats.

---

## Parallel Example: User Story 1 Assessors

```bash
# Launch all 13 assessor implementations together:
T020: "Implement DocumentationAssessor in src/agentready/assessors/documentation.py"
T021: "Implement CodeQualityAssessor in src/agentready/assessors/code_quality.py"
T022: "Implement StructureAssessor in src/agentready/assessors/structure.py"
T023: "Implement TestingAssessor in src/agentready/assessors/testing.py"
T024: "Implement DependenciesAssessor in src/agentready/assessors/dependencies.py"
T025: "Implement VCSAssessor in src/agentready/assessors/vcs.py"
T026: "Implement BuildAssessor in src/agentready/assessors/build.py"
T027: "Implement ErrorsAssessor in src/agentready/assessors/errors.py"
T028: "Implement APIDocsAssessor in src/agentready/assessors/api_docs.py"
T029: "Implement ModularityAssessor in src/agentready/assessors/modularity.py"
T030: "Implement CICDAssessor in src/agentready/assessors/cicd.py"
T031: "Implement SecurityAssessor in src/agentready/assessors/security.py"
T032: "Implement PerformanceAssessor in src/agentready/assessors/performance.py"
```

---

## Implementation Strategy

### MVP First (User Story 1 Only)

1. Complete Phase 1: Setup (T001-T007)
2. Complete Phase 2: Foundational (T008-T019) - CRITICAL
3. Complete Phase 3: User Story 1 (T020-T046)
4. **STOP and VALIDATE**: Run agentready on test repository, verify JSON output
5. Optionally proceed to US2 (HTML) or US4 (Markdown) for human-readable reports

**Rationale**: US1 alone provides immediate value - repository owners get a comprehensive score with findings. HTML/Markdown reports enhance UX but core assessment functionality works without them.

### Incremental Delivery

1. **Sprint 1**: Setup + Foundational â†’ Can run assessments programmatically
2. **Sprint 2**: User Story 1 â†’ MVP with JSON output + CLI
3. **Sprint 3**: User Story 2 â†’ Add interactive HTML reports
4. **Sprint 4**: User Story 3 â†’ Add actionable remediation guidance
5. **Sprint 5**: User Story 4 â†’ Add version-control-friendly Markdown reports
6. **Sprint 6**: Polish â†’ Testing, optimization, documentation

### Parallel Team Strategy

With 3-4 developers after Foundational phase:

1. **Developer A**: User Story 1 (scoring engine)
2. **Developer B**: User Story 2 (HTML reporter) - starts after US1 JSON structure defined
3. **Developer C**: User Story 4 (Markdown reporter) - starts after US1 JSON structure defined
4. **Developer D**: User Story 3 (remediation) - coordinates with A/B/C to enhance outputs

**Critical Path**: US1 is on critical path (blocks US2/US3/US4). Parallelizing assessor implementations (T020-T032) is key to reducing US1 duration.

---

## Notes

- [P] tasks = different files, no dependencies, safe to parallelize
- [Story] label maps task to specific user story (US1, US2, US3, US4)
- Tests NOT included per constitution (TDD requested but not in spec)
- Each assessor is independently implementable (strategy pattern per research.md)
- Constitution compliance: library-first (assessors/services/models standalone), tool-second (CLI wrapper)
- File paths follow plan.md structure exactly
- Checkpoint after each user story enables incremental validation
- US1 assessors (T020-T032) are largest parallelization opportunity (13 independent files)
````

## File: src/agentready/assessors/structure.py
````python
"""Structure assessors for project layout and separation of concerns."""

from pathlib import Path

from ..models.attribute import Attribute
from ..models.finding import Citation, Finding, Remediation
from ..models.repository import Repository
from .base import BaseAssessor


class StandardLayoutAssessor(BaseAssessor):
    """Assesses standard project layout patterns.

    Tier 1 Essential (10% weight) - Standard layouts help AI navigate code.
    """

    @property
    def attribute_id(self) -> str:
        return "standard_layout"

    @property
    def tier(self) -> int:
        return 1  # Essential

    @property
    def attribute(self) -> Attribute:
        return Attribute(
            id=self.attribute_id,
            name="Standard Project Layouts",
            category="Repository Structure",
            tier=self.tier,
            description="Follows standard project structure for language",
            criteria="Standard directories (src/, tests/, docs/) present",
            default_weight=0.10,
        )

    def assess(self, repository: Repository) -> Finding:
        """Check for standard project layout directories.

        Expected patterns:
        - Python: src/, tests/, docs/
        - JavaScript: src/, test/, docs/
        - Java: src/main/java, src/test/java
        """
        # Check for common standard directories
        standard_dirs = {
            "src": repository.path / "src",
        }

        # Check for tests directory (either tests/ or test/)
        tests_path = repository.path / "tests"
        if not tests_path.exists():
            tests_path = repository.path / "test"
        standard_dirs["tests"] = tests_path

        found_dirs = sum(1 for d in standard_dirs.values() if d.exists())
        required_dirs = len(standard_dirs)

        score = self.calculate_proportional_score(
            measured_value=found_dirs,
            threshold=required_dirs,
            higher_is_better=True,
        )

        status = "pass" if score >= 75 else "fail"

        evidence = [
            f"Found {found_dirs}/{required_dirs} standard directories",
            f"src/: {'âœ“' if (repository.path / 'src').exists() else 'âœ—'}",
            f"tests/: {'âœ“' if (repository.path / 'tests').exists() or (repository.path / 'test').exists() else 'âœ—'}",
        ]

        return Finding(
            attribute=self.attribute,
            status=status,
            score=score,
            measured_value=f"{found_dirs}/{required_dirs} directories",
            threshold=f"{required_dirs}/{required_dirs} directories",
            evidence=evidence,
            remediation=self._create_remediation() if status == "fail" else None,
            error_message=None,
        )

    def _create_remediation(self) -> Remediation:
        """Create remediation guidance for standard layout."""
        return Remediation(
            summary="Organize code into standard directories (src/, tests/, docs/)",
            steps=[
                "Create src/ directory for source code",
                "Create tests/ directory for test files",
                "Create docs/ directory for documentation",
                "Move source code into src/",
                "Move tests into tests/",
            ],
            tools=[],
            commands=[
                "mkdir -p src tests docs",
                "# Move source files to src/",
                "# Move test files to tests/",
            ],
            examples=[],
            citations=[
                Citation(
                    source="Python Packaging Authority",
                    title="Python Project Structure",
                    url="https://packaging.python.org/en/latest/tutorials/packaging-projects/",
                    relevance="Standard Python project layout",
                )
            ],
        )
````

## File: src/agentready/reporters/html.py
````python
"""HTML reporter for generating interactive assessment reports."""

import json
from pathlib import Path

from jinja2 import Environment, PackageLoader, select_autoescape

from ..models.assessment import Assessment
from .base import BaseReporter


class HTMLReporter(BaseReporter):
    """Generates self-contained interactive HTML reports.

    Features:
    - Filter by status (pass/fail/skipped)
    - Sort by category, score, or tier
    - Search attributes by name
    - Collapsible finding details
    - Color-coded scores and tiers
    - Certification ladder visualization
    - Works offline (no CDN dependencies)
    """

    def __init__(self):
        """Initialize HTML reporter with Jinja2 environment."""
        self.env = Environment(
            loader=PackageLoader("agentready", "templates"),
            autoescape=select_autoescape(["html", "xml", "j2"]),
        )

    def generate(self, assessment: Assessment, output_path: Path) -> Path:
        """Generate HTML report from assessment data.

        Args:
            assessment: Complete assessment with findings
            output_path: Path where HTML file should be saved

        Returns:
            Path to generated HTML file

        Raises:
            IOError: If HTML cannot be written
        """
        # Load template
        template = self.env.get_template("report.html.j2")

        # Prepare data for template
        template_data = {
            "repository": assessment.repository,
            "timestamp": assessment.timestamp,
            "overall_score": assessment.overall_score,
            "certification_level": assessment.certification_level,
            "attributes_assessed": assessment.attributes_assessed,
            "attributes_skipped": assessment.attributes_skipped,
            "attributes_total": assessment.attributes_total,
            "findings": assessment.findings,
            "duration_seconds": assessment.duration_seconds,
            "config": assessment.config,
            # Embed assessment JSON for JavaScript
            "assessment_json": json.dumps(assessment.to_dict()),
        }

        # Render template
        html_content = template.render(**template_data)

        # Write to file
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        return output_path
````

## File: src/agentready/templates/report.html.j2
````
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AgentReady Assessment - {{ repository.name }}</title>
    <style>
        /* Reset and Base Styles */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        /* Header */
        header {
            border-bottom: 3px solid #2563eb;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }

        h1 {
            font-size: 2rem;
            color: #1e293b;
            margin-bottom: 10px;
        }

        .subtitle {
            color: #64748b;
            font-size: 1rem;
        }

        /* Summary Cards */
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .summary-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
        }

        .summary-card.score {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        }

        .summary-card.certification {
            background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        }

        .summary-card.assessed {
            background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
        }

        .summary-card.duration {
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
        }

        .summary-card h3 {
            font-size: 0.9rem;
            font-weight: 500;
            opacity: 0.9;
            margin-bottom: 10px;
        }

        .summary-card .value {
            font-size: 2rem;
            font-weight: 700;
        }

        /* Controls */
        .controls {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin: 30px 0;
            padding: 20px;
            background: #f8fafc;
            border-radius: 8px;
        }

        .control-group {
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .control-group label {
            font-weight: 600;
            color: #475569;
            font-size: 0.9rem;
        }

        select, input[type="search"] {
            padding: 8px 12px;
            border: 1px solid #cbd5e1;
            border-radius: 6px;
            font-size: 0.9rem;
            background: white;
        }

        input[type="search"] {
            min-width: 250px;
        }

        .filter-btn {
            padding: 8px 16px;
            border: 2px solid #e2e8f0;
            background: white;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: all 0.2s;
        }

        .filter-btn:hover {
            border-color: #2563eb;
            background: #eff6ff;
        }

        .filter-btn.active {
            background: #2563eb;
            color: white;
            border-color: #2563eb;
        }

        .badge {
            display: inline-block;
            padding: 2px 8px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
            margin-left: 5px;
        }

        /* Certification Ladder */
        .cert-ladder {
            display: flex;
            justify-content: space-between;
            margin: 30px 0;
            padding: 20px;
            background: #f8fafc;
            border-radius: 8px;
        }

        .cert-level {
            flex: 1;
            text-align: center;
            padding: 15px 10px;
            border-radius: 6px;
            margin: 0 5px;
            transition: all 0.3s;
        }

        .cert-level.platinum { background: linear-gradient(135deg, #e0e7ff, #c7d2fe); }
        .cert-level.gold { background: linear-gradient(135deg, #fef3c7, #fde68a); }
        .cert-level.silver { background: linear-gradient(135deg, #e5e7eb, #d1d5db); }
        .cert-level.bronze { background: linear-gradient(135deg, #fed7aa, #fdba74); }
        .cert-level.needs-improvement { background: linear-gradient(135deg, #fecaca, #fca5a5); }

        .cert-level.active {
            transform: scale(1.1);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            border: 3px solid #2563eb;
        }

        .cert-level h4 {
            font-size: 0.9rem;
            margin-bottom: 5px;
        }

        .cert-level .range {
            font-size: 0.8rem;
            opacity: 0.7;
        }

        /* Findings */
        .findings {
            margin: 30px 0;
        }

        .finding {
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            margin-bottom: 15px;
            overflow: hidden;
            transition: all 0.2s;
        }

        .finding:hover {
            box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        }

        .finding-header {
            padding: 15px 20px;
            background: #f8fafc;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .finding-header:hover {
            background: #f1f5f9;
        }

        .finding-title {
            display: flex;
            align-items: center;
            gap: 15px;
            flex: 1;
        }

        .status-icon {
            font-size: 1.5rem;
        }

        .finding-info h3 {
            font-size: 1rem;
            color: #1e293b;
            margin-bottom: 3px;
        }

        .finding-meta {
            font-size: 0.85rem;
            color: #64748b;
        }

        .tier-badge {
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
        }

        .tier-1 { background: #fee2e2; color: #991b1b; }
        .tier-2 { background: #fed7aa; color: #9a3412; }
        .tier-3 { background: #fef3c7; color: #92400e; }
        .tier-4 { background: #d1fae5; color: #065f46; }

        .score-display {
            font-size: 1.5rem;
            font-weight: 700;
            min-width: 80px;
            text-align: right;
        }

        .score-pass { color: #059669; }
        .score-fail { color: #dc2626; }
        .score-skip { color: #6b7280; }

        .finding-body {
            padding: 20px;
            display: none;
        }

        .finding.expanded .finding-body {
            display: block;
        }

        .finding-section {
            margin-bottom: 20px;
        }

        .finding-section h4 {
            font-size: 0.9rem;
            color: #475569;
            margin-bottom: 10px;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        .evidence-list, .remediation-steps {
            list-style: none;
            padding-left: 0;
        }

        .evidence-list li {
            padding: 8px 0;
            border-bottom: 1px solid #f1f5f9;
        }

        .evidence-list li:before {
            content: "â–¸ ";
            color: #2563eb;
            font-weight: 700;
            margin-right: 8px;
        }

        .remediation-steps li {
            padding: 10px 0 10px 30px;
            position: relative;
        }

        .remediation-steps li:before {
            content: counter(step);
            counter-increment: step;
            position: absolute;
            left: 0;
            top: 10px;
            background: #2563eb;
            color: white;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.75rem;
            font-weight: 700;
        }

        .remediation-steps {
            counter-reset: step;
        }

        code {
            background: #f1f5f9;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: #1e293b;
            color: #e2e8f0;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            margin: 10px 0;
        }

        pre code {
            background: none;
            color: inherit;
            padding: 0;
        }

        /* Footer */
        footer {
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #e2e8f0;
            text-align: center;
            color: #64748b;
            font-size: 0.9rem;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }

            .summary-grid {
                grid-template-columns: 1fr;
            }

            .controls {
                flex-direction: column;
            }

            input[type="search"] {
                min-width: 100%;
            }

            .cert-ladder {
                flex-direction: column;
            }

            .cert-level {
                margin: 5px 0;
            }
        }

        /* Hidden class */
        .hidden {
            display: none !important;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ðŸ¤– AgentReady Assessment Report</h1>
            <p class="subtitle">{{ repository.name }} â€¢ {{ timestamp.strftime('%B %d, %Y at %H:%M') }}</p>
        </header>

        <!-- Summary Cards -->
        <div class="summary-grid">
            <div class="summary-card score">
                <h3>Overall Score</h3>
                <div class="value">{{ "%.1f"|format(overall_score) }}</div>
            </div>
            <div class="summary-card certification">
                <h3>Certification</h3>
                <div class="value">{{ certification_level }}</div>
            </div>
            <div class="summary-card assessed">
                <h3>Assessed</h3>
                <div class="value">{{ attributes_assessed }}/{{ attributes_total }}</div>
            </div>
            <div class="summary-card duration">
                <h3>Duration</h3>
                <div class="value">{{ "%.1f"|format(duration_seconds) }}s</div>
            </div>
        </div>

        <!-- Certification Ladder -->
        <div class="cert-ladder">
            <div class="cert-level platinum {% if certification_level == 'Platinum' %}active{% endif %}">
                <h4>ðŸ’Ž Platinum</h4>
                <p class="range">90-100</p>
            </div>
            <div class="cert-level gold {% if certification_level == 'Gold' %}active{% endif %}">
                <h4>ðŸ¥‡ Gold</h4>
                <p class="range">75-89</p>
            </div>
            <div class="cert-level silver {% if certification_level == 'Silver' %}active{% endif %}">
                <h4>ðŸ¥ˆ Silver</h4>
                <p class="range">60-74</p>
            </div>
            <div class="cert-level bronze {% if certification_level == 'Bronze' %}active{% endif %}">
                <h4>ðŸ¥‰ Bronze</h4>
                <p class="range">40-59</p>
            </div>
            <div class="cert-level needs-improvement {% if certification_level == 'Needs Improvement' %}active{% endif %}">
                <h4>âš ï¸ Needs Work</h4>
                <p class="range">0-39</p>
            </div>
        </div>

        <!-- Controls -->
        <div class="controls">
            <div class="control-group">
                <label>Filter:</label>
                <button class="filter-btn active" data-filter="all">All <span class="badge">{{ findings|length }}</span></button>
                <button class="filter-btn" data-filter="pass">Pass <span class="badge">{{ findings|selectattr('status', 'equalto', 'pass')|list|length }}</span></button>
                <button class="filter-btn" data-filter="fail">Fail <span class="badge">{{ findings|selectattr('status', 'equalto', 'fail')|list|length }}</span></button>
                <button class="filter-btn" data-filter="skipped">Skipped <span class="badge">{{ (findings|selectattr('status', 'equalto', 'skipped')|list|length + findings|selectattr('status', 'equalto', 'not_applicable')|list|length) }}</span></button>
            </div>

            <div class="control-group">
                <label>Sort:</label>
                <select id="sort-select">
                    <option value="category">Category</option>
                    <option value="score-desc">Score (High to Low)</option>
                    <option value="score-asc">Score (Low to High)</option>
                    <option value="tier">Tier (Essential First)</option>
                </select>
            </div>

            <div class="control-group">
                <input type="search" id="search-box" placeholder="Search attributes...">
            </div>
        </div>

        <!-- Findings -->
        <div class="findings" id="findings-container">
            {% for finding in findings %}
            <div class="finding"
                 data-status="{{ finding.status }}"
                 data-tier="{{ finding.attribute.tier }}"
                 data-category="{{ finding.attribute.category }}"
                 data-score="{{ finding.score if finding.score else 0 }}"
                 data-name="{{ finding.attribute.name|lower }}">

                <div class="finding-header" onclick="toggleFinding(this)">
                    <div class="finding-title">
                        <span class="status-icon">
                            {% if finding.status == 'pass' %}âœ…
                            {% elif finding.status == 'fail' %}âŒ
                            {% elif finding.status in ['skipped', 'not_applicable'] %}âŠ˜
                            {% else %}âš ï¸{% endif %}
                        </span>
                        <div class="finding-info">
                            <h3>{{ finding.attribute.name }}</h3>
                            <div class="finding-meta">
                                {{ finding.attribute.category }} â€¢
                                <span class="tier-badge tier-{{ finding.attribute.tier }}">Tier {{ finding.attribute.tier }}</span>
                                {% if finding.measured_value %} â€¢ {{ finding.measured_value }}{% endif %}
                            </div>
                        </div>
                    </div>
                    {% if finding.score is not none %}
                    <div class="score-display score-{{ finding.status }}">
                        {{ "%.0f"|format(finding.score) }}
                    </div>
                    {% else %}
                    <div class="score-display score-skip">â€”</div>
                    {% endif %}
                </div>

                <div class="finding-body">
                    {% if finding.evidence %}
                    <div class="finding-section">
                        <h4>Evidence</h4>
                        <ul class="evidence-list">
                            {% for item in finding.evidence %}
                            <li>{{ item }}</li>
                            {% endfor %}
                        </ul>
                    </div>
                    {% endif %}

                    {% if finding.remediation %}
                    <div class="finding-section">
                        <h4>Remediation</h4>
                        <p><strong>{{ finding.remediation.summary }}</strong></p>

                        {% if finding.remediation.steps %}
                        <ol class="remediation-steps">
                            {% for step in finding.remediation.steps %}
                            <li>{{ step }}</li>
                            {% endfor %}
                        </ol>
                        {% endif %}

                        {% if finding.remediation.commands %}
                        <h4 style="margin-top: 15px;">Commands</h4>
                        <pre><code>{{ finding.remediation.commands|join('\n') }}</code></pre>
                        {% endif %}

                        {% if finding.remediation.examples %}
                        <h4 style="margin-top: 15px;">Examples</h4>
                        {% for example in finding.remediation.examples %}
                        <pre><code>{{ example }}</code></pre>
                        {% endfor %}
                        {% endif %}
                    </div>
                    {% endif %}

                    {% if finding.error_message %}
                    <div class="finding-section">
                        <h4>Error</h4>
                        <p style="color: #dc2626;">{{ finding.error_message }}</p>
                    </div>
                    {% endif %}
                </div>
            </div>
            {% endfor %}
        </div>

        <footer>
            <p>Generated by <strong>AgentReady v1.0.0</strong></p>
            <p>Repository: {{ repository.path }} â€¢ Branch: {{ repository.branch }} â€¢ Commit: {{ repository.commit_hash[:8] }}</p>
            <p style="margin-top: 10px; font-size: 0.85rem;">
                ðŸ¤– Generated with <a href="https://claude.com/claude-code" style="color: #2563eb;">Claude Code</a>
            </p>
        </footer>
    </div>

    <script>
        // Embedded assessment data (properly escaped to prevent XSS)
        const ASSESSMENT = JSON.parse({{ assessment_json|tojson }});

        // Toggle finding expansion
        function toggleFinding(header) {
            const finding = header.parentElement;
            finding.classList.toggle('expanded');
        }

        // Filter functionality
        document.querySelectorAll('.filter-btn').forEach(btn => {
            btn.addEventListener('click', () => {
                // Update active button
                document.querySelectorAll('.filter-btn').forEach(b => b.classList.remove('active'));
                btn.classList.add('active');

                const filter = btn.dataset.filter;
                const findings = document.querySelectorAll('.finding');

                findings.forEach(finding => {
                    const status = finding.dataset.status;

                    if (filter === 'all') {
                        finding.classList.remove('hidden');
                    } else if (filter === 'skipped') {
                        if (status === 'skipped' || status === 'not_applicable' || status === 'error') {
                            finding.classList.remove('hidden');
                        } else {
                            finding.classList.add('hidden');
                        }
                    } else {
                        if (status === filter) {
                            finding.classList.remove('hidden');
                        } else {
                            finding.classList.add('hidden');
                        }
                    }
                });
            });
        });

        // Sort functionality
        document.getElementById('sort-select').addEventListener('change', (e) => {
            const sortBy = e.target.value;
            const container = document.getElementById('findings-container');
            const findings = Array.from(container.querySelectorAll('.finding'));

            findings.sort((a, b) => {
                if (sortBy === 'category') {
                    return a.dataset.category.localeCompare(b.dataset.category);
                } else if (sortBy === 'score-desc') {
                    return parseFloat(b.dataset.score) - parseFloat(a.dataset.score);
                } else if (sortBy === 'score-asc') {
                    return parseFloat(a.dataset.score) - parseFloat(b.dataset.score);
                } else if (sortBy === 'tier') {
                    return parseInt(a.dataset.tier) - parseInt(b.dataset.tier);
                }
            });

            findings.forEach(finding => container.appendChild(finding));
        });

        // Search functionality
        document.getElementById('search-box').addEventListener('input', (e) => {
            const query = e.target.value.toLowerCase();
            const findings = document.querySelectorAll('.finding');

            findings.forEach(finding => {
                const name = finding.dataset.name;
                const category = finding.dataset.category.toLowerCase();

                if (name.includes(query) || category.includes(query)) {
                    finding.classList.remove('hidden');
                } else {
                    finding.classList.add('hidden');
                }
            });
        });
    </script>
</body>
</html>
````

## File: CLAUDE.md
````markdown
# AgentReady - Repository Quality Assessment Tool

**Purpose**: Assess repositories against agent-ready best practices and generate actionable reports.

**Last Updated**: 2025-11-21

---

## Overview

AgentReady is a Python CLI tool that evaluates repositories against 25 carefully researched attributes that make codebases more effective for AI-assisted development. It generates interactive HTML reports, version-control friendly Markdown reports, and machine-readable JSON output.

**Current Status**: v1.0.0 - Core assessment engine complete, 10/25 attributes implemented

**Self-Assessment Score**: 75.4/100 (Gold) - See `examples/self-assessment/`

---

## Quick Start

```bash
# Install in virtual environment
uv venv && source .venv/bin/activate
uv pip install -e .

# Run assessment on current directory
agentready assess .

# Run with verbose output
agentready assess . --verbose

# Assess different repository
agentready assess /path/to/repo --output-dir ./reports
```

**Outputs**:
- `.agentready/assessment-YYYYMMDD-HHMMSS.json` - Machine-readable results
- `.agentready/report-YYYYMMDD-HHMMSS.html` - Interactive web report
- `.agentready/report-YYYYMMDD-HHMMSS.md` - Git-friendly markdown report

---

## Architecture

### Core Components

```
src/agentready/
â”œâ”€â”€ models/          # Data models (Repository, Attribute, Finding, Assessment)
â”œâ”€â”€ services/        # Scanner orchestration and language detection
â”œâ”€â”€ assessors/       # Attribute assessment implementations
â”‚   â”œâ”€â”€ base.py      # BaseAssessor abstract class
â”‚   â”œâ”€â”€ documentation.py   # CLAUDE.md, README assessors
â”‚   â”œâ”€â”€ code_quality.py    # Type annotations, complexity
â”‚   â”œâ”€â”€ testing.py         # Test coverage, pre-commit hooks
â”‚   â”œâ”€â”€ structure.py       # Standard layout, gitignore
â”‚   â””â”€â”€ stub_assessors.py  # 15 not-yet-implemented assessors
â”œâ”€â”€ reporters/       # Report generation (HTML, Markdown, JSON)
â”‚   â”œâ”€â”€ html.py      # Interactive HTML with Jinja2
â”‚   â””â”€â”€ markdown.py  # GitHub-Flavored Markdown
â”œâ”€â”€ templates/       # Jinja2 templates
â”‚   â””â”€â”€ report.html.j2  # Self-contained HTML report (73KB)
â””â”€â”€ cli/             # Click-based CLI
    â””â”€â”€ main.py      # assess, research-version, generate-config commands
```

### Data Flow

```
Repository â†’ Scanner â†’ Assessors â†’ Findings â†’ Assessment â†’ Reporters â†’ Reports
                â†“
         Language Detection
         (git ls-files)
```

### Scoring Algorithm

1. **Tier-Based Weighting** (50/30/15/5 distribution):
   - Tier 1 (Essential): 50% of total score
   - Tier 2 (Critical): 30% of total score
   - Tier 3 (Important): 15% of total score
   - Tier 4 (Advanced): 5% of total score

2. **Attribute Scoring**: Each attribute returns 0-100 score
3. **Weighted Aggregation**: `final_score = Î£(attribute_score Ã— weight)`
4. **Certification Levels**:
   - Platinum: 90-100
   - Gold: 75-89
   - Silver: 60-74
   - Bronze: 40-59
   - Needs Improvement: 0-39

---

## Development

### Setup

```bash
# Create virtual environment
uv venv
source .venv/bin/activate

# Install dependencies
uv pip install -e .

# Install development tools
uv pip install pytest black isort ruff
```

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src/agentready --cov-report=html

# Run specific test file
pytest tests/unit/test_models.py -v
```

**Current Coverage**: 37% (focused on core logic)

### Code Quality

```bash
# Format code
black src/ tests/

# Sort imports
isort src/ tests/

# Lint code
ruff check src/ tests/

# Run all linters
black src/ tests/ && isort src/ tests/ && ruff check src/ tests/
```

### Adding New Assessors

1. **Expand a stub assessor** in `src/agentready/assessors/stub_assessors.py`
2. **Create new assessor class** inheriting from `BaseAssessor`
3. **Implement required methods**:
   - `attribute_id` property
   - `assess(repository)` method
   - `is_applicable(repository)` method (optional)
4. **Add tests** in `tests/unit/test_assessors_*.py`
5. **Register** in scanner's assessor list

**Example**:
```python
class MyAssessor(BaseAssessor):
    @property
    def attribute_id(self) -> str:
        return "my_attribute_id"

    def assess(self, repository: Repository) -> Finding:
        # Implement assessment logic
        if condition_met:
            return Finding.create_pass(self.attribute, ...)
        else:
            return Finding.create_fail(self.attribute, ...)
```

---

## Project Structure

```
agentready/
â”œâ”€â”€ src/agentready/          # Source code
â”œâ”€â”€ tests/                   # Test suite
â”‚   â”œâ”€â”€ unit/               # Unit tests
â”‚   â””â”€â”€ integration/        # End-to-end tests
â”œâ”€â”€ examples/               # Example reports
â”‚   â””â”€â”€ self-assessment/    # AgentReady's own assessment
â”œâ”€â”€ specs/                  # Feature specifications
â”œâ”€â”€ pyproject.toml          # Python package configuration
â”œâ”€â”€ CLAUDE.md              # This file
â”œâ”€â”€ README.md              # User-facing documentation
â”œâ”€â”€ BACKLOG.md             # Future features and enhancements
â””â”€â”€ GITHUB_ISSUES.md       # GitHub-ready issue templates
```

---

## Technologies

- **Python 3.11+** (only N and N-1 versions supported)
- **Click** - CLI framework
- **Jinja2** - HTML template engine
- **Pytest** - Testing framework
- **Black** - Code formatter
- **isort** - Import sorter
- **Ruff** - Fast Python linter

---

## Contributing

### Workflow

1. **Create feature branch** from `main`
2. **Implement changes** with tests
3. **Run linters**: `black . && isort . && ruff check .`
4. **Run tests**: `pytest`
5. **Commit** with conventional commit messages
6. **Push** and create PR

### Conventional Commits

```
feat: Add new assessor for dependency freshness
fix: Correct type annotation detection in Python 3.12
docs: Update CLAUDE.md with architecture details
test: Add integration test for HTML report generation
refactor: Extract common assessor logic to base class
chore: Update dependencies
```

### Test Requirements

- All new assessors must have unit tests
- Integration tests for new reporters
- Maintain >80% coverage for new code
- All tests must pass before merge

---

## CI/CD

**GitHub Actions** (planned):
- Run tests on PR
- Run linters (black, isort, ruff)
- Generate coverage report
- Run AgentReady self-assessment
- Post assessment results as PR comment

**Current**: Manual workflow (tests run locally before push)

---

## Known Issues & Limitations

1. **Stub Assessors**: 15/25 assessors return "not_applicable" - need implementation
2. **No Lock File**: Intentionally excluded for library project
3. **No Pre-commit Hooks**: Not yet configured (planned P0 fix)
4. **HTML Report Design**: Current color scheme needs improvement (P0 fix)
5. **Report Metadata**: Missing repository context in header (P0 fix)

---

## Roadmap

### v1.1 - Critical UX Fixes (Next Sprint)
- **P0**: Add report header with repository metadata
- **P0**: Redesign HTML report (larger fonts, better colors)

### v1.2 - Automation & Integration
- **P1**: Implement `agentready align` subcommand (automated remediation)
- **P2**: GitHub App integration (badges, status checks, PR comments)
- **P2**: Interactive dashboard (one-click remediation)

### v1.3 - Assessor Expansion
- Expand 15 stub assessors
- Add AI-powered assessors (type annotations, docstrings)
- Improve test coverage to >80%

### v2.0 - Enterprise Features
- Report schema versioning
- Customizable HTML themes with dark/light toggle
- Organization-wide dashboards
- Historical trend analysis

See `BACKLOG.md` for full feature list.

---

## Getting Help

- **Issues**: Create GitHub issue using templates in `GITHUB_ISSUES.md`
- **Documentation**: See `README.md` for user guide
- **Examples**: View `examples/self-assessment/` for reference reports
- **Research**: Read `agent-ready-codebase-attributes.md` for attribute definitions

---

## Related Documents

- **BACKLOG.md** - Future features and enhancements (11 items)
- **GITHUB_ISSUES.md** - GitHub-ready issue templates
- **README.md** - User-facing documentation
- **specs/** - Feature specifications and design documents
- **examples/self-assessment/** - AgentReady's own assessment (75.4/100 Gold)

---

## Notes for Claude Code Agents

**When working on AgentReady**:

1. **Read before modifying**: Always read existing assessors before implementing new ones
2. **Follow patterns**: Use `CLAUDEmdAssessor` and `READMEAssessor` as reference implementations
3. **Test thoroughly**: Add unit tests for all new assessors
4. **Maintain backwards compatibility**: Don't change Assessment model without schema version bump
5. **Stub assessors first**: Check if attribute already has stub before creating new class
6. **Proportional scoring**: Use `calculate_proportional_score()` for partial compliance
7. **Graceful degradation**: Return "skipped" if tools missing, don't crash
8. **Rich remediation**: Provide actionable steps, tools, commands, examples, citations

**Key Principles**:
- Library-first architecture (no global state)
- Strategy pattern for assessors (each is independent)
- Fail gracefully (missing tools â†’ skip, don't crash)
- User-focused (actionable remediation over theoretical guidance)

---

**Last Updated**: 2025-11-21 by Jeremy Eder
**AgentReady Version**: 1.0.0
**Self-Assessment**: 75.4/100 (Gold) âœ¨
````

## File: GITHUB_ISSUES.md
````markdown
# AgentReady - GitHub Issues Template

**Purpose**: GitHub-ready issue descriptions for all backlog items, organized by priority.

**Instructions**: Copy each issue block below into a new GitHub issue.

---

## P0 - Critical Issues (Blocking Adoption)

### Issue #1: Bootstrap AgentReady Repository on GitHub (DOGFOODING!)

**Labels**: `P0-critical`, `feature`, `cli`, `github-integration`, `dogfooding`

**Title**: Implement `agentready bootstrap` command and use it to set up agentready repository itself

**Description**:

**THIS IS THE FIRST FEATURE TO IMPLEMENT** - We'll dogfood our own tool by using it to bootstrap the agentready repository on GitHub!

Build `agentready bootstrap` command that creates:
- GitHub Actions workflows (assessment, tests, linters)
- GitHub templates (issue templates, PR template, CODEOWNERS)
- Pre-commit hooks (.pre-commit-config.yaml)
- Dependabot configuration
- Documentation (CONTRIBUTING.md, CODE_OF_CONDUCT.md, LICENSE)
- Repository badges

**Why This Is P0**:
1. **Dogfooding** - We need this for agentready repo itself FIRST
2. **Demonstrates Value** - Shows AgentReady in action on our own codebase
3. **Foundation** - Required before GitHub App integration
4. **Credibility** - Can't ask others to use it if we don't use it ourselves

**Command Interface**:
```bash
# Bootstrap current repository
agentready bootstrap .

# Dry run (show what would be created)
agentready bootstrap . --dry-run

# Interactive mode (confirm each file)
agentready bootstrap . --interactive
```

**What Gets Created**:
```
.github/
â”œâ”€â”€ workflows/
â”‚   â”œâ”€â”€ agentready-assessment.yml  # Run assessment on PR/push
â”‚   â”œâ”€â”€ tests.yml                  # Run pytest, black, isort, ruff
â”‚   â””â”€â”€ security.yml              # Dependabot + security scanning
â”œâ”€â”€ ISSUE_TEMPLATE/
â”‚   â”œâ”€â”€ bug_report.md
â”‚   â””â”€â”€ feature_request.md
â”œâ”€â”€ PULL_REQUEST_TEMPLATE.md
â”œâ”€â”€ CODEOWNERS
â””â”€â”€ dependabot.yml

.pre-commit-config.yaml
CONTRIBUTING.md
CODE_OF_CONDUCT.md
LICENSE
```

**GitHub Actions - AgentReady Assessment Workflow**:
```yaml
name: AgentReady Assessment
on:
  pull_request:
  push:
    branches: [main]

jobs:
  assess:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install agentready
      - run: agentready assess . --verbose
      - uses: actions/upload-artifact@v4
        with:
          name: agentready-reports
          path: .agentready/
      - name: Comment PR
        if: github.event_name == 'pull_request'
        # Post markdown report as PR comment
```

**Implementation**:

**Files to Create**:
- `src/agentready/cli/bootstrap.py` - CLI command
- `src/agentready/bootstrap/generator.py` - File generator
- `src/agentready/bootstrap/templates/` - Jinja2 templates
  - `github/workflows/agentready.yml.j2`
  - `github/workflows/tests.yml.j2`
  - `github/ISSUE_TEMPLATE/bug_report.md.j2`
  - `github/PULL_REQUEST_TEMPLATE.md.j2`
  - `precommit.yaml.j2`
  - `CONTRIBUTING.md.j2`
- `tests/unit/test_bootstrap.py`

**Implementation Phases**:

**Phase 1**: Core Bootstrap (This Sprint)
- [ ] Implement `agentready bootstrap` CLI command
- [ ] Create template files (GitHub Actions, pre-commit, templates)
- [ ] Add Jinja2 rendering
- [ ] Test on agentready repository
- [ ] Commit all generated files to agentready repo

**Phase 2**: GitHub Integration (Future)
- [ ] Use `gh` CLI to update repository settings
- [ ] Set up branch protection
- [ ] Add repository badges
- [ ] Configure repository topics

**Acceptance Criteria**:
- [ ] `agentready bootstrap .` creates all files
- [ ] GitHub Actions workflows are valid YAML
- [ ] Pre-commit hooks install successfully
- [ ] Dry-run mode shows preview
- [ ] Interactive mode prompts for confirmation
- [ ] **Successfully bootstrap agentready repository itself**
- [ ] All generated files committed to agentready repo
- [ ] GitHub Actions runs on next PR

**Success Metrics**:
After running `agentready bootstrap .` on agentready repo:
- âœ… GitHub Actions workflow runs assessment on PRs
- âœ… Pre-commit hooks prevent bad commits
- âœ… Issue/PR templates guide contributors
- âœ… Dependabot keeps dependencies updated
- âœ… Repository badge shows AgentReady score
- âœ… All linters pass in CI

**Priority Justification**: We need to dogfood our own tool FIRST. This validates the bootstrap design and sets up critical infrastructure.

**Related Issues**: #5 (GitHub App will enhance this), #3 (Align will complement this)

---

### Issue #2: Add Report Header with Repository Metadata

**Labels**: `P0-critical`, `ux`, `reports`, `bug`

**Title**: Add prominent report header showing repository name, path, and assessment metadata

**Description**:

Currently, AgentReady reports (HTML, Markdown, JSON) lack context about what repository was assessed. Users cannot immediately identify:
- What repository was scanned
- When the assessment was run
- Which branch/commit was assessed
- Who executed the assessment

This blocks adoption in multi-repository workflows and CI/CD environments.

**Expected Behavior**:

All reports should have a **prominent header section** at the top showing:
- Repository name (bold, large)
- Repository path (filesystem path or GitHub URL)
- Assessment timestamp (human-readable)
- Git branch and commit hash
- AgentReady version
- User who ran assessment (username@hostname)
- Command executed

**Implementation**:

HTML Report Header:
```html
<header class="report-header">
  <h1>AgentReady Assessment Report</h1>
  <div class="repo-name">Repository: agentready</div>
  <div class="repo-path">/Users/jeder/repos/sk/agentready</div>
  <div class="repo-git">Branch: main | Commit: abc123</div>
  <div class="meta">
    Assessed: November 21, 2025 at 2:11 AM
    AgentReady: v1.0.0
    By: jeder@macbook
  </div>
</header>
```

**Files to Modify**:
- `src/agentready/models/assessment.py` - Add metadata field
- `src/agentready/services/scanner.py` - Collect metadata (version, user, command)
- `src/agentready/templates/report.html.j2` - Add header section
- `src/agentready/reporters/markdown.py` - Add header to markdown
- `src/agentready/reporters/html.py` - Pass metadata to template

**Acceptance Criteria**:
- [ ] Repository name and path visible at top of all reports
- [ ] Assessment timestamp in human-readable format
- [ ] Git context (branch, commit) included
- [ ] AgentReady version tracked
- [ ] Header positioned before score summary
- [ ] All three report formats updated (HTML, Markdown, JSON)

**Priority Justification**: Users are confused about what reports show. This blocks real-world usage.

---

### Issue #3: Redesign HTML Report - Fix Font Sizes and Color Scheme

**Labels**: `P0-critical`, `ux`, `design`, `reports`, `accessibility`

**Title**: Increase font sizes (+4pt) and replace color scheme with professional dark blue/purple/white palette

**Description**:

Current HTML report has two critical UX problems:
1. **Font sizes too small** - Base font is 14px, headings are small, hard to read
2. **Color scheme problems** - Purple gradient everywhere is overwhelming and unprofessional

User feedback: "the color scheme is hideous"

**Current Problems**:
- Body text: 14px (too small for modern displays)
- Purple gradient background used everywhere (#667eea â†’ #764ba2)
- Low contrast in some areas
- Not suitable for Red Hat engineering presentations

**Required Changes**:

**1. Font Size Increases** (minimum +4pt):
```css
body: 14px â†’ 18px
h1: 28px â†’ 36px
h2: 24px â†’ 30px
h3: 20px â†’ 26px
.score: 48px â†’ 56px
.attribute-name: 16px â†’ 22px
```

**2. New Color Scheme** (Dark/Professional):
```css
/* Replace current purple gradient with: */
--background: #0a0e27;      /* Almost black with blue tint */
--surface: #1a1f3a;         /* Dark blue surface */
--primary: #8b5cf6;         /* Purple (accent only) */
--text-primary: #f8fafc;    /* Almost white */
--success: #10b981;         /* Green (pass) */
--danger: #ef4444;          /* Red (fail) */
```

**Design Direction**: Mostly black, dark blue, purple accents, white text - professional and clean.

**Files to Modify**:
- `src/agentready/templates/report.html.j2` - Update all CSS

**Acceptance Criteria**:
- [ ] Base font size increased to 18px minimum
- [ ] All headings increased by 4pt+
- [ ] Purple gradient removed from backgrounds
- [ ] Dark blue (#1a1f3a) used for surfaces
- [ ] Purple (#8b5cf6) used sparingly as accent
- [ ] White text (#f8fafc) on dark backgrounds
- [ ] WCAG 2.1 AA contrast ratios maintained
- [ ] Professional appearance suitable for stakeholder presentations

**Priority Justification**: Visual design blocks adoption. This is the first thing users see.

**Related Issues**: #3 (Theme system will build on this foundation)

---

## P1 - Critical Features

### Issue #4: Implement `agentready align` Subcommand for Automated Remediation

**Labels**: `P1-critical`, `feature`, `cli`, `automation`

**Title**: Add `agentready align` command to automatically fix failing attributes

**Description**:

Users need an automated way to improve their repository scores. The `align` subcommand should generate and apply fixes for failing attributes.

**Vision**: One command to align repository with best practices.

**Proposed Interface**:
```bash
agentready align .                           # Fix all failures
agentready align . --dry-run                 # Preview changes
agentready align . --create-pr               # Create GitHub PR
agentready align . --interactive             # Confirm each fix
agentready align . --attributes claude_md_file,precommit_hooks
```

**Supported Fix Types**:

1. **Template-based** (auto-applicable):
   - CLAUDE.md generation
   - .gitignore patterns
   - .pre-commit-config.yaml
   - README scaffolding
   - GitHub issue/PR templates

2. **Command-based** (execute commands):
   - Generate lock files (`npm install`, `poetry lock`)
   - Install pre-commit hooks
   - Update dependencies

3. **AI-powered** (optional, requires --ai):
   - Add type annotations
   - Generate docstrings
   - Refactor high-complexity functions

**Workflow Example**:
```
$ agentready align . --dry-run

Repository: my-project
Current Score: 62.4/100 (Silver)
Projected Score: 84.7/100 (Gold) ðŸŽ¯

Changes:
  âœ… claude_md_file (+10 pts) - CREATE CLAUDE.md
  âœ… precommit_hooks (+3 pts) - CREATE .pre-commit-config.yaml
  âœ… gitignore (+3 pts) - UPDATE .gitignore

Apply changes? [y/N]
```

**Implementation Phases**:

**Phase 1**: Template-based fixes
- Create `src/agentready/fixers/` module
- Implement base fixer class
- Add template-based fixers (CLAUDE.md, .gitignore, pre-commit)
- Add `align` CLI command
- Support --dry-run flag

**Phase 2**: Command execution & GitHub PR
- Execute shell commands safely
- Add --create-pr flag
- Implement GitHub PR creation via `gh` CLI
- Show before/after scores

**Phase 3**: AI-powered fixes (future)
- Integrate with LLM APIs
- Add type annotation fixer
- Add docstring generator
- Require explicit --ai flag

**Files to Create**:
- `src/agentready/fixers/base.py` - BaseFixer abstract class
- `src/agentready/fixers/template.py` - Template-based fixers
- `src/agentready/fixers/command.py` - Command execution
- `src/agentready/cli/align.py` - CLI command
- `src/agentready/templates/fixes/` - Fix templates directory
- `tests/unit/test_fixers.py` - Fixer tests

**Acceptance Criteria**:
- [ ] `agentready align .` applies all fixable failures
- [ ] `--dry-run` shows preview without applying changes
- [ ] `--create-pr` creates GitHub PR with fixes
- [ ] Score improvement projection shown
- [ ] Template-based fixes working (CLAUDE.md, .gitignore, pre-commit)
- [ ] Comprehensive tests

**Priority Justification**: Most requested feature. Dramatically improves value proposition.

**Related**: Interactive Dashboard (#5), Bootstrap (#1)

---

## P2 - High Value Features

### Issue #5: Create Interactive Dashboard with One-Click Remediation

**Labels**: `P2-high`, `feature`, `dashboard`, `ux`, `github-integration`

**Title**: Build interactive dashboard for live assessment updates and one-click GitHub issue/PR creation

**Description**:

Transform static HTML report into interactive dashboard with:
- Real-time assessment updates (WebSocket)
- "Fix This" button on each failure â†’ creates GitHub issue + draft PR
- Historical trend visualization
- Live filtering/sorting (already implemented)

**Vision**: Click "Fix This" â†’ GitHub issue created â†’ Draft PR with proposed changes

**Implementation Phases**:

**Phase 1**: Enhanced Static Report
- Add "Copy gh command" buttons to HTML report
- Generate `gh issue create` commands for each failure
- Include fix templates in issue descriptions

**Phase 2**: Local Dashboard Server
- Flask/FastAPI server serving dashboard
- WebSocket for live updates
- GitHub integration via `gh` CLI
- Preview diffs before applying fixes

**Phase 3**: Cloud Service (future)
- Hosted dashboard at agentready.redhat.com
- GitHub App installation
- Continuous monitoring
- Team collaboration

**Acceptance Criteria (Phase 1)**:
- [ ] "Copy Command" button for each failure
- [ ] Generated `gh` commands create properly formatted issues
- [ ] Fix templates embedded in issue descriptions
- [ ] Links to AgentReady documentation

**Priority Justification**: Significantly improves workflow - users can fix issues with one click.

**Related**: Align command (#4), GitHub App (#6), Bootstrap (#1)

---

### Issue #6: Implement GitHub App Integration (Badge & Status Checks)

**Labels**: `P2-high`, `feature`, `github-integration`, `ci-cd`

**Title**: Create GitHub App for repository badges, PR status checks, and automated comments

**Description**:

Provide GitHub integration for visibility and CI/CD workflows:
- Repository badges showing certification level
- GitHub Actions integration
- PR status checks (block merge if score < threshold)
- Automated PR comments with score deltas

**Core Features**:

1. **Badge Service**
   - Endpoint: `https://agentready.redhat.com/badge/{owner}/{repo}.svg`
   - Dynamic color based on certification (Platinum, Gold, Silver, Bronze)
   - Shields.io-compatible format

2. **GitHub Actions**
   - Official `agentready/assess-action`
   - Run on PR events and pushes
   - Upload artifacts (HTML/JSON reports)

3. **PR Status Checks**
   - Use GitHub Checks API
   - Configurable thresholds
   - Link to detailed report

4. **PR Comments**
   - Automated bot comments
   - Score delta: "72.4 â†’ 78.3 (+5.9)"
   - New failures/fixes listed

**Implementation**:

**Phase 1**: GitHub Actions Integration
```yaml
# Create .github/workflows/agentready.yml
- uses: agentready/assess-action@v1
  with:
    threshold: 75
    post-comment: true
```

**Phase 2**: Badge Service
- FastAPI endpoint generating SVG badges
- Fetch latest assessment from GitHub Actions artifacts

**Phase 3**: GitHub App
- App installation via Red Hat GitHub Enterprise
- Dashboard at agentready.redhat.com
- Red Hat SSO authentication

**Acceptance Criteria**:
- [ ] GitHub Action published and working
- [ ] Badge service generating SVGs
- [ ] Status checks posted to PRs
- [ ] Comments show score deltas
- [ ] Integration with Red Hat infrastructure

**Priority Justification**: Critical for CI/CD adoption and visibility within Red Hat.

**Notes**: All infrastructure stays within Red Hat (no external services).

---

## P3 - Important Enhancements

### Issue #7: Implement Report Schema Versioning

**Labels**: `P3-important`, `schema`, `contracts`, `backwards-compatibility`

**Title**: Define and version JSON/HTML/Markdown report schemas for backwards compatibility

**Description**:

Formalize report schemas and establish versioning to ensure backwards compatibility as the tool evolves.

**Requirements**:
- JSON schema for assessment reports
- HTML schema documentation
- Markdown schema documentation
- Semantic versioning strategy
- Schema migration tools

**Files**:
- `contracts/assessment-schema.json` (already exists, needs versioning)
- `contracts/report-html-schema.md` (exists)
- `contracts/report-markdown-schema.md` (exists)

**Acceptance Criteria**:
- [ ] JSON Schema Draft 2020-12 used
- [ ] `agentready validate-report` command
- [ ] Schema version in report metadata
- [ ] Migration tools for major version changes

**Priority Justification**: Important for enterprise adoption and long-term maintenance.

---

### Issue #8: Create AgentReady Repository Development Agent

**Labels**: `P3-important`, `developer-experience`, `claude-code`

**Title**: Build specialized Claude Code agent for AgentReady development tasks

**Description**:

Create `.claude/agents/agentready-dev.md` agent with deep knowledge of AgentReady architecture to assist with:
- Implementing new assessors
- Writing tests
- Debugging assessment issues
- Optimizing performance

**Acceptance Criteria**:
- [ ] Agent specification created
- [ ] Links to design docs (data-model.md, plan.md)
- [ ] Common development patterns documented
- [ ] Test structure examples included

**Priority Justification**: Accelerates development and helps contributors.

---

## P4 - Nice-to-Have Enhancements

### Issue #9: Build Research Report Generator/Updater Utility

**Labels**: `P4-enhancement`, `tooling`, `documentation`

**Title**: Create utility to maintain research report (agent-ready-codebase-attributes.md)

**Description**:

Build CLI tool to validate and update research report:
- `agentready research validate`
- `agentready research add-attribute`
- `agentready research format`

**Acceptance Criteria**:
- [ ] Schema validation against contracts/research-report-schema.md
- [ ] Automated metadata generation
- [ ] Attribute numbering checks
- [ ] Citation deduplication

**Priority Justification**: Improves research report quality and maintainability.

---

### Issue #10: Integrate Repomix for Repository Context Generation

**Labels**: `P4-enhancement`, `ai-integration`, `tooling`

**Title**: Add Repomix integration for AI-optimized repository context files

**Description**:

Integrate with Repomix (https://github.com/yamadashy/repomix) to generate repository context:
- `agentready repomix-generate`
- Include in bootstrapped repositories
- GitHub Actions integration

**Acceptance Criteria**:
- [ ] Generate Repomix output for existing repos
- [ ] Include in bootstrap workflow
- [ ] GitHub Actions workflow template

**Priority Justification**: Enhances AI-assisted development workflow.

---

### Issue #11: Implement Customizable HTML Report Themes

**Labels**: `P4-enhancement`, `ux`, `accessibility`, `theming`

**Title**: Add theme system with dark/light mode toggle and custom color schemes

**Description**:

Allow users to customize HTML report appearance:
- **Theme dropdown** in top-right corner
- **Quick dark/light toggle button**
- Multiple built-in themes (solarized, dracula, high-contrast)
- localStorage persistence

**Note**: This builds on Issue #2 (P0) which establishes the base dark theme.

**Acceptance Criteria**:
- [ ] Theme dropdown in HTML report
- [ ] Dark/light toggle button
- [ ] 5+ built-in themes
- [ ] localStorage saves preference
- [ ] All themes WCAG 2.1 AA compliant

**Priority Justification**: Improves user experience and accessibility.

---

## P5 - Future Enhancements

_No items currently - Bootstrap feature was promoted to P0 as Issue #1 (Dogfooding!)_

---

## Issue Creation Checklist

When creating GitHub issues from this template:

1. **Copy the entire issue block** (including title, labels, description, acceptance criteria)
2. **Set labels** as specified in each issue
3. **Set milestone**: Create milestones for v1.1 (P0), v1.2 (P1-P2), v1.3 (P3-P4)
4. **Assign**: Assign P0 issues immediately
5. **Link related issues**: Use "Related to #X" in issue descriptions
6. **Add to project board**: Create columns for P0, P1, P2, P3+

## Label Definitions

Create these labels in GitHub:
- `P0-critical` (red) - Blocking adoption
- `P1-critical` (orange) - Critical features
- `P2-high` (yellow) - High value
- `P3-important` (blue) - Important enhancements
- `P4-enhancement` (green) - Nice to have
- `P5-future` (gray) - Future work
- `ux` - User experience
- `reports` - Report generation
- `feature` - New feature
- `bug` - Bug fix
- `cli` - CLI changes
- `automation` - Automation features
- `github-integration` - GitHub integration
- `accessibility` - Accessibility improvements

## Milestones

**v1.1 - Bootstrap & Critical UX** (Target: Sprint 1)
- Issue #1: Bootstrap Command (FIRST - Dogfooding!)
- Issue #2: Report Header Metadata
- Issue #3: HTML Design Improvements

**v1.2 - Automation & Integration** (Target: Sprint 2-3)
- Issue #4: Align Subcommand
- Issue #5: Interactive Dashboard (Phase 1)
- Issue #6: GitHub App Integration (Phase 1)

**v1.3 - Polish & Documentation** (Target: Sprint 4+)
- Issue #7: Schema Versioning
- Issue #8: Development Agent
- Issue #9-11: Enhancements
````

## File: src/agentready/cli/main.py
````python
"""CLI entry point for agentready tool."""

import json
import sys
from pathlib import Path

import click

from ..assessors.code_quality import (
    CyclomaticComplexityAssessor,
    TypeAnnotationsAssessor,
)

# Import all assessors
from ..assessors.documentation import CLAUDEmdAssessor, READMEAssessor
from ..assessors.structure import StandardLayoutAssessor
from ..assessors.stub_assessors import (
    ConventionalCommitsAssessor,
    GitignoreAssessor,
    LockFilesAssessor,
    create_stub_assessors,
)
from ..assessors.testing import PreCommitHooksAssessor, TestCoverageAssessor
from ..models.config import Config
from ..reporters.html import HTMLReporter
from ..reporters.markdown import MarkdownReporter
from ..services.research_loader import ResearchLoader
from ..services.scanner import Scanner
from .bootstrap import bootstrap


def create_all_assessors():
    """Create all 25 assessors for assessment."""
    assessors = [
        # Tier 1 Essential (5 assessors)
        CLAUDEmdAssessor(),
        READMEAssessor(),
        TypeAnnotationsAssessor(),
        StandardLayoutAssessor(),
        LockFilesAssessor(),
        # Tier 2 Critical (10 assessors - 3 implemented, 7 stubs)
        TestCoverageAssessor(),
        PreCommitHooksAssessor(),
        ConventionalCommitsAssessor(),
        GitignoreAssessor(),
        CyclomaticComplexityAssessor(),  # Actually Tier 3, but including here
    ]

    # Add remaining stub assessors
    assessors.extend(create_stub_assessors())

    return assessors


@click.group(invoke_without_command=True)
@click.option("--version", is_flag=True, help="Show version information")
@click.pass_context
def cli(ctx, version):
    """AgentReady Repository Scorer - Assess repositories for AI-assisted development.

    Evaluates repositories against 25 evidence-based attributes and generates
    comprehensive reports with scores, findings, and remediation guidance.

    Examples:

        \b
        # Assess current repository
        agentready assess .

        \b
        # Assess with custom configuration
        agentready assess /path/to/repo --config my-config.yaml

        \b
        # Show research version
        agentready research-version

        \b
        # Generate example config
        agentready generate-config
    """
    if version:
        show_version()
        ctx.exit()

    if ctx.invoked_subcommand is None:
        click.echo(ctx.get_help())


@cli.command()
@click.argument("repository", type=click.Path(exists=True), required=False, default=".")
@click.option("--verbose", "-v", is_flag=True, help="Enable verbose output")
@click.option(
    "--output-dir",
    "-o",
    type=click.Path(),
    default=None,
    help="Output directory for reports (default: .agentready/)",
)
@click.option(
    "--config",
    "-c",
    type=click.Path(exists=True),
    default=None,
    help="Path to configuration file",
)
def assess(repository, verbose, output_dir, config):
    """Assess a repository against agent-ready criteria.

    REPOSITORY: Path to git repository (default: current directory)
    """
    run_assessment(repository, verbose, output_dir, config)


def run_assessment(repository_path, verbose, output_dir, config_path):
    """Execute repository assessment."""
    repo_path = Path(repository_path).resolve()

    if verbose:
        click.echo(f"AgentReady Repository Scorer")
        click.echo(f"{'=' * 50}\n")

    # Load configuration if provided
    config = None
    if config_path:
        config = load_config(Path(config_path))

    # Set output directory
    if output_dir:
        output_path = Path(output_dir)
    else:
        output_path = repo_path / ".agentready"

    output_path.mkdir(parents=True, exist_ok=True)

    # Create scanner
    try:
        scanner = Scanner(repo_path, config)
    except ValueError as e:
        click.echo(f"Error: {str(e)}", err=True)
        sys.exit(1)

    # Create assessors
    assessors = create_all_assessors()

    if verbose:
        click.echo(f"Repository: {repo_path}")
        click.echo(f"Assessors: {len(assessors)}")
        click.echo(f"Output: {output_path}\n")

    # Run scan
    try:
        assessment = scanner.scan(assessors, verbose=verbose)
    except Exception as e:
        click.echo(f"Error during assessment: {str(e)}", err=True)
        if verbose:
            import traceback

            traceback.print_exc()
        sys.exit(1)

    # Generate timestamp for file naming
    timestamp = assessment.timestamp.strftime("%Y%m%d-%H%M%S")

    # Save JSON output
    json_file = output_path / f"assessment-{timestamp}.json"
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(assessment.to_dict(), f, indent=2)

    # Generate HTML report
    html_reporter = HTMLReporter()
    html_file = output_path / f"report-{timestamp}.html"
    html_reporter.generate(assessment, html_file)

    # Generate Markdown report
    markdown_reporter = MarkdownReporter()
    markdown_file = output_path / f"report-{timestamp}.md"
    markdown_reporter.generate(assessment, markdown_file)

    # Create latest symlinks
    latest_json = output_path / "assessment-latest.json"
    latest_html = output_path / "report-latest.html"
    latest_md = output_path / "report-latest.md"

    for latest, target in [
        (latest_json, json_file),
        (latest_html, html_file),
        (latest_md, markdown_file),
    ]:
        if latest.exists():
            latest.unlink()
        try:
            latest.symlink_to(target.name)
        except OSError:
            # Windows doesn't support symlinks easily, just copy
            import shutil

            shutil.copy(target, latest)

    if verbose:
        click.echo(f"\n{'=' * 50}")

    click.echo(f"\nAssessment complete!")
    click.echo(
        f"  Score: {assessment.overall_score:.1f}/100 ({assessment.certification_level})"
    )
    click.echo(
        f"  Assessed: {assessment.attributes_assessed}/{assessment.attributes_total}"
    )
    click.echo(f"  Skipped: {assessment.attributes_skipped}")
    click.echo(f"  Duration: {assessment.duration_seconds:.1f}s")
    click.echo(f"\nReports generated:")
    click.echo(f"  JSON: {json_file}")
    click.echo(f"  HTML: {html_file}")
    click.echo(f"  Markdown: {markdown_file}")


def load_config(config_path: Path) -> Config:
    """Load configuration from YAML file."""
    import yaml

    with open(config_path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)

    return Config(
        weights=data.get("weights", {}),
        excluded_attributes=data.get("excluded_attributes", []),
        language_overrides=data.get("language_overrides", {}),
        output_dir=Path(data["output_dir"]) if "output_dir" in data else None,
    )


@cli.command()
def research_version():
    """Show bundled research report version."""
    loader = ResearchLoader()
    try:
        content, metadata, is_valid, errors, warnings = loader.load_and_validate()

        click.echo(f"Research Report Version: {metadata.version}")
        click.echo(f"Date: {metadata.date}")
        click.echo(f"Attributes: {metadata.attribute_count}")
        click.echo(f"References: {metadata.reference_count}")
        click.echo(f"\nValidation: {'âœ“ PASS' if is_valid else 'âœ— FAIL'}")

        if errors:
            click.echo("\nErrors:")
            for error in errors:
                click.echo(f"  - {error}")

        if warnings:
            click.echo("\nWarnings:")
            for warning in warnings:
                click.echo(f"  - {warning}")

    except FileNotFoundError as e:
        click.echo(f"Error: {str(e)}", err=True)
        sys.exit(1)


@cli.command()
def generate_config():
    """Generate example configuration file."""
    example_path = Path(".agentready-config.example.yaml")

    if not example_path.exists():
        click.echo("Error: .agentready-config.example.yaml not found", err=True)
        sys.exit(1)

    target = Path(".agentready-config.yaml")

    if target.exists():
        if not click.confirm(f"{target} already exists. Overwrite?"):
            return

    import shutil

    shutil.copy(example_path, target)
    click.echo(f"Created {target}")
    click.echo("Edit this file to customize weights and behavior.")


# Register bootstrap command
cli.add_command(bootstrap)


def show_version():
    """Show version information."""
    click.echo("AgentReady Repository Scorer v1.0.0")
    click.echo("Research Report: bundled")


if __name__ == "__main__":
    cli()
````

## File: BACKLOG.md
````markdown
# AgentReady Backlog

**Purpose**: Track future features and improvements for the agentready tool.

---

## Critical Issues (P0)

### Create Automated Demo

**Priority**: P0 (Critical - Showcase Value)

**Description**: Create an automated, self-contained demonstration of AgentReady that shows the tool assessing a sample repository, generating reports, and providing remediation guidance. This should be runnable with a single command and suitable for demos, presentations, and onboarding.

**Requirements**:
- Single command to run: `agentready demo`
- Self-contained sample repository (embedded in package or generated on-the-fly)
- Demonstrates full workflow:
  1. Repository analysis
  2. Attribute assessment
  3. Score calculation
  4. HTML/Markdown report generation
  5. Remediation suggestions
- Interactive terminal output showing progress
- Opens generated HTML report automatically in browser
- Reusable for presentations and stakeholder demos

**Implementation**:

```bash
# Run automated demo
agentready demo

# Output:
# ðŸ¤– AgentReady Demo
# ===================
#
# Creating sample repository...
# Analyzing structure...
# Running 25 assessments...
#   âœ… claude_md_file (100/100)
#   âŒ precommit_hooks (0/100)
#   ... [progress indicators]
#
# Assessment complete!
# Score: 67.3/100 (Silver)
#
# Generating reports...
#   ðŸ“„ demo-report.html (generated)
#   ðŸ“„ demo-report.md (generated)
#   ðŸ“„ demo-assessment.json (generated)
#
# Opening HTML report in browser...
```

**Sample Repository Options**:

**Option 1: Embedded Examples** (Ship with package)
```
src/agentready/demo/
â”œâ”€â”€ sample-python-repo/      # Python project (minimal)
â”‚   â”œâ”€â”€ src/myapp/
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â””â”€â”€ .gitignore
â”œâ”€â”€ sample-js-repo/          # JavaScript project
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ README.md
â””â”€â”€ sample-go-repo/          # Go project
```

**Option 2: Generate On-the-Fly** (Dynamic)
```python
def create_demo_repo(tmp_path: Path, language: str = "python") -> Path:
    """Create sample repository for demo."""
    repo = tmp_path / "demo-repo"
    repo.mkdir()

    if language == "python":
        # Create minimal Python project
        create_file(repo / "README.md", "# Demo Project\n\nSample Python project.")
        create_file(repo / "src/main.py", "def main(): pass")
        create_file(repo / ".gitignore", "*.pyc\n__pycache__/")
        # Missing: CLAUDE.md, tests/, pre-commit hooks (intentional for demo)

    return repo
```

**Demo Script Features**:
- **Progress indicators**: Show assessment running in real-time
- **Color-coded output**: Green for pass, red for fail, yellow for warnings
- **Simulated delays**: Add realistic pauses for "dramatic effect" in demos
- **Narration mode**: Optional verbose output explaining each step
- **Screenshot mode**: Generate high-quality terminal screenshots for docs
- **Record mode**: Save terminal session as GIF/video for presentations

**Configuration**:

```yaml
# Demo config embedded in package
demo:
  sample_repo: python  # or javascript, go, minimal
  show_progress: true
  auto_open_browser: true
  output_dir: ./demo-output
  narration: true  # Explain each step
  delay_ms: 500    # Pause between steps for visibility
```

**Use Cases**:

**Use Case 1: Stakeholder Demo**
```bash
# Quick 2-minute demo for leadership
agentready demo --narration
# Shows: What AgentReady does, how it scores, what reports look like
```

**Use Case 2: Onboarding New Users**
```bash
# Help new users understand the tool
agentready demo --tutorial
# Interactive walkthrough with explanations
```

**Use Case 3: Generate Demo Content for Docs**
```bash
# Create screenshots and videos for documentation
agentready demo --screenshot --record demo.gif
```

**Acceptance Criteria**:
- [ ] `agentready demo` runs without any setup
- [ ] Creates sample repository automatically
- [ ] Runs full assessment workflow
- [ ] Generates all report formats (HTML, Markdown, JSON)
- [ ] Opens HTML report in browser
- [ ] Colorful, engaging terminal output
- [ ] Completes in < 5 seconds
- [ ] No external dependencies required
- [ ] Works offline
- [ ] Includes narration mode for presentations

**Priority Justification**:
- Critical for showcasing tool value to stakeholders
- Needed for Red Hat internal demos and presentations
- Helps with user onboarding and adoption
- Low effort, high impact for visibility
- Required before pitching to other Red Hat teams

**Related**: Onboarding, documentation, marketing

**Notes**:
- Keep demo simple and fast (< 5 seconds)
- Focus on visual impact (colors, progress bars)
- Make it suitable for screenshots/videos
- Consider adding "failure scenario" demo too
- Could expand to multiple language demos
- Add to bootstrap command as part of repo setup

---

### Fix Critical Security & Logic Bugs from Code Review

**Priority**: P0 (Critical - Security & Correctness)

**Description**: Address critical bugs discovered in code review that affect security and assessment accuracy.

**Issues to Fix**:

1. **XSS Vulnerability in HTML Reports** (CRITICAL - Security)
   - **Location**: `src/agentready/templates/report.html.j2:579`
   - **Problem**: `assessment_json|safe` disables autoescaping for JSON embedded in JavaScript
   - **Risk**: Repository names, commit messages, file paths from git could contain malicious content
   - **Fix**: Replace with `JSON.parse({{ assessment_json|tojson }})`
   - **Add**: Content Security Policy headers to HTML reports

2. **StandardLayoutAssessor Logic Bug** (CRITICAL - Incorrect Scoring)
   - **Location**: `src/agentready/assessors/structure.py:48`
   - **Problem**: `(repository.path / "tests") or (repository.path / "test")` always evaluates to first path
   - **Impact**: Projects with `test/` instead of `tests/` scored incorrectly
   - **Fix**: Check both paths properly:
     ```python
     tests_path = repository.path / "tests"
     if not tests_path.exists():
         tests_path = repository.path / "test"
     has_tests = tests_path.exists()
     ```

**Implementation**:

**File 1**: `src/agentready/templates/report.html.j2`
```jinja2
<!-- BEFORE (VULNERABLE): -->
const ASSESSMENT = {{ assessment_json|safe }};

<!-- AFTER (SECURE): -->
const ASSESSMENT = JSON.parse({{ assessment_json|tojson }});
```

**File 2**: `src/agentready/assessors/structure.py`
```python
# BEFORE (BUGGY):
standard_dirs = {
    "src": repository.path / "src",
    "tests": (repository.path / "tests") or (repository.path / "test"),  # BUG!
}

# AFTER (CORRECT):
standard_dirs = {
    "src": repository.path / "src",
}

# Check for tests directory (either tests/ or test/)
tests_path = repository.path / "tests"
if not tests_path.exists():
    tests_path = repository.path / "test"
standard_dirs["tests"] = tests_path
```

**Test Cases to Add**:
```python
def test_xss_in_repository_name():
    """Test that malicious repo names are escaped in HTML."""
    repo = Repository(
        name="<script>alert('xss')</script>",
        # ...
    )
    html = HTMLReporter().generate(assessment, output)
    assert "<script>" not in html  # Should be escaped

def test_standard_layout_with_test_dir():
    """Test that 'test/' directory is recognized (not just 'tests/')."""
    # Create repo with test/ directory only
    repo_path = tmp_path / "repo"
    (repo_path / "test").mkdir(parents=True)

    assessor = StandardLayoutAssessor()
    finding = assessor.assess(Repository(...))
    assert finding.status == "pass"  # Should recognize test/ dir
```

**Acceptance Criteria**:
- [ ] XSS vulnerability patched with `tojson` filter
- [ ] CSP headers added to HTML reports (future)
- [ ] StandardLayoutAssessor recognizes both `tests/` and `test/`
- [ ] Tests added for XSS prevention
- [ ] Tests added for both test directory naming patterns
- [ ] All existing tests still pass

**Priority Justification**:
- **Security**: XSS is a P0 vulnerability
- **Correctness**: Incorrect scoring undermines tool credibility
- **Quick fixes**: Both are 5-10 minute changes

**Related**: Issue #2 (Report improvements), Bootstrap (#1 - needs secure reports)

---

### Bootstrap AgentReady Repository on GitHub

**Priority**: P0 (Critical - Dogfooding)

**Description**: Implement `agentready bootstrap` command to set up the agentready repository itself on GitHub with all best practices. This is the FIRST feature to implement - we'll dogfood our own tool!

**Vision**: Use AgentReady to bootstrap the AgentReady repository - demonstrating the tool's value while setting up our own infrastructure.

**Why P0**:
- Demonstrates tool value immediately (dogfooding)
- Sets up critical GitHub infrastructure (Actions, badges, PR templates)
- Validates bootstrap command design before external users
- Creates the foundation for GitHub App integration

**Requirements**:

1. **GitHub Repository Setup**
   - Create/update repository on GitHub via `gh` CLI
   - Set repository description and topics
   - Configure repository settings (PR requirements, branch protection)

2. **GitHub Actions Workflows**
   - `.github/workflows/agentready-assessment.yml` - Run assessment on PR/push
   - `.github/workflows/tests.yml` - Run pytest, linters
   - `.github/workflows/release.yml` - Publish to PyPI (future)

3. **GitHub Templates**
   - `.github/ISSUE_TEMPLATE/bug_report.md`
   - `.github/ISSUE_TEMPLATE/feature_request.md`
   - `.github/PULL_REQUEST_TEMPLATE.md`
   - `.github/CODEOWNERS`

4. **Pre-commit Hooks**
   - `.pre-commit-config.yaml` with black, isort, ruff
   - Conventional commit linting (commitlint)
   - Auto-run tests before commit

5. **Dependency Management**
   - Dependabot configuration (`.github/dependabot.yml`)
   - Security scanning (`.github/workflows/security.yml`)

6. **Documentation Updates**
   - Update README.md with badges
   - Add CONTRIBUTING.md
   - Add CODE_OF_CONDUCT.md (Red Hat standard)
   - Add LICENSE (Apache 2.0 or MIT)

**Command Interface**:

```bash
# Bootstrap current repository on GitHub
agentready bootstrap .

# Bootstrap with specific language
agentready bootstrap . --language python

# Bootstrap and create GitHub repo
agentready bootstrap . --create-repo redhat/agentready

# Dry run (show what would be created)
agentready bootstrap . --dry-run

# Interactive mode (confirm each file)
agentready bootstrap . --interactive
```

**What Gets Created**:

```
.github/
â”œâ”€â”€ workflows/
â”‚   â”œâ”€â”€ agentready-assessment.yml  # Run assessment on PR
â”‚   â”œâ”€â”€ tests.yml                  # Run tests and linters
â”‚   â””â”€â”€ dependabot.yml            # Dependency updates
â”œâ”€â”€ ISSUE_TEMPLATE/
â”‚   â”œâ”€â”€ bug_report.md
â”‚   â””â”€â”€ feature_request.md
â”œâ”€â”€ PULL_REQUEST_TEMPLATE.md
â”œâ”€â”€ CODEOWNERS
â””â”€â”€ dependabot.yml

.pre-commit-config.yaml
CONTRIBUTING.md
CODE_OF_CONDUCT.md
LICENSE
```

**GitHub Actions Workflow Example**:

```yaml
# .github/workflows/agentready-assessment.yml
name: AgentReady Assessment

on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches: [main]

jobs:
  assess:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install AgentReady
        run: |
          pip install agentready

      - name: Run Assessment
        run: |
          agentready assess . --verbose

      - name: Upload Reports
        uses: actions/upload-artifact@v4
        with:
          name: agentready-reports
          path: .agentready/

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('.agentready/report-latest.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
```

**Implementation Phases**:

**Phase 1: Core Bootstrap** (This Sprint)
- Implement `agentready bootstrap` command
- Create template files for GitHub Actions, pre-commit, templates
- Test on agentready repository itself
- Commit all generated files

**Phase 2: GitHub Integration** (Next Sprint)
- Use `gh` CLI to create/update repository
- Set up branch protection rules
- Configure repository settings
- Add repository badges

**Phase 3: Language-Specific Templates** (Future)
- Python-specific templates (pytest, black, mypy)
- JavaScript-specific (eslint, prettier, jest)
- Go-specific (golangci-lint, gotestsum)

**Files to Create**:

```
src/agentready/
â”œâ”€â”€ cli/bootstrap.py           # Bootstrap CLI command
â”œâ”€â”€ bootstrap/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ generator.py          # File generator
â”‚   â”œâ”€â”€ github_setup.py       # GitHub integration
â”‚   â””â”€â”€ templates/            # Template files
â”‚       â”œâ”€â”€ github/
â”‚       â”‚   â”œâ”€â”€ workflows/
â”‚       â”‚   â”‚   â”œâ”€â”€ agentready.yml.j2
â”‚       â”‚   â”‚   â””â”€â”€ tests.yml.j2
â”‚       â”‚   â”œâ”€â”€ ISSUE_TEMPLATE/
â”‚       â”‚   â””â”€â”€ PULL_REQUEST_TEMPLATE.md.j2
â”‚       â”œâ”€â”€ precommit.yaml.j2
â”‚       â”œâ”€â”€ CONTRIBUTING.md.j2
â”‚       â””â”€â”€ CODE_OF_CONDUCT.md.j2
tests/unit/test_bootstrap.py
```

**Acceptance Criteria**:

- [ ] `agentready bootstrap .` creates all required files
- [ ] GitHub Actions workflows are valid and functional
- [ ] Pre-commit hooks install and run successfully
- [ ] Repository badges added to README.md
- [ ] Dry-run mode shows what would be created
- [ ] Interactive mode prompts for confirmation
- [ ] Successfully bootstrap agentready repository itself
- [ ] All generated files committed to agentready repo
- [ ] GitHub Actions runs assessment on every PR

**Success Metrics**:

- AgentReady repository on GitHub has:
  - âœ… GitHub Actions running assessments
  - âœ… PR template with checklist
  - âœ… Issue templates for bugs/features
  - âœ… Pre-commit hooks configured
  - âœ… Dependabot enabled
  - âœ… Repository badge showing score
  - âœ… All linters passing in CI

**Priority Justification**:

This is P0 because:
1. **Dogfooding** - We need this for our own repository first
2. **Demonstrates value** - Shows AgentReady in action on itself
3. **Foundation** - Required before GitHub App integration
4. **Credibility** - Can't tell others to use it if we don't use it ourselves

**Related**: GitHub App Integration (#5), Align Command (#3)

**Notes**:
- Start with Python-specific templates (our use case)
- Keep templates simple and focused
- Use Jinja2 for template rendering
- Integrate with `gh` CLI for GitHub operations
- All templates should pass AgentReady assessment!

---

### Report Header with Repository Metadata

**Priority**: P0 (Critical - Blocking Usability)

**Description**: Add prominent report header showing what repository/agent/code was scanned. Currently reports lack context about what was assessed.

**Problem**: Users cannot identify what the report is about without digging into the details. No repository name, path, timestamp, or assessment context visible at the top.

**Requirements**:
- **Prominent header section** at the top of all report formats (HTML, Markdown, JSON)
- Repository name (bold, large font)
- Repository path (absolute path on filesystem or GitHub URL)
- Assessment timestamp (human-readable: "November 21, 2025 at 2:11 AM")
- Branch name and commit hash
- AgentReady version used for assessment
- Who ran the assessment (username@hostname)
- Command used: `agentready assess /path/to/repo --verbose`

**HTML Report Header Design**:
```html
<header class="report-header">
  <div class="repo-info">
    <h1>AgentReady Assessment Report</h1>
    <div class="repo-name">Repository: agentready</div>
    <div class="repo-path">/Users/jeder/repos/sk/agentready</div>
    <div class="repo-git">Branch: 001-agentready-scorer | Commit: d49947c</div>
  </div>
  <div class="meta-info">
    <div>Assessed: November 21, 2025 at 2:11 AM</div>
    <div>AgentReady Version: 1.0.0</div>
    <div>Run by: jeder@macbook</div>
  </div>
</header>
```

**Markdown Report Header**:
```markdown
# ðŸ¤– AgentReady Assessment Report

**Repository**: agentready
**Path**: `/Users/jeder/repos/sk/agentready`
**Branch**: `001-agentready-scorer` | **Commit**: `d49947c`
**Assessed**: November 21, 2025 at 2:11 AM
**AgentReady Version**: 1.0.0
**Run by**: jeder@macbook

---
```

**JSON Report Metadata**:
```json
{
  "metadata": {
    "agentready_version": "1.0.0",
    "assessment_timestamp": "2025-11-21T02:11:05Z",
    "assessment_timestamp_human": "November 21, 2025 at 2:11 AM",
    "executed_by": "jeder@macbook",
    "command": "agentready assess . --verbose",
    "working_directory": "/Users/jeder/repos/sk/agentready"
  },
  "repository": { ... }
}
```

**Implementation**:
- Add metadata collection to Scanner
- Update all reporter templates (HTML, Markdown)
- Enhance Assessment model with metadata field
- Position header prominently (before score summary)

**Acceptance Criteria**:
- âœ… User can immediately identify what repository was assessed
- âœ… Timestamp shows when assessment was run
- âœ… Git context (branch, commit) visible
- âœ… AgentReady version tracked for reproducibility

**Related**: Report generation, usability, debugging

**Notes**:
- This is blocking adoption - users confused about report context
- Critical for multi-repository workflows
- Needed for CI/CD integration (track which build)

---

### Improve HTML Report Design (Font Size & Color Scheme)

**Priority**: P0 (Critical - Poor User Experience)

**Description**: Completely redesign HTML report color scheme and increase all font sizes by at least 4 points for readability.

**Problems**:
1. **Color scheme is "hideous"** (user feedback) - current purple gradient doesn't work
2. **Font sizes too small** - hard to read on modern displays
3. **Poor contrast** - some text hard to distinguish

**New Color Scheme** (Dark/Professional):
```css
:root {
  /* Base colors - mostly black, dark blue, purple, white */
  --background: #0a0e27;           /* Almost black with blue tint */
  --surface: #1a1f3a;              /* Dark blue surface */
  --surface-elevated: #252b4a;     /* Slightly lighter surface */

  /* Primary colors */
  --primary: #8b5cf6;              /* Purple (accent) */
  --primary-light: #a78bfa;        /* Light purple */
  --primary-dark: #6d28d9;         /* Dark purple */

  /* Text colors */
  --text-primary: #f8fafc;         /* Almost white */
  --text-secondary: #cbd5e1;       /* Light gray */
  --text-muted: #94a3b8;           /* Muted gray */

  /* Status colors */
  --success: #10b981;              /* Green (pass) */
  --warning: #f59e0b;              /* Amber (warning) */
  --danger: #ef4444;               /* Red (fail) */
  --neutral: #64748b;              /* Gray (skipped) */

  /* UI elements */
  --border: #334155;               /* Dark border */
  --shadow: rgba(0, 0, 0, 0.5);   /* Deep shadows */
}
```

**Font Size Increases** (+4pt minimum):
```css
/* Current â†’ New */
body { font-size: 14px â†’ 18px; }
h1 { font-size: 28px â†’ 36px; }
h2 { font-size: 24px â†’ 30px; }
h3 { font-size: 20px â†’ 26px; }
.score { font-size: 48px â†’ 56px; }
.attribute-name { font-size: 16px â†’ 22px; }
.evidence { font-size: 13px â†’ 17px; }
code { font-size: 13px â†’ 16px; }
```

**Design Mockup**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Dark navy blue background #1a1f3a]                â”‚
â”‚                                                     â”‚
â”‚  ðŸ¤– AgentReady Assessment Report                   â”‚
â”‚  Repository: agentready                            â”‚
â”‚  /Users/jeder/repos/sk/agentready                  â”‚
â”‚  [White text #f8fafc, 18px base font]             â”‚
â”‚                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [Purple accent card #8b5cf6]                       â”‚
â”‚                                                     â”‚
â”‚           75.4 / 100                               â”‚
â”‚           [56px, bold, white]                      â”‚
â”‚           ðŸ¥‡ Gold                                   â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation Checklist**:
- [ ] Replace gradient backgrounds with dark blue/black
- [ ] Update all font sizes (+4pt minimum)
- [ ] Use purple (#8b5cf6) sparingly as accent color
- [ ] Ensure white text on dark backgrounds (WCAG AA)
- [ ] Update certification level colors
- [ ] Redesign score cards with new scheme
- [ ] Test with colorblind simulators
- [ ] Add light mode as alternative (with same professional palette)

**Before/After Color Comparison**:
```
Current (Problems):
- Purple gradient everywhere: #667eea â†’ #764ba2 âŒ
- Small text: 14px base âŒ
- Busy, overwhelming âŒ

New (Professional):
- Dark blue/black base: #0a0e27, #1a1f3a âœ…
- Purple accents only: #8b5cf6 âœ…
- Large text: 18px base âœ…
- Clean, readable âœ…
```

**Acceptance Criteria**:
- âœ… All text easily readable (18px minimum body text)
- âœ… Color scheme uses black, dark blue, purple, white palette
- âœ… High contrast (WCAG 2.1 AA compliant)
- âœ… Professional appearance suitable for Red Hat engineering
- âœ… Purple used as accent, not dominant color

**Related**: HTML report generation, UX, accessibility

**Notes**:
- Current design blocks user adoption (visual issues)
- This is the first thing users see - must be excellent
- Consider adding screenshot to docs after redesign
- Font size critical for presentations and stakeholder reviews

---

## Schema & Configuration

### Report Schema Versioning

**Priority**: P3 (Important)

**Description**: Define and version the JSON/HTML/Markdown report schemas to ensure backwards compatibility and enable schema evolution.

**Requirements**:
- JSON schema for assessment reports (contracts/assessment-schema.json exists)
- HTML schema for interactive reports (contracts/report-html-schema.md exists)
- Markdown schema for version control reports (contracts/report-markdown-schema.md exists)
- Schema versioning strategy (semantic versioning)
- Backwards compatibility testing
- Schema migration tools for major version changes

**Use Case**:
```bash
# Validate report against schema
agentready validate-report assessment-2025-11-20.json

# Migrate old report to new schema version
agentready migrate-report --from 1.0.0 --to 2.0.0 old-report.json
```

**Related**: Report generation, data model evolution

**Notes**:
- Schemas exist in contracts/ directory but need formal versioning
- Consider using JSON Schema Draft 2020-12
- Tool should validate generated reports against bundled schema
- Breaking schema changes should trigger major version bump

---

### Research Report Generator/Updater Utility

**Priority**: P4 (Enhancement)

**Description**: Create a utility tool to help maintain and update the research report (agent-ready-codebase-attributes.md) following the validation schema defined in contracts/research-report-schema.md.

**Requirements**:
- Generate new research reports from templates
- Validate existing reports against schema (contracts/research-report-schema.md)
- Update/add attributes while maintaining schema compliance
- Automatically format citations and references
- Extract tier assignments and metadata
- Verify 25 attributes, 4 tiers, 20+ references
- Check for required sections (Definition, Measurable Criteria, Impact on Agent Behavior)

**Use Case**:
```bash
# Validate existing research report
agentready research validate agent-ready-codebase-attributes.md

# Generate new research report from template
agentready research init --output new-research.md

# Add new attribute to research report
agentready research add-attribute \
  --id "attribute_26" \
  --name "New Attribute" \
  --tier 2 \
  --file research.md

# Update metadata (version, date)
agentready research bump-version --type minor

# Lint and format research report
agentready research format research.md
```

**Features**:
- Schema validation (errors vs warnings per research-report-schema.md)
- Automated metadata header generation (version, date in YAML frontmatter)
- Attribute numbering consistency checks (1.1, 1.2, ..., 15.1)
- Citation deduplication and formatting
- Tier distribution balance warnings
- Category coverage analysis
- Markdown formatting enforcement (consistent structure)
- Reference URL reachability checks

**Related**: Research report maintenance, schema compliance, documentation quality

**Notes**:
- Must follow contracts/research-report-schema.md validation rules
- Should prevent invalid reports from being committed
- Could integrate with pre-commit hooks for research report changes
- Consider CLI commands under `agentready research` subcommand
- Tool should be self-documenting (help users fix validation errors)
- Future: Could use LLMs to help generate attribute descriptions from academic papers

---

### Repomix Integration

**Priority**: P4 (Enhancement)

**Description**: Integrate with Repomix (https://github.com/yamadashy/repomix) to generate AI-optimized repository context files for both new and existing repositories.

**Requirements**:
- Generate Repomix output for existing repositories
- Include Repomix configuration in bootstrapped new repositories
- Optional GitHub Actions integration for automatic regeneration
- Support Repomix configuration customization
- Integrate with agentready assessment workflow

**Use Case**:
```bash
# Generate Repomix output for current repository
agentready repomix-generate

# Bootstrap new repo with Repomix integration
agentready init --repo my-project --language python --repomix

# This would:
# 1. Set up Repomix configuration
# 2. Add GitHub Action for automatic regeneration
# 3. Generate initial repository context file
# 4. Include Repomix output in .gitignore appropriately
```

**Features**:
- Automatic Repomix configuration based on repository language
- GitHub Actions workflow for CI/CD integration
- Custom ignore patterns aligned with agentready assessment
- Support for both markdown and XML output formats
- Integration with agentready bootstrap command

**Related**: Repository initialization, AI-assisted development workflows

**Notes**:
- Repomix generates optimized repository context for LLMs
- Could enhance CLAUDE.md with reference to Repomix output
- Should align with existing .gitignore patterns
- Consider adding Repomix freshness check to assessment attributes
- May improve agentready's own repository understanding

---

### AgentReady Repository Agent

**Priority**: P3 (Important)

**Description**: Create a specialized Claude Code agent for the AgentReady repository to assist with development, testing, and maintenance tasks.

**Requirements**:
- Agent with deep knowledge of AgentReady architecture
- Understands assessment workflow, scoring logic, and report generation
- Can help with:
  - Implementing new assessors
  - Enhancing existing assessors
  - Writing tests for new features
  - Debugging assessment issues
  - Improving report templates
  - Optimizing performance

**Use Case**:
```bash
# In Claude Code, use the agentready-dev agent
/agentready-dev implement new assessor for dependency security scanning
/agentready-dev debug why Python type annotation detection is failing
/agentready-dev optimize assessment performance for large repositories
```

**Features**:
- Pre-loaded context about AgentReady codebase structure
- Knowledge of assessment attributes and scoring algorithm
- Understanding of tier-based weighting system
- Familiar with reporter implementations (HTML, Markdown)
- Can generate new assessors following established patterns

**Implementation**:
- Create `.claude/agents/agentready-dev.md` with agent specification
- Include links to key design documents (data-model.md, plan.md, research.md)
- Provide common development patterns and examples
- Reference test structure and coverage requirements

**Related**: Development workflow, code generation, testing

**Notes**:
- Agent should follow constitution principles (library-first, TDD when requested)
- Should know about stub assessors and how to expand them
- Can help with performance benchmarking and optimization
- Should understand the research report structure and attribute definitions

---

### Customizable HTML Report Themes

**Priority**: P4 (Enhancement)

**Description**: Allow users to customize the appearance of HTML reports with themes, color schemes, and layout options.

**Requirements**:
- Theme system for HTML reports
- Pre-built themes (default, dark mode, high contrast, colorblind-friendly)
- Custom theme support via configuration
- Maintain accessibility standards (WCAG 2.1 AA)
- Preview themes before applying

**Use Case**:
```yaml
# .agentready-config.yaml
report_theme: dark  # or 'light', 'high-contrast', 'custom'

custom_theme:
  primary_color: "#2563eb"
  success_color: "#10b981"
  warning_color: "#f59e0b"
  danger_color: "#ef4444"
  background: "#1e293b"
  text: "#e2e8f0"
  font_family: "Inter, sans-serif"
```

**Features**:
- **Theme dropdown in top-right corner of HTML report** (runtime switching)
- **Quick dark/light mode toggle button** (one-click switching between dark and light)
- Multiple built-in themes (light, dark, high-contrast, solarized, dracula)
- Dark mode support with proper color inversion
- Custom color palettes
- Font selection (system fonts + web-safe fonts)
- Layout density options (compact, comfortable, spacious)
- Logo/branding customization
- Export theme as reusable configuration
- Save theme preference to localStorage (persists across reports)

**Implementation**:
- CSS custom properties (variables) for theming
- JavaScript theme switcher in HTML report (no page reload)
- Theme loader in HTMLReporter
- Validate theme configurations
- Preserve accessibility in all themes (WCAG 2.1 AA)
- Add theme preview command: `agentready theme-preview dark`
- Embed all theme CSS in single HTML file (offline-capable)

**Related**: HTML report generation, user experience

**Notes**:
- All themes must maintain WCAG 2.1 AA contrast ratios
- Dark mode should invert appropriately, not just be dark
- Consider colorblind-friendly palettes (Viridis, ColorBrewer)
- Custom themes should be shareable (export/import)
- Could add theme gallery in documentation

---

### Fix Code Quality Issues from Code Review

**Priority**: P1 (High - Quality & Reliability)

**Description**: Address P1 issues discovered in code review that affect reliability, accuracy, and code quality.

**Issues to Fix**:

1. **TOCTOU (Time-of-Check-Time-of-Use) in File Operations**
   - **Location**: Multiple assessors (`documentation.py:46-50`, `documentation.py:174-191`)
   - **Problem**: Check if file exists, then read in separate operation - file could be deleted in between
   - **Impact**: Crashes instead of graceful degradation
   - **Fix**: Use try-except around file reads instead of existence checks
   ```python
   # BEFORE:
   if claude_md_path.exists():
       size = claude_md_path.stat().st_size

   # AFTER:
   try:
       with open(claude_md_path, "r") as f:
           size = len(f.read())
   except FileNotFoundError:
       return Finding(...status="fail"...)
   except OSError as e:
       return Finding.error(self.attribute, f"Could not read: {e}")
   ```

2. **Inaccurate Type Annotation Detection**
   - **Location**: `src/agentready/assessors/code_quality.py:98-102`
   - **Problem**: Regex-based detection has false positives (string literals, dict literals)
   - **Impact**: Inflated type annotation coverage scores
   - **Fix**: Use AST parsing instead of regex:
   ```python
   import ast
   tree = ast.parse(content)
   for node in ast.walk(tree):
       if isinstance(node, ast.FunctionDef):
           total_functions += 1
           has_annotations = (node.returns is not None or
                            any(arg.annotation for arg in node.args.args))
           if has_annotations:
               typed_functions += 1
   ```

3. **Assessment Validation Semantic Confusion**
   - **Location**: `src/agentready/models/assessment.py:54-59`
   - **Problem**: Field named `attributes_skipped` but includes `error` and `not_applicable` statuses
   - **Impact**: Confusing API, unclear semantics
   - **Fix**: Rename to `attributes_not_assessed` OR add separate counters

**Acceptance Criteria**:
- [ ] All file operations use try-except pattern
- [ ] Type annotation detection uses AST parsing
- [ ] Assessment model fields clearly named
- [ ] Tests added for TOCTOU edge cases
- [ ] Tests added for type annotation false positives
- [ ] Documentation updated

**Priority Justification**: These affect reliability and measurement accuracy - critical for a quality assessment tool.

**Related**: Testing improvements, code quality

---

### Improve Test Coverage and Edge Case Handling

**Priority**: P1 (High - Quality Assurance)

**Description**: Increase test coverage from 37% to >80% and add tests for critical edge cases discovered in code review.

**Critical Test Gaps**:

1. **Error Handling Paths** (Currently 0% coverage)
   - OSError, PermissionError in file operations
   - MissingToolError in assessors
   - Invalid repository paths
   - Malformed git repositories

2. **Edge Cases** (No tests)
   - Empty repositories
   - Binary files instead of text
   - Symlinks in repository
   - Very large repositories (>10k files)
   - Repositories with `test/` vs `tests/` directories

3. **Security Test Cases**
   - XSS in repository names, commit messages
   - Path traversal attempts
   - Malicious file names

4. **Scorer Edge Cases**
   - All attributes skipped (score should be 0.0)
   - Config weights don't sum to 1.0
   - Division by zero scenarios

**Implementation**:
```python
# tests/unit/test_edge_cases.py
def test_empty_repository(tmp_path):
    """Test assessment of completely empty repository."""
    # Create empty git repo
    repo = Repository(path=tmp_path, ...)
    scanner = Scanner(repo, config=None)
    assessment = scanner.scan(assessors)
    # Should not crash, should have valid score
    assert 0.0 <= assessment.overall_score <= 100.0

def test_permission_denied_file(tmp_path):
    """Test graceful handling of permission errors."""
    # Create unreadable file
    restricted = tmp_path / "CLAUDE.md"
    restricted.write_text("test")
    restricted.chmod(0o000)

    assessor = CLAUDEmdAssessor()
    finding = assessor.assess(Repository(...))
    assert finding.status == "error"
    assert "permission" in finding.error_message.lower()

def test_binary_file_as_readme(tmp_path):
    """Test handling of binary files."""
    readme = tmp_path / "README.md"
    readme.write_bytes(b"\x00\x01\x02\x03")

    assessor = READMEAssessor()
    finding = assessor.assess(Repository(...))
    # Should not crash
```

**Acceptance Criteria**:
- [ ] Test coverage increased to >80%
- [ ] All error handling paths tested
- [ ] Edge cases for empty/malformed repos covered
- [ ] Security test cases added
- [ ] Integration tests for complete workflows
- [ ] CI runs coverage report and fails <75%

**Priority Justification**: Quality assessment tool must be thoroughly tested. Current 37% coverage is unacceptable.

**Related**: CI/CD improvements, reliability

---

### Add Security & Quality Improvements from Code Review

**Priority**: P2 (Medium - Polish)

**Description**: Address P2 improvements from code review for better UX and robustness.

**Improvements**:

1. **Input Validation Warnings**
   - Warn when scanning sensitive directories (`/etc`, `/.ssh`, `/var`)
   - Confirm before scanning large repositories (>10k files)

2. **Scorer Semantic Clarity**
   - Document behavior when all attributes skipped (returns 0.0)
   - Consider returning `None` or special value for "not assessable"
   - Add explicit documentation of edge cases

3. **Content Security Policy Headers**
   - Add CSP to HTML reports for defense-in-depth
   - Prevent inline script execution
   - Whitelist only necessary sources

**Implementation**:
```python
# In CLI
sensitive_dirs = ['/etc', '/sys', '/proc', '/.ssh', '/var']
if any(str(repo_path).startswith(p) for p in sensitive_dirs):
    click.confirm(
        f"Warning: Scanning sensitive directory {repo_path}. Continue?",
        abort=True
    )

# In HTMLReporter
csp_header = (
    "<meta http-equiv='Content-Security-Policy' "
    "content=\"default-src 'self'; script-src 'unsafe-inline'; "
    "style-src 'unsafe-inline'\">"
)
```

**Acceptance Criteria**:
- [ ] Warnings for sensitive directories
- [ ] CSP headers in HTML reports
- [ ] Scorer edge cases documented
- [ ] User guide updated with best practices

**Priority Justification**: UX polish and defense-in-depth, not critical bugs.

**Related**: User experience, security hardening

---

### Align Subcommand (Automated Remediation)

**Priority**: P1 (Critical)

**Description**: Implement `agentready align` subcommand that automatically fixes failing attributes by generating and applying changes to the repository.

**Vision**: One command to align your repository with best practices - automatically create missing files, configure tooling, and improve code quality.

**Core Command**:

```bash
# Align repository to best practices
agentready align .

# Preview changes without applying
agentready align . --dry-run

# Apply specific attributes only
agentready align . --attributes claude_md_file,precommit_hooks

# Create GitHub PR instead of direct changes
agentready align . --create-pr

# Interactive mode (confirm each change)
agentready align . --interactive
```

**Supported Fixes**:

1. **Template-Based Fixes** (Auto-applicable):
   - `claude_md_file`: Generate CLAUDE.md from repository analysis
   - `gitignore_completeness`: Add missing patterns to .gitignore
   - `precommit_hooks`: Create .pre-commit-config.yaml with language-specific hooks
   - `readme_structure`: Scaffold missing README sections
   - `lock_files`: Generate lock files (package-lock.json, requirements.txt, etc.)
   - `issue_pr_templates`: Create .github/ISSUE_TEMPLATE and PULL_REQUEST_TEMPLATE
   - `conventional_commits`: Add commitlint configuration

2. **Command-Based Fixes** (Execute commands):
   - `lock_files`: Run `npm install`, `poetry lock`, `go mod tidy`
   - `precommit_hooks`: Run `pre-commit install`
   - `dependency_freshness`: Run `npm update`, `pip install --upgrade`

3. **AI-Powered Fixes** (Require LLM, optional):
   - `type_annotations`: Add type hints to Python functions
   - `inline_documentation`: Generate docstrings from function signatures
   - `cyclomatic_complexity`: Refactor high-complexity functions
   - `file_size_limits`: Split large files into smaller modules

**Workflow**:

```bash
# User runs alignment
$ agentready align . --dry-run

AgentReady Alignment Preview
============================

Repository: /Users/jeder/my-project
Current Score: 62.4/100 (Silver)
Projected Score: 84.7/100 (Gold) ðŸŽ¯

Changes to be applied:

  âœ… claude_md_file (10 points)
     CREATE CLAUDE.md (1.2 KB)

  âœ… precommit_hooks (3 points)
     CREATE .pre-commit-config.yaml (845 bytes)
     RUN pre-commit install

  âœ… gitignore_completeness (3 points)
     MODIFY .gitignore (+15 patterns)

  âš ï¸  type_annotations (10 points) - requires AI
     MODIFY 23 Python files (add type hints)
     Use --ai to enable AI-powered fixes

Total: 3 automatic fixes, 1 AI fix available
Apply changes? [y/N]
```

**Implementation**:

```python
# src/agentready/fixers/base.py
class BaseFixer(ABC):
    """Base class for attribute fixers."""

    @abstractmethod
    def can_fix(self, finding: Finding) -> bool:
        """Check if this fixer can fix the finding."""
        pass

    @abstractmethod
    def generate_fix(self, repository: Repository, finding: Finding) -> Fix:
        """Generate fix for the finding."""
        pass

# src/agentready/fixers/template_fixer.py
class TemplateFixer(BaseFixer):
    """Fixer that generates files from templates."""

    def generate_fix(self, repository: Repository, finding: Finding) -> Fix:
        template = self.load_template(finding.attribute.id)
        content = self.render_template(template, repository)
        return FileCreationFix(path="CLAUDE.md", content=content)

# src/agentready/cli/align.py
@cli.command()
@click.argument("repository", type=click.Path(exists=True), default=".")
@click.option("--dry-run", is_flag=True, help="Preview changes without applying")
@click.option("--create-pr", is_flag=True, help="Create GitHub PR instead of direct changes")
@click.option("--interactive", is_flag=True, help="Confirm each change")
@click.option("--attributes", help="Comma-separated attribute IDs to fix")
@click.option("--ai", is_flag=True, help="Enable AI-powered fixes (requires API key)")
def align(repository, dry_run, create_pr, interactive, attributes, ai):
    """Align repository with best practices by applying automatic fixes."""

    # Run assessment first
    assessment = run_assessment(repository)

    # Identify fixable failures
    failures = [f for f in assessment.findings if f.status == "fail"]
    fixable = identify_fixable_failures(failures, enable_ai=ai)

    # Generate fixes
    fixes = [fixer.generate_fix(repo, finding) for finding in fixable]

    # Preview changes
    show_fix_preview(fixes, assessment.overall_score, projected_score)

    if dry_run:
        return

    if interactive and not confirm_each_fix(fixes):
        return

    # Apply fixes
    if create_pr:
        create_github_pr_with_fixes(fixes)
    else:
        apply_fixes(fixes)

    # Re-run assessment to show improvement
    new_assessment = run_assessment(repository)
    show_improvement(assessment.overall_score, new_assessment.overall_score)
```

**Fix Types**:

```python
class Fix(ABC):
    """Base class for fixes."""
    attribute_id: str
    description: str

class FileCreationFix(Fix):
    """Create a new file."""
    path: Path
    content: str

class FileModificationFix(Fix):
    """Modify existing file."""
    path: Path
    changes: List[TextChange]

class CommandFix(Fix):
    """Execute command."""
    command: str
    working_dir: Path

class MultiStepFix(Fix):
    """Combination of multiple fixes."""
    steps: List[Fix]
```

**GitHub PR Integration**:

```bash
# Create PR with fixes
$ agentready align . --create-pr

Creating fix branch: agentready-align-20251121
Applying 3 fixes...
  âœ… Created CLAUDE.md
  âœ… Created .pre-commit-config.yaml
  âœ… Modified .gitignore

Committing changes...
Pushing to origin...

Created PR: https://github.com/redhat/my-project/pull/42
  Title: "Improve AgentReady score from 62.4 to 84.7 (Silver â†’ Gold)"
  Score improvement: +22.3 points
  Attributes fixed: 3
```

**Configuration**:

```yaml
# .agentready-config.yaml
align:
  enabled: true

  auto_fix:
    # Attributes to automatically fix without confirmation
    - claude_md_file
    - gitignore_completeness
    - precommit_hooks

  confirm_before_fix:
    # Attributes requiring confirmation
    - type_annotations
    - cyclomatic_complexity

  never_fix:
    # Attributes to skip (user will fix manually)
    - container_setup
    - openapi_specs

  ai_fixes:
    enabled: false  # Require --ai flag
    provider: "anthropic"  # or "openai"
    model: "claude-3-5-sonnet-20241022"
    max_tokens: 4096
```

**Use Cases**:

**Use Case 1: New Repository Setup**
```bash
# Clone new project
git clone github.com/redhat/new-project
cd new-project

# Align to best practices
agentready align . --interactive

# Review and commit changes
git add .
git commit -m "chore: Align repository with AgentReady best practices"
```

**Use Case 2: Continuous Improvement**
```bash
# Weekly CI job to check and create alignment PRs
agentready align . --create-pr --dry-run
# If score < threshold, create PR automatically
```

**Use Case 3: Pre-commit Hook**
```bash
# .pre-commit-config.yaml
- repo: local
  hooks:
    - id: agentready-align
      name: AgentReady Alignment Check
      entry: agentready align --dry-run
      language: system
      pass_filenames: false
```

**Safety Features**:

- **Dry-run by default** for destructive operations
- **Git worktree** for isolated changes (optional)
- **Backup creation** before modifying files
- **Rollback support** if fixes fail
- **Validation** of generated files before writing
- **Interactive confirmation** for AI-powered fixes

**Related**: Automated remediation, repository improvement, onboarding

**Notes**:
- Start with template-based fixes (highest ROI, lowest risk)
- AI-powered fixes require API key and user consent
- Some attributes cannot be automatically fixed (requires human judgment)
- Consider integration with `git stash` for safety
- Could generate shell script of changes for manual review
- Align with Red Hat's AI-assisted development workflow

---

### Interactive Dashboard with Automated Remediation

**Priority**: P2 (High Value)

**Description**: Transform the static HTML report into an interactive dashboard that enables one-click remediation via automated GitHub issue creation and draft PR generation.

**Vision**: "How to Align Your Repo with Best Practices" - Click a button â†’ Open GitHub issue with draft PR containing fixes.

**Core Features**:

1. **Interactive Dashboard Mode**
   - Real-time assessment (WebSocket updates)
   - Live filtering and sorting (already have this)
   - Action buttons on each failing attribute
   - Progress tracking across multiple runs
   - Historical trend visualization

2. **One-Click Remediation Actions**
   - "Fix This" button on each failing attribute
   - Generates GitHub issue automatically
   - Creates draft PR with proposed changes
   - Links issue to PR
   - Assigns to repository owner/team

3. **Automated Fix Generation**
   - Template-based fixes for common issues:
     - Missing .gitignore â†’ Generate language-specific .gitignore
     - No CLAUDE.md â†’ Generate template with repo analysis
     - Missing pre-commit hooks â†’ Add .pre-commit-config.yaml
     - No lock files â†’ Generate appropriate lock file
     - Missing README sections â†’ Scaffold missing sections
   - AI-powered fixes for complex issues:
     - Refactor high-complexity functions
     - Add type annotations to functions
     - Generate docstrings from function signatures
     - Split large files

4. **GitHub Integration**
   - OAuth authentication with GitHub
   - gh CLI integration for seamless workflow
   - Create issues via GitHub API
   - Create draft PRs via GitHub API
   - Auto-label issues (e.g., `agentready`, `automated-fix`, `tier-1-essential`)
   - Link to AgentReady assessment report

**Technical Architecture**:

```yaml
# New components needed:

Backend (Optional - could be fully client-side with gh CLI):
  - GitHub OAuth app for authentication
  - Issue/PR template generator
  - Fix generator service (template-based + AI-powered)
  - Assessment history tracker

Frontend (Enhanced HTML report):
  - Action buttons with loading states
  - GitHub auth flow UI
  - Progress indicators
  - Toast notifications for actions
  - Modal dialogs for fix preview

CLI Extensions:
  - agentready dashboard .  # Launch local web server
  - agentready fix <attribute-id>  # Generate fix for specific attribute
  - agentready create-issue <attribute-id>  # Create GitHub issue
  - agentready create-pr <attribute-id>  # Create draft PR
```

**Use Cases**:

**Use Case 1: Quick Fixes**
```bash
# User runs assessment
agentready assess . --dashboard

# Opens dashboard in browser at http://localhost:8000
# User clicks "Fix This" on "Missing CLAUDE.md"
# â†’ Creates issue: "Add CLAUDE.md configuration file"
# â†’ Creates draft PR with generated CLAUDE.md template
# â†’ PR includes: project analysis, detected languages, suggested structure
```

**Use Case 2: Batch Remediation**
```bash
# Dashboard shows all failures
# User selects multiple attributes
# Clicks "Fix All Selected"
# â†’ Creates single issue: "Improve AgentReady Score from Silver to Gold"
# â†’ Creates draft PR with all fixes
# â†’ PR description includes before/after score projection
```

**Use Case 3: CI/CD Integration**
```bash
# GitHub Action runs assessment
# Posts comment on PR with assessment results
# Includes links to create remediation issues
# Can auto-create draft PR for improvements
```

**Implementation Approach**:

**Phase 1: Client-Side with gh CLI** (Simplest, no backend needed)
- Use JavaScript in HTML report to call gh CLI via local proxy
- Generate fix files locally
- Use `gh issue create` and `gh pr create`
- Works for users with gh CLI installed

**Phase 2: Dashboard Server** (Enhanced UX)
- Flask/FastAPI server serving dashboard
- WebSocket for live updates
- GitHub OAuth for authentication
- Background workers for fix generation

**Phase 3: Cloud Service** (SaaS offering)
- Hosted dashboard at agentready.dev
- GitHub App installation
- Webhook integration for continuous monitoring
- Team collaboration features

**Fix Templates by Attribute**:

```yaml
claude_md_file:
  type: template
  generates:
    - file: CLAUDE.md
      content: |
        # {repository_name}

        ## Overview
        {auto_generated_description}

        ## Architecture
        {detected_patterns}

        ## Development
        {build_commands}

lock_files:
  type: command
  commands:
    - condition: has_package_json
      run: npm install
    - condition: has_pyproject_toml
      run: poetry lock || pip freeze > requirements.txt

precommit_hooks:
  type: template
  generates:
    - file: .pre-commit-config.yaml
      content: {language_specific_hooks}

readme_structure:
  type: enhancement
  modifies: README.md
  adds_sections:
    - Installation
    - Usage
    - Development
    - Contributing

type_annotations:
  type: ai_powered
  uses: ast_analysis + llm
  modifies: "*.py"
  adds: type_hints
```

**Dashboard vs Report Decision**:

**Keep Both**:
- Static reports for CI/CD, documentation, archiving
- Dashboard for interactive development workflow
- Reports can link to dashboard for remediation
- Dashboard can export static reports

**Benefits of Dashboard**:
- âœ… Interactive remediation workflow
- âœ… Live assessment updates
- âœ… Progress tracking over time
- âœ… Team collaboration (comments, assignments)
- âœ… Automated fix preview before applying
- âœ… Integration with existing tools (GitHub, IDEs)

**Challenges**:
- Authentication complexity (GitHub OAuth)
- Fix generation quality (need good templates + AI)
- PR review overhead (lots of automated PRs)
- Maintaining fix templates as best practices evolve

**Recommended Approach**:

1. **Start with enhanced static report**:
   - Add "Create Issue" buttons that generate gh CLI commands
   - Users copy/paste commands to create issues
   - Include fix templates in issue descriptions

2. **Add local dashboard** (Phase 2):
   - Flask server with WebSocket updates
   - GitHub integration via gh CLI
   - Generate fixes, preview diffs, create PRs

3. **Consider hosted service** (Phase 3+):
   - If adoption is high
   - SaaS model for teams
   - Continuous monitoring and recommendations

**Related**: GitHub integration, automation, remediation, UX

**Notes**:
- This is a MAJOR feature that could become a standalone product
- Consider MVP: "Copy this gh command" buttons in HTML report
- AI-powered fix generation requires careful validation
- Some fixes (like refactoring) need human review
- Could integrate with existing tools (Dependabot, Renovate)
- May want to partner with GitHub for official integration

---

### GitHub App Integration (Badge & Status Checks)

**Priority**: P2 (High Value)

**Description**: Create a GitHub App that provides badge integration, PR status checks, and automated assessment comments to help Red Hat engineering teams track and improve repository quality.

**Core Features**:

1. **Repository Badge**
   - Shields.io-compatible SVG badge showing certification level
   - Endpoint: `https://agentready.redhat.com/badge/{owner}/{repo}.svg`
   - Dynamic color based on certification (Platinum=purple, Gold=yellow, Silver=silver, Bronze=brown)
   - Include score: "AgentReady: 85.2 (Gold)"
   - Click badge to view latest assessment report

2. **GitHub Actions Integration**
   - Create official `agentready/assess-action` GitHub Action
   - Run assessment on PR events (opened, synchronized, reopened)
   - Run assessment on push to main/master
   - Support custom triggers via workflow_dispatch

3. **PR Status Checks**
   - Use GitHub Commit Status API to report assessment results
   - Set check status: success (>90), warning (75-89), failure (<75)
   - Configurable thresholds via `.agentready-config.yaml`
   - Block PR merge if score below threshold (optional)
   - Link to detailed HTML report in check details

4. **PR Comments**
   - Automated bot comments on PRs with assessment summary
   - Show score delta: "Score changed: 72.4 â†’ 78.3 (+5.9)"
   - List new failures and fixes
   - Collapsible sections for full findings
   - Trend chart showing last 10 assessments (ASCII or embedded image)
   - Include remediation suggestions for new failures

**Technical Implementation**:

**Phase 1: GitHub Actions Integration**
```yaml
# .github/workflows/agentready.yml
name: AgentReady Assessment
on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches: [main, master]

jobs:
  assess:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: agentready/assess-action@v1
        with:
          threshold: 75
          post-comment: true
          update-status: true
```

**Phase 2: Badge Service**
```python
# FastAPI endpoint for badge generation
@app.get("/badge/{owner}/{repo}.svg")
async def get_badge(owner: str, repo: str):
    # Fetch latest assessment from GitHub Actions artifacts
    # Or run quick assessment on-demand
    score, level = get_latest_assessment(owner, repo)
    color = LEVEL_COLORS[level]
    return SVGResponse(generate_badge(score, level, color))
```

**Phase 3: GitHub App**
- App permissions: Contents (read), Checks (write), Pull requests (write)
- Webhook events: push, pull_request
- Installed via GitHub Marketplace (Red Hat internal)
- Dashboard at agentready.redhat.com showing:
  - Repository list with scores
  - Historical trends
  - Organization-wide statistics
  - Top repositories by improvement

**Integration Points**:

1. **GitHub Actions Artifacts**
   - Store assessment reports as workflow artifacts
   - Keep last 30 days of reports for trend analysis
   - Generate downloadable HTML/JSON/Markdown reports

2. **GitHub Status API**
   ```python
   POST /repos/{owner}/{repo}/statuses/{commit_sha}
   {
     "state": "success",  # or "pending", "failure", "error"
     "target_url": "https://agentready.redhat.com/reports/{run_id}",
     "description": "AgentReady: 85.2 (Gold)",
     "context": "agentready/assessment"
   }
   ```

3. **GitHub Checks API** (preferred over Status API)
   ```python
   POST /repos/{owner}/{repo}/check-runs
   {
     "name": "AgentReady Assessment",
     "status": "completed",
     "conclusion": "success",
     "output": {
       "title": "Score: 85.2/100 (Gold)",
       "summary": "Passed 20/25 attributes",
       "text": "Detailed findings..."
     }
   }
   ```

**Use Cases**:

**Use Case 1: Add Badge to README**
```markdown
# My Project

[![AgentReady](https://agentready.redhat.com/badge/redhat/my-project.svg)](https://agentready.redhat.com/reports/redhat/my-project)
```

**Use Case 2: Enforce Quality Gates**
```yaml
# .agentready-config.yaml
github:
  status_checks:
    enabled: true
    min_score: 75  # Block merge if score < 75
    require_improvement: true  # Block if score decreased
```

**Use Case 3: Track Organization Progress**
- Dashboard shows all repos in Red Hat org
- Filter by team, language, certification level
- Identify repos needing attention
- Celebrate improvements (score increases)

**Configuration**:

```yaml
# .agentready-config.yaml
github:
  badge:
    enabled: true
    style: flat-square  # or flat, plastic, for-the-badge
    label: "AgentReady"

  actions:
    enabled: true
    trigger_on: [pull_request, push]
    post_comment: true
    update_status: true
    upload_artifacts: true

  status_checks:
    enabled: true
    min_score: 75
    require_improvement: false

  comments:
    enabled: true
    show_delta: true
    show_trend: true
    collapse_details: true
```

**Implementation Checklist**:

- [ ] Create `agentready/assess-action` GitHub Action
- [ ] Implement badge generation service
- [ ] Add GitHub Status API integration
- [ ] Add GitHub Checks API integration
- [ ] Implement PR comment generation
- [ ] Add score delta calculation
- [ ] Create assessment artifact storage
- [ ] Build organization dashboard
- [ ] Add Red Hat SSO authentication
- [ ] Deploy to Red Hat infrastructure
- [ ] Create documentation for Red Hat teams
- [ ] Add to Red Hat developer onboarding

**Related**: CI/CD integration, automation, visibility, quality gates

**Notes**:
- Focus on internal Red Hat adoption first
- Badge service could be hosted on Red Hat infrastructure
- Dashboard should integrate with Red Hat IdM for authentication
- Consider integration with Red Hat's existing code quality tools
- GitHub App should be installable via Red Hat GitHub Enterprise
- All data stays within Red Hat infrastructure (no external services)
- Align with Red Hat's OpenShift AI strategy for agentic development
- Could become part of Red Hat's AI-assisted development workflow

---

## Backlog Metadata

**Created**: 2025-11-21
**Last Updated**: 2025-11-21
**Total Items**: 14 (11 original + 3 from code review)

## Priority Summary

- **P0 (Critical)**: 4 items - Security/Logic Bugs (FIX FIRST!), Bootstrap Command, Report Header Metadata, HTML Design Improvements
- **P1 (Critical)**: 4 items - Code Quality Fixes, Test Coverage Improvements, Align Subcommand
- **P2 (High Value)**: 3 items - Security Polish, Interactive Dashboard, GitHub App Integration
- **P3 (Important)**: 2 items - Report Schema Versioning, AgentReady Repository Agent
- **P4 (Enhancement)**: 3 items - Research Report Utility, Repomix Integration, Customizable Themes
````
