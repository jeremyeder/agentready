---
layout: home
title: Home
---

<div class="hero">
  <h1>AgentReady</h1>
  <p class="subtitle">Build and maintain agent-ready codebases with automated infrastructure generation and continuous quality assessment.</p>
  <p class="hero-tagline">Bootstrap infrastructure, assess quality, align automatically, monitor continuously. Four workflows to transform any repository into an AI-ready codebase.</p>
  <div class="hero-buttons">
    <a href="user-guide.html#bootstrap-your-repository" class="button button-primary">âš¡ Bootstrap</a>
    <a href="user-guide.html#running-assessments" class="button button-secondary">ğŸ“Š Assess</a>
    <a href="user-guide.html#align-command" class="button button-tertiary">ğŸ”§ Align</a>
    <a href="#cicd-integration-headless-mode" class="button button-tertiary">ğŸ“ˆ Monitor</a>
  </div>
</div>

## Why AgentReady?

AI-assisted development tools like Claude Code, GitHub Copilot, and Cursor AI work best with well-structured, documented codebases. AgentReady **builds the infrastructure** you need and **continuously assesses** your repository across **25 research-backed attributes** to ensure lasting AI effectiveness.

### Four Powerful Workflows

<div class="feature-grid">
  <div class="feature">
    <h3>âš¡ Bootstrap (Automated Setup)</h3>
    <p><strong>One command to complete infrastructure.</strong> Generates GitHub Actions workflows, pre-commit hooks, issue/PR templates, Dependabot config, and development standards tailored to your language.</p>
    <p><strong>When to use:</strong> New projects, repositories missing automation, or when you want instant best practices.</p>
  </div>
  <div class="feature">
    <h3>ğŸ“Š Assess (Diagnostic Analysis)</h3>
    <p><strong>Deep analysis of 25 attributes.</strong> Evaluates documentation, code quality, testing, structure, and security. Provides actionable remediation guidance with specific tools and commands.</p>
    <p><strong>When to use:</strong> Understanding current state, one-time diagnostics, or pre-commit validation.</p>
  </div>
  <div class="feature">
    <h3>ğŸ”§ Align (Automated Remediation)</h3>
    <p><strong>Automatically fix infrastructure gaps.</strong> Creates missing files (CLAUDE.md, README), configures pre-commit hooks, updates .gitignore, and establishes foundations that make AI agents immediately effective.</p>
    <p><strong>When to use:</strong> After assessment, to automatically establish baseline agent-ready infrastructure without manual work.</p>
  </div>
  <div class="feature">
    <h3>ğŸ“ˆ Monitor (Continuous Quality)</h3>
    <p><strong>Runs assess automatically in CI/CD.</strong> Posts PR comments, tracks codebase quality over time, fails builds on infrastructure regressions, and maintains agent effectiveness gates.</p>
    <p><strong>When to use:</strong> After bootstrap, runs automatically on every PR and push to main branch.</p>
  </div>
</div>

## Quick Start

### Bootstrap-First Workflow (Recommended)

```bash
# Install AgentReady
pip install agentready

# Bootstrap your repository (generates all infrastructure)
cd /path/to/your/repo
agentready bootstrap .

# Review generated files
ls -la .github/workflows/
cat .pre-commit-config.yaml

# Commit and push
git add .
git commit -m "build: Bootstrap agent-ready infrastructure"
git push

# Assessment runs automatically on next PR!
```

**What you get in <60 seconds:** GitHub Actions workflows (tests, security, assessment), pre-commit hooks (language-specific formatters/linters), issue & PR templates, Dependabot automation, contributing guidelines, and automatic AgentReady assessment on every PR.

### Align Workflow (Automated Remediation)

```bash
# Assess first to identify gaps
agentready assess .

# Automatically apply fixes
agentready align --dry-run .  # Preview changes first
agentready align .            # Apply fixes

# Re-assess to validate improvements
agentready assess .
```

**What align fixes:** Creates CLAUDE.md for project context, creates/updates README.md for clear entry points, adds pre-commit hooks to prevent broken commits, updates .gitignore to reduce noise, creates .gitattributes for consistency, and establishes foundations that make agents immediately more effective.

### Manual Assessment Workflow

```bash
# Run one-time assessment without infrastructure changes
agentready assess .

# View interactive HTML report
open .agentready/report-latest.html
```

**Assessment output:** Overall score and certification level, detailed findings for 25 attributes, specific remediation steps with tools and examples, three report formats (HTML, Markdown, JSON).

### CI/CD Integration (Headless Mode)

```yaml
# GitHub Actions workflow (auto-generated by bootstrap)
name: AgentReady Assessment
on: [pull_request, push]
jobs:
  assess:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Install AgentReady
        run: pip install agentready
      - name: Run Assessment
        run: agentready assess . --verbose
      - name: Upload Reports
        uses: actions/upload-artifact@v4
        with:
          name: agentready-reports
          path: .agentready/
      - name: Comment PR
        run: gh pr comment ${{ github.event.number }} --body-file .agentready/report-latest.md
```

**CI/CD benefits:** Automated quality gates on every PR, infrastructure regression detection (fail if foundations degrade), historical tracking with artifacts, automatic PR comments with results, zero manual intervention.

[Read the complete user guide â†’](user-guide.html)

## CLI Reference

AgentReady provides a comprehensive CLI with multiple commands for different workflows:

```
Usage: agentready [OPTIONS] COMMAND [ARGS]...

  AgentReady Repository Scorer - Assess repositories for AI-assisted
  development.

  Evaluates repositories against 25 evidence-based attributes and generates
  comprehensive reports with scores, findings, and remediation guidance.

Options:
  --version  Show version information
  --help     Show this message and exit.

Commands:
  align             Align repository with best practices by applying fixes
  assess            Assess a repository against agent-ready criteria
  assess-batch      Assess multiple repositories in a batch operation
  bootstrap         Bootstrap repository with GitHub infrastructure
  demo              Run an automated demonstration of AgentReady
  experiment        SWE-bench experiment commands
  extract-skills    Extract reusable patterns and generate Claude Code skills
  generate-config   Generate example configuration file
  learn             Extract reusable patterns and generate skills (alias)
  migrate-report    Migrate assessment report to different schema version
  repomix-generate  Generate Repomix repository context for AI consumption
  research          Manage and validate research reports
  research-version  Show bundled research report version
  submit            Submit assessment results to AgentReady leaderboard
  validate-report   Validate assessment report against schema version
```

### Core Commands

<div class="command-grid">
  <div class="command-box">
    <h4>ğŸš€ <a href="user-guide.html#bootstrap-your-repository">bootstrap</a></h4>
    <p>One-command infrastructure generation. Creates GitHub Actions, pre-commit hooks, issue/PR templates, and more.</p>
    <code>agentready bootstrap .</code>
  </div>

  <div class="command-box">
    <h4>ğŸ”§ <a href="user-guide.html#align-command">align</a></h4>
    <p>Automated remediation. Applies fixes to improve your score (create CLAUDE.md, add pre-commit hooks, update .gitignore).</p>
    <code>agentready align --dry-run .</code>
  </div>

  <div class="command-box">
    <h4>ğŸ“Š <a href="user-guide.html#running-assessments">assess</a></h4>
    <p>Deep analysis of 25 attributes. Generates HTML, Markdown, and JSON reports with remediation guidance.</p>
    <code>agentready assess .</code>
  </div>

  <div class="command-box">
    <h4>ğŸ† <a href="leaderboard.html">submit</a></h4>
    <p>Submit your score to the public leaderboard. Track improvements and compare with other repositories.</p>
    <code>agentready submit</code>
  </div>
</div>

### Specialized Commands

- **`assess-batch`** - Assess multiple repositories in parallel ([batch documentation â†’](user-guide.html#batch-assessment))
- **`demo`** - Interactive demonstration mode showing AgentReady in action
- **`extract-skills`/`learn`** - Generate Claude Code skills from repository patterns
- **`repomix-generate`** - Create AI-optimized repository context files
- **`experiment`** - Run SWE-bench validation studies ([experiments â†’](developer-guide.html#experiments))
- **`research`** - Manage research report versions and validation
- **`migrate-report`/`validate-report`** - Schema management and migration tools

[View detailed command documentation â†’](user-guide.html#command-reference)

## Certification Levels

AgentReady scores repositories on a 0-100 scale with tier-weighted attributes:

<div class="certification-ladder">
  <div class="cert-level platinum">
    <div class="cert-badge">ğŸ† Platinum</div>
    <div class="cert-range">90-100</div>
    <div class="cert-desc">Exemplary agent-ready codebase</div>
  </div>
  <div class="cert-level gold">
    <div class="cert-badge">ğŸ¥‡ Gold</div>
    <div class="cert-range">75-89</div>
    <div class="cert-desc">Highly optimized for AI agents</div>
  </div>
  <div class="cert-level silver">
    <div class="cert-badge">ğŸ¥ˆ Silver</div>
    <div class="cert-range">60-74</div>
    <div class="cert-desc">Well-suited for AI development</div>
  </div>
  <div class="cert-level bronze">
    <div class="cert-badge">ğŸ¥‰ Bronze</div>
    <div class="cert-range">40-59</div>
    <div class="cert-desc">Basic agent compatibility</div>
  </div>
  <div class="cert-level needs-improvement">
    <div class="cert-badge">ğŸ“ˆ Needs Improvement</div>
    <div class="cert-range">0-39</div>
    <div class="cert-desc">Significant friction for AI agents</div>
  </div>
</div>

**AgentReady itself scores 80.0/100 (Gold)** â€” see our [self-assessment report](examples.html#agentready-self-assessment).

## What Gets Assessed?

AgentReady evaluates 25 attributes organized into four weighted tiers. **Tier 1 (50% of score)** covers the fundamentals enabling basic AI agent functionality: CLAUDE.md file for project context, README structure as documentation entry point, type annotations for code understanding, standard project layout for predictability, and lock files for reproducible dependencies.

**Tier 2 (30% of score)** provides major quality improvements and safety nets: test coverage for confident refactoring, pre-commit hooks for quality enforcement, conventional commits for structured history, gitignore completeness for clean navigation, and one-command setup for easy reproduction.

**Tier 3 (15% of score)** delivers significant improvements in specific areas: cyclomatic complexity metrics, structured logging for debugging, API documentation specs, architecture decision records, and semantic naming conventions.

**Tier 4 (5% of score)** handles refinement and optimization: security scanning, performance benchmarks, code smell elimination, PR/issue templates, and container setup.

## Report Formats

AgentReady generates three complementary formats. **Interactive HTML** provides color-coded findings, search/filter/sort capabilities, collapsible sections, and works offlineâ€”ideal for sharing with stakeholders. **Version-Control Markdown** offers GitHub-Flavored format that's git-diffable for tracking progress, with certification ladder and next stepsâ€”commit it to track improvements over time. **Machine-Readable JSON** contains complete assessment data with timestamps, metadata, and structured findingsâ€”perfect for CI/CD integration and programmatic analysis.

[See example reports â†’](examples.html)

## Evidence-Based Research

All 25 attributes derive from authoritative sources: Anthropic (Claude Code best practices), Microsoft (Code metrics and Azure DevOps), Google (SRE handbook and style guides), ArXiv (software engineering research), and IEEE/ACM (academic publications on code quality). Every attribute includes specific citations and measurable criteriaâ€”no subjective opinions, just proven practices.

[Read the research document â†’](https://github.com/ambient-code/agentready/blob/main/agent-ready-codebase-attributes.md)

## What The AI Bubble Taught Us

> "Fired all our junior developers because 'AI can code now,' then spent $2M on GitHub Copilot Enterprise only to discover it works better with... documentation? And tests? Turns out you can't replace humans with spicy autocomplete and vibes."
> â€” *CTO, Currently Rehiring*

> "My AI coding assistant told me it was 'very confident' about a solution that would have deleted production. Running AgentReady revealed our codebase has the readability of a ransom note. The AI was confident because it had no idea what it was doing. Just like us!"
> â€” *Senior Developer, Trust Issues Intensifying*

> "We added 'AI-driven development' to the Series B deck before checking if our monolith had a README. AgentReady scored us 23/100. The AI couldn't figure out our codebase because *we* couldn't figure out our codebase. Investors were not impressed."
> â€” *VP Engineering, Learning About README Files The Hard Way*

> "Spent the year at conferences saying 'AI will 10x productivity' while our agents hallucinated imports, invented APIs, and confidently suggested `rm -rf /`. AgentReady showed us we're missing pre-commit hooks, type annotations, and basic self-awareness. The only thing getting 10x'd was our incident rate."
> â€” *Tech Lead, Reformed Hype Man*

> "Asked ChatGPT to refactor our auth system. It wrote beautiful code that compiled perfectly and had zero relation to our actual database schema. Turns out when you have no CLAUDE.md file, no ADRs, and variable names like `data2_final_FINAL`, even AGI would just be guessing. And AGI doesn't exist yet."
> â€” *Staff Engineer, Back to Documentation Basics*

> "My manager saw a demo where AI 'wrote an entire app' and asked why I'm still employed. I showed him our AgentReady score of 31/100, explained that missing lock files and zero test coverage make AI as useful as a Magic 8-Ball, and we spent the next quarter actually engineering instead of prompt-debugging. AI didn't replace me. Basic hygiene saved me."
> â€” *Developer, Still Employed, Surprisingly*

## Ready to Get Started?

<div class="cta-section">
  <h3>Assess your repository in 60 seconds</h3>
  <pre><code>pip install agentready
agentready assess .
</code></pre>
  <a href="user-guide.html" class="button button-primary button-large">Read the User Guide</a>
</div>

---

## What Bootstrap Generates

AgentReady Bootstrap creates production-ready infrastructure tailored to your language. **GitHub Actions workflows** include agentready-assessment.yml (runs on every PR/push, posts results as comments, tracks score progression), tests.yml (language-specific with pytest/jest/go test and coverage), and security.yml (CodeQL analysis and dependency scanning). **GitHub templates** provide structured issue templates for bugs and features with auto-labeling, PR template with testing checklist, and CODEOWNERS for review assignments. **Development infrastructure** includes .pre-commit-config.yaml (language-specific formatters and linters), .github/dependabot.yml (weekly updates with automatic PRs), plus CONTRIBUTING.md and CODE_OF_CONDUCT.md if missing.

## Latest News

**Version 1.27.2 Released** (2025-11-23)
Stability improvements with comprehensive pytest fixes! Resolved 35 test failures through enhanced model validation and path sanitization. Added shared test fixtures and improved Assessment schema handling. Significantly improved test coverage with comprehensive CLI and service module tests.

**Version 1.0.0 Released** (2025-11-21)
Initial release with 10 implemented assessors, interactive HTML reports, and comprehensive documentation. AgentReady achieves Gold certification (80.0/100) on its own codebase.

[View full changelog â†’](https://github.com/ambient-code/agentready/releases)

## Community

- **GitHub**: [github.com/ambient-code/agentready](https://github.com/ambient-code/agentready)
- **Issues**: Report bugs or request features
- **Discussions**: Ask questions and share experiences
- **Contributing**: See the [Developer Guide](developer-guide.html)

## License

AgentReady is open source under the [MIT License](https://github.com/ambient-code/agentready/blob/main/LICENSE).
